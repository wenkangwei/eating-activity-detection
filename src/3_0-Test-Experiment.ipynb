{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3_1 Comparison experiments\n",
    "This  section is to compare the Group trained model with individual trained models performance (cross validation applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda Device Count:  2 Device Name:  Tesla V100-PCIE-16GB\n",
      "Torch version: 1.7.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from dataset import create_train_test_file_list, Person_MealsDataset, balance_data_indices\n",
    "from utils import *\n",
    "from model import *\n",
    "\n",
    "\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device: \",device,\"Device Count: \", torch.cuda.device_count(), \"Device Name: \",torch.cuda.get_device_name()  )\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "### imports\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Turn off TensorFlow logging\n",
    "import tensorflow.compat.v1 as tf # maintain compatibility with TensorFlow 2.2.0\n",
    "\n",
    "import keras\n",
    "# from tensorflow.compat.v1.keras import backend as K # changed for compatibility with TensorFlow 2.2.0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "import loadfile\n",
    "import addons\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l1\n",
    "from keras.models import load_model, save_model\n",
    "\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "shimmer_global_mean = [-0.012359981,-0.0051663737,0.011612018,\n",
    "                        0.05796114,0.1477952,-0.034395125 ]\n",
    "\n",
    "shimmer_global_stddev = [0.05756385,0.040893298,0.043825723,\n",
    "                        17.199743,15.311142,21.229317 ]\n",
    "\n",
    "shimmer_trended_mean = [-0.000002,-0.000002,-0.000000,\n",
    "                0.058144,0.147621,-0.033260 ]\n",
    "\n",
    "shimmer_trended_stddev = [0.037592,0.034135,0.032263,\n",
    "                17.209038,15.321441,21.242532 ]\n",
    "\n",
    "all_zero_means = [0,0,0,0,0,0]\n",
    "\n",
    "meanvals = all_zero_means\n",
    "stdvals = shimmer_trended_stddev\n",
    "\n",
    "\n",
    "random_seed  = 1000\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/20/2020, 15:21:17\n",
      "WindowLength: 3.00 min (2700 datum)\tSlide: 15 (225 datum)\tEpochs:30\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.10/10.10.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.12/10.12.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.14/10.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.16/10.16.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.19/10.19.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/afternoon_2hr33min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/evening_2hr_20min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.21/10.21.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.26/10.26.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.28/10.28.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/evening-2hr_goodDinnerTemplate_CFAmeal/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.4/10.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.6/10.6.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.8/10.8.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.11/11.11.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.14/11.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.15/11.15.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.2/11.2.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.3/11.3.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.4/11.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.23/9.23_13hr.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.25/9.25_1-46.shm\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "winmin = 3 \n",
    "stridesec = 15\n",
    "\n",
    "print_settings(winmin,stridesec, EPOCHS)\n",
    "# Load the dataset\n",
    "meal_data_train = Person_MealsDataset(person_name= \"lawler\", file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "# meal_data_train = Person_MealsDataset(person_name= \"adam\", file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "# meal_data_test = Person_MealsDataset(person_name= \"adam\", file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meal_data_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b93fa0ec3a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainingsamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraininglabels\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmeal_data_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeal_data_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtestsamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestlabels\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmeal_data_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeal_data_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_shuffledUnderSampledBalancedIndices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbalance_data_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraininglabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"under\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'meal_data_test' is not defined"
     ]
    }
   ],
   "source": [
    "#random.seed(random_seed)\n",
    "\n",
    "# load\n",
    "trainingsamples,traininglabels =  meal_data_train.data_indices, meal_data_train.labels\n",
    "testsamples,testlabels =  meal_data_test.data_indices, meal_data_test.labels\n",
    "\n",
    "train_shuffledUnderSampledBalancedIndices = balance_data_indices(traininglabels,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "test_shuffledUnderSampledBalancedIndices = balance_data_indices(testlabels,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "print(\"Train set size: %d, Test set size: %d\"%(len(train_shuffledUnderSampledBalancedIndices),len(test_shuffledUnderSampledBalancedIndices)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 8620, with 4310 positive samples and 4310 negative samples\n",
      "Test set size: 2156, with 1078 positive samples and 1078 negative samples\n"
     ]
    }
   ],
   "source": [
    "train_indices, valid_indices = split_train_test_indices(X= train_shuffledUnderSampledBalancedIndices,\n",
    "                                                        y = traininglabels, test_size = 0.2,\n",
    "                                                       random_seed = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 2875, with 1437 positive samples and 1438 negative samples\n",
      "Test set size: 719, with 360 positive samples and 359 negative samples\n",
      "Train set size: 2300, with 1150 positive samples and 1150 negative samples\n",
      "Test set size: 575, with 287 positive samples and 288 negative samples\n"
     ]
    }
   ],
   "source": [
    "trainingsamples,traininglabels =  meal_data_train.data_indices, meal_data_train.labels\n",
    "train_shuffledUnderSampledBalancedIndices = balance_data_indices(traininglabels,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "train_indices, test_indices = split_train_test_indices(X= train_shuffledUnderSampledBalancedIndices,\n",
    "                                                        y = traininglabels, test_size = 0.2,\n",
    "                                                       random_seed = random_seed)\n",
    "\n",
    "train_indices, valid_indices = split_train_test_indices(X= train_indices,\n",
    "                                                        y = traininglabels, test_size = 0.2,\n",
    "                                                       random_seed = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set(train_indices)\n",
    "b = set(valid_indices)\n",
    "c=set(test_indices)\n",
    "d = a.intersection(b).intersection(c)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loader Created\n"
     ]
    }
   ],
   "source": [
    "train_set_balanced = torch.utils.data.Subset(meal_data_train, train_indices)\n",
    "valid_set_balanced = torch.utils.data.Subset(meal_data_train, valid_indices)\n",
    "\n",
    "# test_set_balanced = torch.utils.data.Subset(meal_data_test, test_shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(test_set_balanced ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "print(\"Data Loader Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set batch amounts: 36\n",
      "Test set : 9\n",
      "Start Training..\n",
      "Epoch: 0,  Epoch_Loss: 1.4536, Train Acc: 52.9130 %, Train Recall: 0.5139 \n",
      "Validation Acc:  52.8696 %,  Validation Recall: 0.8467 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 1,  Epoch_Loss: 0.6582, Train Acc: 62.6522 %, Train Recall: 0.6443 \n",
      "Validation Acc:  53.5652 %,  Validation Recall: 0.4042 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 2,  Epoch_Loss: 0.5968, Train Acc: 68.0435 %, Train Recall: 0.6757 \n",
      "Validation Acc:  54.7826 %,  Validation Recall: 0.2091 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 3,  Epoch_Loss: 0.5674, Train Acc: 69.7391 %, Train Recall: 0.7000 \n",
      "Validation Acc:  63.6522 %,  Validation Recall: 0.7422 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 4,  Epoch_Loss: 0.5240, Train Acc: 74.9565 %, Train Recall: 0.7304 \n",
      "Validation Acc:  78.4348 %,  Validation Recall: 0.7596 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 5,  Epoch_Loss: 0.5142, Train Acc: 76.3478 %, Train Recall: 0.7565 \n",
      "Validation Acc:  76.5217 %,  Validation Recall: 0.6411 \n",
      "\n",
      "\n",
      "Epoch: 6,  Epoch_Loss: 0.5037, Train Acc: 76.8696 %, Train Recall: 0.7478 \n",
      "Validation Acc:  80.5217 %,  Validation Recall: 0.8293 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 7,  Epoch_Loss: 0.4898, Train Acc: 77.9565 %, Train Recall: 0.7496 \n",
      "Validation Acc:  80.5217 %,  Validation Recall: 0.7840 \n",
      "\n",
      "\n",
      "Epoch: 8,  Epoch_Loss: 0.4870, Train Acc: 77.5652 %, Train Recall: 0.7583 \n",
      "Validation Acc:  81.2174 %,  Validation Recall: 0.8014 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 9,  Epoch_Loss: 0.4870, Train Acc: 77.5217 %, Train Recall: 0.7652 \n",
      "Validation Acc:  80.6957 %,  Validation Recall: 0.7805 \n",
      "\n",
      "\n",
      "Epoch: 10,  Epoch_Loss: 0.4863, Train Acc: 77.8261 %, Train Recall: 0.7670 \n",
      "Validation Acc:  81.5652 %,  Validation Recall: 0.8188 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 11,  Epoch_Loss: 0.4827, Train Acc: 77.8696 %, Train Recall: 0.7643 \n",
      "Validation Acc:  81.7391 %,  Validation Recall: 0.8014 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 12,  Epoch_Loss: 0.4801, Train Acc: 78.0000 %, Train Recall: 0.7730 \n",
      "Validation Acc:  81.5652 %,  Validation Recall: 0.8014 \n",
      "\n",
      "\n",
      "Epoch: 13,  Epoch_Loss: 0.4812, Train Acc: 78.8696 %, Train Recall: 0.7748 \n",
      "Validation Acc:  81.5652 %,  Validation Recall: 0.8084 \n",
      "\n",
      "\n",
      "Epoch: 14,  Epoch_Loss: 0.4814, Train Acc: 78.3478 %, Train Recall: 0.7696 \n",
      "Validation Acc:  81.5652 %,  Validation Recall: 0.7944 \n",
      "\n",
      "\n",
      "Epoch: 15,  Epoch_Loss: 0.4795, Train Acc: 78.2174 %, Train Recall: 0.7574 \n",
      "Validation Acc:  81.5652 %,  Validation Recall: 0.8014 \n",
      "\n",
      "\n",
      "Epoch: 16,  Epoch_Loss: 0.4821, Train Acc: 78.3043 %, Train Recall: 0.7643 \n",
      "Validation Acc:  81.0435 %,  Validation Recall: 0.8188 \n",
      "\n",
      "\n",
      "Epoch: 17,  Epoch_Loss: 0.4810, Train Acc: 79.0000 %, Train Recall: 0.7791 \n",
      "Validation Acc:  81.5652 %,  Validation Recall: 0.8014 \n",
      "\n",
      "\n",
      "Epoch: 18,  Epoch_Loss: 0.4785, Train Acc: 78.5217 %, Train Recall: 0.7670 \n",
      "Validation Acc:  81.0435 %,  Validation Recall: 0.8084 \n",
      "\n",
      "\n",
      "Epoch: 19,  Epoch_Loss: 0.4806, Train Acc: 78.4783 %, Train Recall: 0.7678 \n",
      "Validation Acc:  81.3913 %,  Validation Recall: 0.8084 \n",
      "\n",
      "\n",
      "Epoch: 20,  Epoch_Loss: 0.4825, Train Acc: 78.5652 %, Train Recall: 0.7687 \n",
      "Validation Acc:  81.2174 %,  Validation Recall: 0.8014 \n",
      "\n",
      "\n",
      "Epoch: 21,  Epoch_Loss: 0.4816, Train Acc: 78.1739 %, Train Recall: 0.7696 \n",
      "Validation Acc:  80.8696 %,  Validation Recall: 0.8153 \n",
      "\n",
      "\n",
      "Epoch: 22,  Epoch_Loss: 0.4785, Train Acc: 79.0435 %, Train Recall: 0.7800 \n",
      "Validation Acc:  81.2174 %,  Validation Recall: 0.8049 \n",
      "\n",
      "\n",
      "Epoch: 23,  Epoch_Loss: 0.4831, Train Acc: 78.0000 %, Train Recall: 0.7687 \n",
      "Validation Acc:  80.8696 %,  Validation Recall: 0.8049 \n",
      "\n",
      "\n",
      "Epoch: 24,  Epoch_Loss: 0.4810, Train Acc: 78.2174 %, Train Recall: 0.7687 \n",
      "Validation Acc:  81.2174 %,  Validation Recall: 0.8014 \n",
      "\n",
      "\n",
      "Epoch: 25,  Epoch_Loss: 0.4794, Train Acc: 78.9130 %, Train Recall: 0.7730 \n",
      "Validation Acc:  81.2174 %,  Validation Recall: 0.8153 \n",
      "\n",
      "\n",
      "Epoch: 26,  Epoch_Loss: 0.4797, Train Acc: 78.5652 %, Train Recall: 0.7704 \n",
      "Validation Acc:  81.3913 %,  Validation Recall: 0.8014 \n",
      "\n",
      "\n",
      "Epoch: 27,  Epoch_Loss: 0.4805, Train Acc: 78.3478 %, Train Recall: 0.7652 \n",
      "Validation Acc:  81.7391 %,  Validation Recall: 0.8014 \n",
      "\n",
      "\n",
      "Epoch: 28,  Epoch_Loss: 0.4768, Train Acc: 79.1739 %, Train Recall: 0.7835 \n",
      "Validation Acc:  81.3913 %,  Validation Recall: 0.8049 \n",
      "\n",
      "\n",
      "Epoch: 29,  Epoch_Loss: 0.4770, Train Acc: 78.4783 %, Train Recall: 0.7722 \n",
      "Validation Acc:  81.3913 %,  Validation Recall: 0.7979 \n",
      "\n",
      "\n",
      "Load Best Model.\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from utils import *\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "input_shape = (train_set_balanced[0][0].shape[0], train_set_balanced[0][0].shape[1])\n",
    "model_4 = Discriminator_ResNet( ngpu=1, input_shape =input_shape , out_fea = 1)\n",
    "# Since I use a dynamic created layer in network, need to input a sample to initialize the model first\n",
    "_ = model_4(torch.rand((1, input_shape[0],input_shape[1])))\n",
    "model_4.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_4 = optim.Adam(model_4.parameters(),lr=0.01,  weight_decay=0.1)\n",
    "lrscheduler_4 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_4, mode='min',patience= 2, factor = 0.1,threshold=0.01)\n",
    "dataloader = (train_loader, valid_loader)\n",
    "model_4, best_model_4,val_score_4,loss_ls_4, train_acc_ls_4, valid_acc_ls_4 = train_model(model_4,dataloader, optimizer_4, \n",
    "                                                                    criterion, lrscheduler_4, device= device,\n",
    "                                                                    n_epochs=30, patience = 5, l1_enabled=False,\n",
    "                                                                    checkpoint_name =\"../models/lawler_models/checkpoint_model_resnet.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.10/10.10.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.12/10.12.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.14/10.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.16/10.16.shm\n",
      "Test Accuracy: 77.99166948103118\n"
     ]
    }
   ],
   "source": [
    "meal_data_test = Person_MealsDataset(person_name= \"lawler\", file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "# test_set_balanced = torch.utils.data.Subset(meal_data_test, test_indices)\n",
    "test_loader = torch.utils.data.DataLoader(meal_data_test ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "best_model_4.eval()\n",
    "acc, recall = eval_model(best_model_4, test_loader,device)\n",
    "print(\"Test Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(k, model, ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "### imports\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Turn off TensorFlow logging\n",
    "import tensorflow.compat.v1 as tf # maintain compatibility with TensorFlow 2.2.0\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "import keras\n",
    "# from tensorflow.compat.v1.keras import backend as K # changed for compatibility with TensorFlow 2.2.0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "import loadfile\n",
    "import addons\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l1\n",
    "from keras.models import load_model, save_model\n",
    "\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "shimmer_global_mean = [-0.012359981,-0.0051663737,0.011612018,\n",
    "                        0.05796114,0.1477952,-0.034395125 ]\n",
    "\n",
    "shimmer_global_stddev = [0.05756385,0.040893298,0.043825723,\n",
    "                        17.199743,15.311142,21.229317 ]\n",
    "\n",
    "shimmer_trended_mean = [-0.000002,-0.000002,-0.000000,\n",
    "                0.058144,0.147621,-0.033260 ]\n",
    "\n",
    "shimmer_trended_stddev = [0.037592,0.034135,0.032263,\n",
    "                17.209038,15.321441,21.242532 ]\n",
    "\n",
    "all_zero_means = [0,0,0,0,0,0]\n",
    "\n",
    "meanvals = all_zero_means\n",
    "stdvals = shimmer_trended_stddev\n",
    "\n",
    "\n",
    "random_seed  = 1000\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.10/10.10.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.12/10.12.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.14/10.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.16/10.16.shm\n",
      "Train on 2300 samples, validate on 575 samples\n",
      "Epoch 1/30\n",
      "2300/2300 [==============================] - ETA: 0s - loss: 2.3800 - accuracy: 0.5283WARNING:tensorflow:From /home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2300/2300 [==============================] - 1s 620us/sample - loss: 2.3800 - accuracy: 0.5283 - val_loss: 2.2612 - val_accuracy: 0.6365\n",
      "Epoch 2/30\n",
      "2300/2300 [==============================] - 1s 634us/sample - loss: 2.1714 - accuracy: 0.5996 - val_loss: 2.0536 - val_accuracy: 0.6104\n",
      "Epoch 3/30\n",
      "2300/2300 [==============================] - 1s 616us/sample - loss: 1.9747 - accuracy: 0.5683 - val_loss: 1.8619 - val_accuracy: 0.6139\n",
      "Epoch 4/30\n",
      "2300/2300 [==============================] - 1s 623us/sample - loss: 1.7931 - accuracy: 0.6087 - val_loss: 1.6857 - val_accuracy: 0.6557\n",
      "Epoch 5/30\n",
      "2300/2300 [==============================] - 1s 635us/sample - loss: 1.6282 - accuracy: 0.6287 - val_loss: 1.5263 - val_accuracy: 0.6661\n",
      "Epoch 6/30\n",
      "2300/2300 [==============================] - 1s 626us/sample - loss: 1.4791 - accuracy: 0.6483 - val_loss: 1.3834 - val_accuracy: 0.6800\n",
      "Epoch 7/30\n",
      "2300/2300 [==============================] - 1s 630us/sample - loss: 1.3470 - accuracy: 0.6578 - val_loss: 1.2579 - val_accuracy: 0.6922\n",
      "Epoch 8/30\n",
      "2300/2300 [==============================] - 1s 617us/sample - loss: 1.2337 - accuracy: 0.6574 - val_loss: 1.1497 - val_accuracy: 0.6957\n",
      "Epoch 9/30\n",
      "2300/2300 [==============================] - 1s 635us/sample - loss: 1.1339 - accuracy: 0.6587 - val_loss: 1.0610 - val_accuracy: 0.6852\n",
      "Epoch 10/30\n",
      "2300/2300 [==============================] - 1s 612us/sample - loss: 1.0555 - accuracy: 0.6600 - val_loss: 0.9954 - val_accuracy: 0.6557\n",
      "Epoch 11/30\n",
      "2300/2300 [==============================] - 1s 630us/sample - loss: 0.9877 - accuracy: 0.6600 - val_loss: 0.9169 - val_accuracy: 0.7078\n",
      "Epoch 12/30\n",
      "2300/2300 [==============================] - 1s 622us/sample - loss: 0.9358 - accuracy: 0.6726 - val_loss: 0.8734 - val_accuracy: 0.7513\n",
      "Epoch 13/30\n",
      "2300/2300 [==============================] - 1s 625us/sample - loss: 0.8909 - accuracy: 0.6991 - val_loss: 0.8276 - val_accuracy: 0.7600\n",
      "Epoch 14/30\n",
      "2300/2300 [==============================] - 1s 617us/sample - loss: 0.8426 - accuracy: 0.7291 - val_loss: 0.8003 - val_accuracy: 0.7443\n",
      "Epoch 15/30\n",
      "2300/2300 [==============================] - 1s 646us/sample - loss: 0.8126 - accuracy: 0.7270 - val_loss: 0.7599 - val_accuracy: 0.7809\n",
      "Epoch 16/30\n",
      "2300/2300 [==============================] - 1s 637us/sample - loss: 0.7843 - accuracy: 0.7322 - val_loss: 0.7448 - val_accuracy: 0.7757\n",
      "Epoch 17/30\n",
      "2300/2300 [==============================] - 1s 610us/sample - loss: 0.7757 - accuracy: 0.7143 - val_loss: 0.7231 - val_accuracy: 0.7600\n",
      "Epoch 18/30\n",
      "2300/2300 [==============================] - 1s 645us/sample - loss: 0.7508 - accuracy: 0.7378 - val_loss: 0.7115 - val_accuracy: 0.7513\n",
      "Epoch 19/30\n",
      "2300/2300 [==============================] - 1s 640us/sample - loss: 0.7254 - accuracy: 0.7448 - val_loss: 0.6948 - val_accuracy: 0.7704\n",
      "Epoch 20/30\n",
      "2300/2300 [==============================] - 1s 648us/sample - loss: 0.7197 - accuracy: 0.7265 - val_loss: 0.6804 - val_accuracy: 0.7704\n",
      "Epoch 21/30\n",
      "2300/2300 [==============================] - 2s 718us/sample - loss: 0.6931 - accuracy: 0.7483 - val_loss: 0.6560 - val_accuracy: 0.7843\n",
      "Epoch 22/30\n",
      "2300/2300 [==============================] - 1s 624us/sample - loss: 0.6775 - accuracy: 0.7504 - val_loss: 0.6403 - val_accuracy: 0.7930\n",
      "Epoch 23/30\n",
      "2300/2300 [==============================] - 1s 644us/sample - loss: 0.6656 - accuracy: 0.7430 - val_loss: 0.6292 - val_accuracy: 0.7930\n",
      "Epoch 24/30\n",
      "2300/2300 [==============================] - 1s 631us/sample - loss: 0.6539 - accuracy: 0.7504 - val_loss: 0.6311 - val_accuracy: 0.7652\n",
      "Epoch 25/30\n",
      "2300/2300 [==============================] - 1s 637us/sample - loss: 0.6523 - accuracy: 0.7400 - val_loss: 0.6155 - val_accuracy: 0.7861\n",
      "Epoch 26/30\n",
      "2300/2300 [==============================] - 1s 620us/sample - loss: 0.6459 - accuracy: 0.7483 - val_loss: 0.6085 - val_accuracy: 0.7757\n",
      "Epoch 27/30\n",
      "2300/2300 [==============================] - 1s 650us/sample - loss: 0.6279 - accuracy: 0.7496 - val_loss: 0.5992 - val_accuracy: 0.7878\n",
      "Epoch 28/30\n",
      "2300/2300 [==============================] - 1s 630us/sample - loss: 0.6231 - accuracy: 0.7578 - val_loss: 0.6012 - val_accuracy: 0.7826\n",
      "Epoch 29/30\n",
      "2300/2300 [==============================] - 1s 617us/sample - loss: 0.6269 - accuracy: 0.7483 - val_loss: 0.5927 - val_accuracy: 0.7896\n",
      "Epoch 30/30\n",
      "2300/2300 [==============================] - 2s 659us/sample - loss: 0.6088 - accuracy: 0.7600 - val_loss: 0.5779 - val_accuracy: 0.7948\n",
      "Max value:  0.76  at epoch 30\n",
      "Test Accuracy: 0.7295958572554317\n",
      "Recall Accuracy: 0.08697347893915756\n",
      "AUC Score: 0.538660026382065\n"
     ]
    }
   ],
   "source": [
    "balancedData,balancedLabels = meal_data_train.get_subset( train_indices)\n",
    "valid_balancedData,valid_balancedLabels = meal_data_train.get_subset( valid_indices)\n",
    "\n",
    "meal_data_test = Person_MealsDataset(person_name= \"lawler\", file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "balancedData_test,balancedLabels_test = meal_data_train.get_subset([i for i in range(len(meal_data_test))])\n",
    "\n",
    "pathtemp = \"../models/ActiModels/M_F_\"\n",
    "modelpath = pathtemp + \"{:f}Min.h5\".format(winmin)\n",
    "jsonpath = pathtemp + \"{:f}Min.json\".format(winmin)\n",
    "winlength = int(winmin * 60 * 15)\n",
    "step = int(stridesec * 15)\n",
    "start_time = datetime.now()\n",
    "\n",
    "mcp_save = keras.callbacks.ModelCheckpoint(modelpath, save_best_only=True, monitor='accuracy')\n",
    "model = Sequential()\n",
    "model.add(Conv1D(10, 44, strides=2,activation='relu', input_shape=(winlength, 6)))\n",
    "model.add(Conv1D(10, 20, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "model.add(Conv1D(10, 4, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "K.get_session().close()\n",
    "K.set_session(tf.Session())\n",
    "K.get_session().run(tf.global_variables_initializer())\n",
    "H = model.fit(x=balancedData, y = balancedLabels,\n",
    "                       validation_data=[valid_balancedData, valid_balancedLabels],\n",
    "                    epochs = EPOCHS, batch_size=256, verbose=1, validation_split=0.2,\n",
    "                    callbacks=[mcp_save]) # removed addons.LossHistory(jsonpath) for compatibility with TensorFlow 2.2.0, needs to be re-added at some point\n",
    "\n",
    "print(\"Max value: \", max(H.history['accuracy']), \" at epoch\", np.argmax(H.history['accuracy']) + 1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "predictions = model.predict(x=balancedData_test)\n",
    "threshold = 0.5\n",
    "acc =  accuracy_score(predictions>threshold,balancedLabels_test)\n",
    "recall = recall_score(predictions>threshold,balancedLabels_test)\n",
    "auc = roc_auc_score(predictions>threshold,balancedLabels_test)\n",
    "print(\"Test Accuracy:\", acc)\n",
    "print(\"Recall Accuracy:\", recall)\n",
    "print(\"AUC Score:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
