{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Explore My Model\n",
    "# Note: Hardward settings should be: \n",
    "+ **GPU model: P100,  connection: 10ge or higher, one GPU per chunk!,  CPU: 4, Memory: 63GB, Chunk: 2**\n",
    "\n",
    "# Problems that I have met and need to Check:\n",
    "+ hardware setting in palmetto should be **GPU model: P100,  connection: 10ge or higher, one GPU per chunk!,  CPU: 4, Memory: 63GB, Chunk: 2**.  **The number of GPU should be 1**, otherwise, pytorch will raise Runtime Error: Memory is illegal and can not be accessed.\n",
    "I have not figure out why this happen. I may do this later\n",
    "+ In Neural network model, the output should be  avgpooling -> linear(in_fea, 200)-> relu -> linear(200, 1), **we have 2 linear layers after convolution**\n",
    "+ in reshaping batch samples: **Use permute to transpose sample, NOT view!**ï¼Œ using view() could lead to disorder of data (different axises of data are mixed together!)\n",
    "+ Use linear layer output + BCEWithlogitLoss  Or  sigmoid output + BCELoss\n",
    "+ **Use Adam or AdaGrad Optimizer in this experiment, Don't Use SGD!** Since the Loss is too large when SGD and hard to optimize\n",
    "+ If you see the training accuracy doesn't increase and model doesn't fit in pytorch, **Reset the notebook or Re-check the program! Sometimes the notebook stores some previous data we don't want to see, but the was used in notebook!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from dataset import create_train_test_file_list, Person_MealsDataset, balance_data_indices\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device: \",device,\"Device Count: \", torch.cuda.device_count(), \"Device Name: \",torch.cuda.get_device_name()  )\n",
    "    print(\"Torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "### imports\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Turn off TensorFlow logging\n",
    "import tensorflow.compat.v1 as tf # maintain compatibility with TensorFlow 2.2.0\n",
    "\n",
    "import keras\n",
    "# from tensorflow.compat.v1.keras import backend as K # changed for compatibility with TensorFlow 2.2.0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "import loadfile\n",
    "import addons\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l1\n",
    "from keras.models import load_model, save_model\n",
    "\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "shimmer_global_mean = [-0.012359981,-0.0051663737,0.011612018,\n",
    "                        0.05796114,0.1477952,-0.034395125 ]\n",
    "\n",
    "shimmer_global_stddev = [0.05756385,0.040893298,0.043825723,\n",
    "                        17.199743,15.311142,21.229317 ]\n",
    "\n",
    "shimmer_trended_mean = [-0.000002,-0.000002,-0.000000,\n",
    "                0.058144,0.147621,-0.033260 ]\n",
    "\n",
    "shimmer_trended_stddev = [0.037592,0.034135,0.032263,\n",
    "                17.209038,15.321441,21.242532 ]\n",
    "\n",
    "all_zero_means = [0,0,0,0,0,0]\n",
    "\n",
    "meanvals = all_zero_means\n",
    "stdvals = shimmer_trended_stddev\n",
    "\n",
    "\n",
    "random_seed  = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_eager_execution()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/17/2020, 14:26:56\n",
      "WindowLength: 1.00 min (900 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-14-20/10-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-3-20/10-3-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-9-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-14-20/11-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-16-20/11-16-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-18-20/11-18-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/lunch/lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Lunch/Lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-17-20/9-17-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-27-20/9-27-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-29-20/9-29-20.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-10-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-12-20/10-12-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-13-20/10-13-20.shm\n",
      "Data loaded\n",
      "Dataset ready\n"
     ]
    }
   ],
   "source": [
    "ModelNum = 3\n",
    "EPOCHS = 10\n",
    "winmin = 1 \n",
    "stridesec = 1\n",
    "\n",
    "outfile = sys.stdout\n",
    "\n",
    "winlength = int(winmin * 60 * 15)\n",
    "step = int(stridesec * 15)\n",
    "start_time = datetime.now()\n",
    "arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "      \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "      \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "\n",
    "pathtemp = \"../models/ActiModels/M_F_\"\n",
    "modelpath = pathtemp + \"{:f}Min.h5\".format(winmin)\n",
    "jsonpath = pathtemp + \"{:f}Min.json\".format(winmin)\n",
    "\n",
    "[os.system(cmd) for cmd in arr]\n",
    "print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "meal_data_train = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "meal_data_test = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "# meal_data_train = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "\n",
    "print(\"Data loaded\", file=outfile, flush=True)\n",
    "print(\"Dataset ready\", file=outfile, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[333221  20939]\n",
      "Training using every file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(meal_data_train.labels, return_counts=True)\n",
    "print(unique_elements)\n",
    "print(counts_elements)\n",
    "\n",
    "### Set the seed for randomizing\n",
    "random.seed(random_seed)\n",
    "\n",
    "# load\n",
    "print(\"Training using every file\\n\",file=outfile, flush=True)\n",
    "trainingsamples,traininglabels =  meal_data_train.data_indices, meal_data_train.labels\n",
    "testsamples,testlabels =  meal_data_test.data_indices, meal_data_test.labels\n",
    "\n",
    "train_shuffledUnderSampledBalancedIndices = balance_data_indices(traininglabels,mode=\"under\", shuffle=True)\n",
    "test_shuffledUnderSampledBalancedIndices = balance_data_indices(testlabels,mode=\"under\", shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating balanced training data array\", file=outfile, flush=True)\n",
    "balancedData,balancedLabels = meal_data_train.get_subset( train_shuffledUnderSampledBalancedIndices)\n",
    "balancedData_test,balancedLabels_test = meal_data_test.get_subset( test_shuffledUnderSampledBalancedIndices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Training on 8940 samples of length 900\n",
      "\n",
      "Train on 36529 samples, validate on 9133 samples\n",
      "Epoch 1/10\n",
      "36529/36529 [==============================] - 23s 636us/sample - loss: 1.3206 - accuracy: 0.6670 - val_loss: 0.7854 - val_accuracy: 0.7021\n",
      "Epoch 2/10\n",
      "36529/36529 [==============================] - 23s 638us/sample - loss: 0.6816 - accuracy: 0.7116 - val_loss: 0.6520 - val_accuracy: 0.6989\n",
      "Epoch 3/10\n",
      "36529/36529 [==============================] - 24s 653us/sample - loss: 0.6115 - accuracy: 0.7235 - val_loss: 0.6021 - val_accuracy: 0.7324\n",
      "Epoch 4/10\n",
      "36529/36529 [==============================] - 24s 646us/sample - loss: 0.5807 - accuracy: 0.7382 - val_loss: 0.5599 - val_accuracy: 0.7551\n",
      "Epoch 5/10\n",
      "36529/36529 [==============================] - 24s 646us/sample - loss: 0.5597 - accuracy: 0.7517 - val_loss: 0.5440 - val_accuracy: 0.7719\n",
      "Epoch 6/10\n",
      "36529/36529 [==============================] - 23s 640us/sample - loss: 0.5532 - accuracy: 0.7565 - val_loss: 0.5317 - val_accuracy: 0.7694\n",
      "Epoch 7/10\n",
      "36529/36529 [==============================] - 23s 635us/sample - loss: 0.5385 - accuracy: 0.7686 - val_loss: 0.5358 - val_accuracy: 0.7662\n",
      "Epoch 8/10\n",
      "36529/36529 [==============================] - 23s 631us/sample - loss: 0.5226 - accuracy: 0.7799 - val_loss: 0.5166 - val_accuracy: 0.7784\n",
      "Epoch 9/10\n",
      "36529/36529 [==============================] - 23s 632us/sample - loss: 0.5222 - accuracy: 0.7798 - val_loss: 0.5054 - val_accuracy: 0.7859\n",
      "Epoch 10/10\n",
      "36529/36529 [==============================] - 23s 634us/sample - loss: 0.5122 - accuracy: 0.7882 - val_loss: 0.4841 - val_accuracy: 0.8054\n",
      "Max value:  0.788223  at epoch 10\n"
     ]
    }
   ],
   "source": [
    "print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "print(\"Training on {:d} samples of length {:d}\\n\".format(len(shuffledUnderSampledBalancedIndices), len(balancedData[0])), file=outfile, flush=True)\n",
    "\n",
    "\n",
    "K.get_session().close()\n",
    "K.set_session(tf.Session())\n",
    "K.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "mcp_save = keras.callbacks.ModelCheckpoint(modelpath, save_best_only=True, monitor='accuracy')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(10, 44, strides=2,activation='relu', input_shape=(winlength, 6)))\n",
    "model.add(Conv1D(10, 20, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "model.add(Conv1D(10, 4, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "H = model.fit(x=balancedData, y = balancedLabels,\n",
    "#                validation_data=[undersampledTestData, undersampledTestLabels],\n",
    "            epochs = EPOCHS, batch_size=256, verbose=1, validation_split=0.2,\n",
    "            callbacks=[mcp_save]) # removed addons.LossHistory(jsonpath) for compatibility with TensorFlow 2.2.0, needs to be re-added at some point\n",
    "\n",
    "print(\"Max value: \", max(H.history['accuracy']), \" at epoch\", np.argmax(H.history['accuracy']) + 1)\n",
    "K.get_session().close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6607382550335571\n",
      "Recall Accuracy: 0.7155715571557155\n",
      "AUC Score: 0.6718574746720257\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "predictions = model.predict(x=balancedData_test)\n",
    "threshold = 0.5\n",
    "acc_60s =  accuracy_score(predictions>threshold,balancedLabels_test)\n",
    "recall_60s = recall_score(predictions>threshold,balancedLabels_test)\n",
    "auc_60s = roc_auc_score(predictions>threshold,balancedLabels_test)\n",
    "print(\"Test Accuracy:\", acc_60s)\n",
    "print(\"Recall Accuracy:\", recall_60s)\n",
    "print(\"AUC Score:\", auc_60s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45662, 900, 6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balancedData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1498febbe7f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set_balanced = torch.utils.data.Subset(meal_data_train, train_shuffledUnderSampledBalancedIndices)\n",
    "test_set_balanced = torch.utils.data.Subset(meal_data_test, test_shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set_balanced ,batch_size=batch_size, shuffle=True,num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1-SimpleCNNNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.01)\n",
    "#         nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "#         nn.init.normal_(m.weight.data, 0.0, 0.01)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu, input_shape , out_fea = 1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # (input_shape[0], input_shape[1] )=  (number of data point, 6 axis channel )\n",
    "        in_channels, win_size = input_shape[0], input_shape[1]\n",
    "        self.in_channels = in_channels\n",
    "        self.ngpu = ngpu  \n",
    "        self.out_fea = out_fea\n",
    "        filter_size = 10\n",
    "        self.conv0 = nn.Conv1d(in_channels = self.in_channels, \n",
    "                               out_channels = filter_size, \n",
    "                               kernel_size= 44,  stride= 2, padding=0, bias=False)\n",
    "        \n",
    "        self.relu0= nn.ReLU()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(filter_size,filter_size, kernel_size= 20,stride= 2, padding=0, bias=False)\n",
    "        self.relu1= nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(filter_size,filter_size, kernel_size= 4, stride= 2, padding=0, bias=False)\n",
    "        self.relu2= nn.ReLU()\n",
    "        \n",
    "#         self.conv3 = nn.Conv1d(filter_size,filter_size, kernel_size= 4, stride= 2, padding=0, bias=False)\n",
    "#         self.relu3= nn.LeakyReLU(0, inplace=True)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=10)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = None \n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=200, out_features=out_fea, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=out_fea)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.std = 1.\n",
    "        nn.init.normal_(self.conv0.weight.data, 0.0, self.std)\n",
    "        nn.init.normal_(self.conv1.weight.data, 0.0, self.std)\n",
    "        nn.init.normal_(self.conv2.weight.data, 0.0, self.std)\n",
    "        nn.init.normal_(self.linear2.bias.data, 0.0, self.std)\n",
    "        \n",
    "    def l1_loss(self,factor=0.01):\n",
    "        l1_crit = nn.L1Loss(size_average=False,reduction='sum')\n",
    "        reg_loss = 0.\n",
    "        loss = 0.\n",
    "        layers = [self.conv0, self.conv1, self.conv2]\n",
    "        for layer in layers:\n",
    "            for p in layer.parameters():\n",
    "                #print(p)\n",
    "                reg_loss += l1_crit(p, torch.zeros(p.shape))\n",
    "\n",
    "        loss = factor * reg_loss\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input):\n",
    "#         print(\"input shape:\",input.shape)\n",
    "        x = input.permute(0,2,1)\n",
    "        x = self.conv0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        if self.linear1 == None:\n",
    "            self.linear1 = nn.Linear(in_features=x.shape[1], out_features=200, bias=True)\n",
    "            nn.init.normal_(self.linear1.weight.data, 0.0, self.std)\n",
    "            nn.init.normal_(self.linear1.bias.data, 0.0, self.std)\n",
    "            \n",
    "        x = self.relu4(self.linear1(x))\n",
    "        out = self.linear2(x)\n",
    "\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'balancedData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b04285796814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test with model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbalancedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalancedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mngpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mout_fea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'balancedData' is not defined"
     ]
    }
   ],
   "source": [
    "# Test with model \n",
    "from torch.nn import functional as F\n",
    "input_shape = (train_set_balanced[0][0].shape[1], train_set_balanced[0][0].shape[0])\n",
    "print(input_shape)\n",
    "model = Discriminator( ngpu=1, input_shape =input_shape , out_fea = 1).to(device)\n",
    "model.apply(weights_init)\n",
    "for i, (x, label) in enumerate(train_loader):\n",
    "    if i ==2:\n",
    "        x = x.to(device)\n",
    "        label = label.to(device)\n",
    "        out = model(x)\n",
    "#         print(out.dtype, label.dtype)\n",
    "#         print(out)\n",
    "        preds = out\n",
    "        print(\"pred\",preds.shape)\n",
    "        break\n",
    "\n",
    "l1_crit = nn.L1Loss(size_average=False,reduction='sum').to(device)\n",
    "reg_loss = torch.tensor(0).to(device)\n",
    "loss = torch.tensor(0).to(device)\n",
    "# for p in model.linear1.parameters():\n",
    "#     print(p.shape)\n",
    "#     reg_loss += l1_crit(p, torch.zeros(p.shape).to(device)).to(device)\n",
    "\n",
    "# factor = 0.0005\n",
    "# loss += factor * reg_loss\n",
    "# print(\"Loss:\",loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report,recall_score, precision_score\n",
    "\n",
    "\n",
    "def eval_model(model,dataloader):\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    # without update\n",
    "    with torch.no_grad():\n",
    "        for samples, labels in dataloader:\n",
    "            samples = samples.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(samples).to(device).squeeze()\n",
    "            #print(\"Output: \", outputs)\n",
    "            outputs = torch.round(torch.sigmoid(outputs))\n",
    "            preds = outputs>=0.5\n",
    "            preds = preds.to(dtype = torch.float)\n",
    "            preds.requires_grad = False\n",
    "#             _,preds = torch.max(outputs,1)\n",
    "            for i in range(len(preds)):\n",
    "                if preds[i] == 1 and labels[i] == 1:\n",
    "                    TP += 1\n",
    "                if preds[i] == 0 and labels[i] == 1:\n",
    "                    FN += 1\n",
    "            correct += torch.sum((preds == labels)).item()\n",
    "            total += float(len(labels))\n",
    "        acc =100 * correct/ total\n",
    "        recall = TP/(TP+FN)\n",
    "#         print(\"Evaluation Acc: %.4f %%,  Recall: %.4f \"%(acc , recall))\n",
    "    return acc, recall\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "def train_model(model,dataloader, optimizer, criterion, lrscheduler, n_epochs=20,\n",
    "                earlystopping=True, patience= 3, checkpoint_name =\"checkpoint.pt\" ):\n",
    "    loss_ls = []\n",
    "    train_acc_ls = []\n",
    "    valid_acc_ls = []\n",
    "    valid_acc = 0.0\n",
    "    loss =0.0\n",
    "    train_acc = 0.0\n",
    "    patience_count = 0\n",
    "    best_val_score = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    train_dataloader, valid_dataloader = dataloader\n",
    "    print(\"Training set batch amounts:\", len(train_dataloader))\n",
    "    print(\"Test set :\", len(valid_dataloader))\n",
    "    print(\"Start Training..\")\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        correct_cnt = 0.0\n",
    "        total_cnt = 0.0\n",
    "        TP = 0.\n",
    "        FN = 0.\n",
    "        model.train()\n",
    "        for i, (samples, labels) in enumerate(train_dataloader):\n",
    "            samples = samples.to(device)\n",
    "            labels = labels.to(device, dtype=torch.float32)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # reshape samples\n",
    "            outputs = model(samples).squeeze()\n",
    "\n",
    "            #print(\"Output: \", outputs, \"label: \", labels)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            L1_loss = model.l1_loss(0.01).to(device)\n",
    "            loss += L1_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_ls.append(loss)\n",
    "            \n",
    "            # prediction\n",
    "            #_,preds = torch.max(outputs,1)\n",
    "            outputs = torch.round(torch.sigmoid(outputs))\n",
    "            preds = outputs>=0.5\n",
    "            preds = preds.to(dtype = torch.float)\n",
    "            preds.requires_grad = False\n",
    "            \n",
    "            # Compute count of TP, FN\n",
    "            for j in range(len(preds)):\n",
    "                if preds[j] == 1. and labels[j] == 1.:\n",
    "                    TP += 1\n",
    "                if preds[j] == 0. and labels[j] == 1.:\n",
    "                    FN += 1\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            correct_cnt += torch.sum((preds == labels)).item()\n",
    "            total_cnt += float(len(labels))\n",
    "            batch_acc = 100. * (preds == labels).sum().item()/ float(len(labels))\n",
    "            if i %50 ==0:\n",
    "                #print(\"===> Batch: %d,  Batch_Loss: %.4f, Train Acc: %.4f %%,  Recall: %.f\\n\"%(i, loss,batch_acc, recall))\n",
    "                pass\n",
    "\n",
    "            \n",
    "        \n",
    "        # Compute accuracy and loss of one epoch\n",
    "        epoch_loss = running_loss / len(train_dataloader)  \n",
    "        epoch_acc = 100* correct_cnt/ total_cnt  # in percentage\n",
    "        correct_cnt = 0.0\n",
    "        total_cnt = 0.0\n",
    "        train_recall = TP/(TP+FN)\n",
    "        \n",
    "        #Validation mode\n",
    "        model.eval()\n",
    "        valid_acc, valid_recall= eval_model(model,valid_dataloader)\n",
    "        valid_acc_ls.append(valid_acc)   \n",
    "        if e %1==0:\n",
    "            print(\"Epoch: %d,  Epoch_Loss: %.4f, Train Acc: %.4f %%, Train Recall: %.4f \"%(e, epoch_loss,\n",
    "                                                                                     epoch_acc,train_recall))\n",
    "            print(\"Validation Acc:  %.4f %%,  Validation Recall: %.4f \"%(valid_acc, valid_recall))\n",
    "        \n",
    "        # Reset train mode\n",
    "        model.train()\n",
    "        lrscheduler.step(valid_acc)\n",
    "        \n",
    "        \n",
    "        # If earlystopping is enabled, then save model if performance is improved\n",
    "        if earlystopping:\n",
    "            if valid_acc > best_val_score or best_val_score == 0.0:\n",
    "                best_val_score = valid_acc\n",
    "                torch.save(model,checkpoint_name)\n",
    "                patience_count = 0\n",
    "                print(\"Checkpoint Saved\")\n",
    "            else:\n",
    "                if patience_count < patience:\n",
    "                    patience_count += 1\n",
    "                else:\n",
    "                    break\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "            \n",
    "    # Load best model\n",
    "    best_model = torch.load(checkpoint_name)\n",
    "    print(\"Load Best Model.\")\n",
    "    print(\"Training completed\")\n",
    "        \n",
    "    return model, best_model,best_val_score,loss_ls, train_acc_ls, valid_acc_ls\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set batch amounts: 328\n",
      "Test set : 106\n",
      "Start Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0,  Epoch_Loss: 61.8664, Train Acc: 60.3802 %, Train Recall: 0.6082 \n",
      "\n",
      "Training TP: 12736 ,  FN: 8203 \n",
      "Validation Acc:  61.2648 %,  Validation Recall: 0.4343 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 1,  Epoch_Loss: 35.0800, Train Acc: 66.2902 %, Train Recall: 0.5215 \n",
      "\n",
      "Training TP: 10919 ,  FN: 10020 \n",
      "Validation Acc:  65.4882 %,  Validation Recall: 0.4695 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 2,  Epoch_Loss: 29.3741, Train Acc: 72.3411 %, Train Recall: 0.5974 \n",
      "\n",
      "Training TP: 12508 ,  FN: 8431 \n",
      "Validation Acc:  69.7781 %,  Validation Recall: 0.5228 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 3,  Epoch_Loss: 23.4283, Train Acc: 75.9444 %, Train Recall: 0.6472 \n",
      "\n",
      "Training TP: 13552 ,  FN: 7387 \n",
      "Validation Acc:  70.9985 %,  Validation Recall: 0.5293 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 4,  Epoch_Loss: 18.0192, Train Acc: 77.3031 %, Train Recall: 0.6687 \n",
      "\n",
      "Training TP: 14002 ,  FN: 6937 \n",
      "Validation Acc:  72.0488 %,  Validation Recall: 0.5572 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 5,  Epoch_Loss: 13.4778, Train Acc: 78.3395 %, Train Recall: 0.6944 \n",
      "\n",
      "Training TP: 14539 ,  FN: 6400 \n",
      "Validation Acc:  73.0843 %,  Validation Recall: 0.6726 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 6,  Epoch_Loss: 9.9877, Train Acc: 79.9393 %, Train Recall: 0.7271 \n",
      "\n",
      "Training TP: 15224 ,  FN: 5715 \n",
      "Validation Acc:  71.9601 %,  Validation Recall: 0.4951 \n",
      "\n",
      "\n",
      "Epoch: 7,  Epoch_Loss: 7.5036, Train Acc: 80.9255 %, Train Recall: 0.7533 \n",
      "\n",
      "Training TP: 15773 ,  FN: 5166 \n",
      "Validation Acc:  74.7411 %,  Validation Recall: 0.6575 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 8,  Epoch_Loss: 5.7008, Train Acc: 81.8855 %, Train Recall: 0.7746 \n",
      "\n",
      "Training TP: 16220 ,  FN: 4719 \n",
      "Validation Acc:  77.4334 %,  Validation Recall: 0.6686 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 9,  Epoch_Loss: 4.3107, Train Acc: 82.9696 %, Train Recall: 0.7933 \n",
      "\n",
      "Training TP: 16611 ,  FN: 4328 \n",
      "Validation Acc:  77.3077 %,  Validation Recall: 0.6407 \n",
      "\n",
      "\n",
      "Epoch: 10,  Epoch_Loss: 3.2658, Train Acc: 83.4042 %, Train Recall: 0.8016 \n",
      "\n",
      "Training TP: 16784 ,  FN: 4155 \n",
      "Validation Acc:  76.0947 %,  Validation Recall: 0.6293 \n",
      "\n",
      "\n",
      "Epoch: 11,  Epoch_Loss: 2.5232, Train Acc: 82.8191 %, Train Recall: 0.7911 \n",
      "\n",
      "Training TP: 16564 ,  FN: 4375 \n",
      "Validation Acc:  78.5947 %,  Validation Recall: 0.6928 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 12,  Epoch_Loss: 2.0399, Train Acc: 85.1832 %, Train Recall: 0.8212 \n",
      "\n",
      "Training TP: 17195 ,  FN: 3744 \n",
      "Validation Acc:  78.9349 %,  Validation Recall: 0.6932 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 13,  Epoch_Loss: 1.9612, Train Acc: 85.4028 %, Train Recall: 0.8247 \n",
      "\n",
      "Training TP: 17269 ,  FN: 3670 \n",
      "Validation Acc:  78.5947 %,  Validation Recall: 0.7179 \n",
      "\n",
      "\n",
      "Epoch: 14,  Epoch_Loss: 1.8934, Train Acc: 85.3479 %, Train Recall: 0.8230 \n",
      "\n",
      "Training TP: 17232 ,  FN: 3707 \n",
      "Validation Acc:  79.1938 %,  Validation Recall: 0.7033 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Load Best Model.\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# torch.cuda.set_device(0)\n",
    "# device =\"cpu\"\n",
    "\n",
    "input_shape = (train_set_balanced[0][0].shape[1], train_set_balanced[0][0].shape[0])\n",
    "model_1 = Discriminator( ngpu=1, input_shape =input_shape , out_fea = 1).to(device)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer_1 = optim.SGD(model_1.parameters(),lr=0.01,momentum = 0.9,weight_decay=0.01)\n",
    "optimizer_1 = optim.Adam(model_1.parameters(),lr=0.01)\n",
    "lrscheduler_1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_1, mode='min')\n",
    "dataloader = (train_loader, test_loader)\n",
    "model_1, best_model_1,val_score,loss_ls, train_acc_ls, valid_acc_ls = train_model(model_1,dataloader, optimizer_1, \n",
    "                                                                    criterion, lrscheduler_1, \n",
    "                                                                    n_epochs=15, patience = 10,\n",
    "                                                                    checkpoint_name =\"../models/torch_models/checkpoint_model_1.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model =best_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set batch amounts: 328\n",
      "Test set : 106\n",
      "Start Training..\n",
      "Epoch: 0,  Epoch_Loss: 132.0078, Train Acc: 58.4125 %, Train Recall: 0.6100 \n",
      "\n",
      "Training TP: 12772 ,  FN: 8167 \n",
      "Validation Acc:  59.3935 %,  Validation Recall: 0.6803 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 1,  Epoch_Loss: 68.4900, Train Acc: 65.0413 %, Train Recall: 0.6741 \n",
      "\n",
      "Training TP: 14114 ,  FN: 6825 \n",
      "Validation Acc:  62.9364 %,  Validation Recall: 0.7667 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 2,  Epoch_Loss: 58.0614, Train Acc: 68.7927 %, Train Recall: 0.7069 \n",
      "\n",
      "Training TP: 14801 ,  FN: 6138 \n",
      "Validation Acc:  65.7396 %,  Validation Recall: 0.7929 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 3,  Epoch_Loss: 53.1808, Train Acc: 70.9919 %, Train Recall: 0.7255 \n",
      "\n",
      "Training TP: 15192 ,  FN: 5747 \n",
      "Validation Acc:  69.0459 %,  Validation Recall: 0.5638 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 4,  Epoch_Loss: 50.1739, Train Acc: 72.3124 %, Train Recall: 0.7359 \n",
      "\n",
      "Training TP: 15408 ,  FN: 5531 \n",
      "Validation Acc:  70.8136 %,  Validation Recall: 0.6611 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 5,  Epoch_Loss: 48.1663, Train Acc: 73.1028 %, Train Recall: 0.7425 \n",
      "\n",
      "Training TP: 15547 ,  FN: 5392 \n",
      "Validation Acc:  70.5030 %,  Validation Recall: 0.7886 \n",
      "\n",
      "\n",
      "Epoch: 6,  Epoch_Loss: 46.6492, Train Acc: 73.6377 %, Train Recall: 0.7471 \n",
      "\n",
      "Training TP: 15643 ,  FN: 5296 \n",
      "Validation Acc:  67.2485 %,  Validation Recall: 0.4306 \n",
      "\n",
      "\n",
      "Epoch: 7,  Epoch_Loss: 45.5022, Train Acc: 74.1463 %, Train Recall: 0.7487 \n",
      "\n",
      "Training TP: 15676 ,  FN: 5263 \n",
      "Validation Acc:  69.8891 %,  Validation Recall: 0.5130 \n",
      "\n",
      "\n",
      "Epoch: 8,  Epoch_Loss: 44.5292, Train Acc: 74.5021 %, Train Recall: 0.7511 \n",
      "\n",
      "Training TP: 15728 ,  FN: 5211 \n",
      "Validation Acc:  72.3817 %,  Validation Recall: 0.6880 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 9,  Epoch_Loss: 43.7638, Train Acc: 74.8030 %, Train Recall: 0.7548 \n",
      "\n",
      "Training TP: 15804 ,  FN: 5135 \n",
      "Validation Acc:  72.4260 %,  Validation Recall: 0.6534 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 10,  Epoch_Loss: 43.1024, Train Acc: 75.0227 %, Train Recall: 0.7559 \n",
      "\n",
      "Training TP: 15828 ,  FN: 5111 \n",
      "Validation Acc:  72.9142 %,  Validation Recall: 0.6805 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 11,  Epoch_Loss: 42.5038, Train Acc: 75.1779 %, Train Recall: 0.7575 \n",
      "\n",
      "Training TP: 15861 ,  FN: 5078 \n",
      "Validation Acc:  72.1302 %,  Validation Recall: 0.5746 \n",
      "\n",
      "\n",
      "Epoch: 12,  Epoch_Loss: 42.1224, Train Acc: 75.5385 %, Train Recall: 0.7547 \n",
      "\n",
      "Training TP: 15803 ,  FN: 5136 \n",
      "Validation Acc:  72.6331 %,  Validation Recall: 0.6877 \n",
      "\n",
      "\n",
      "Epoch: 13,  Epoch_Loss: 42.0607, Train Acc: 75.5528 %, Train Recall: 0.7608 \n",
      "\n",
      "Training TP: 15930 ,  FN: 5009 \n",
      "Validation Acc:  72.6997 %,  Validation Recall: 0.6772 \n",
      "\n",
      "\n",
      "Epoch: 14,  Epoch_Loss: 41.9987, Train Acc: 75.6029 %, Train Recall: 0.7599 \n",
      "\n",
      "Training TP: 15911 ,  FN: 5028 \n",
      "Validation Acc:  72.6997 %,  Validation Recall: 0.6942 \n",
      "\n",
      "\n",
      "Load Best Model.\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "input_shape = (train_set_balanced[0][0].shape[1], train_set_balanced[0][0].shape[0])\n",
    "model_2 = Discriminator( ngpu=1, input_shape =input_shape , out_fea = 1).to(device)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer_2 = optim.SGD(model_2.parameters(),lr=0.01)\n",
    "optimizer_2 = optim.Adagrad(model_2.parameters(), lr=0.01, lr_decay=0, weight_decay=0)\n",
    "# optimizer_2 = optim.Adam(model_2.parameters(),lr=0.01)\n",
    "lrscheduler_2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_2, mode='min')\n",
    "dataloader = (train_loader, test_loader)\n",
    "model_2, best_model_2,val_score,loss_ls, train_acc_ls, valid_acc_ls = train_model(model_2,dataloader, optimizer_2, \n",
    "                                                                    criterion, lrscheduler_2, \n",
    "                                                                    n_epochs=15, patience = 10,\n",
    "                                                                    checkpoint_name =\"../models/torch_models/checkpoint_model_2.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11, 12, 13, 14],\n",
       "         [15, 16, 17, 18],\n",
       "         [21, 22, 23, 24]],\n",
       "\n",
       "        [[25, 26, 27, 28],\n",
       "         [31, 32, 33, 34],\n",
       "         [35, 36, 37, 38]]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[[11,12,13,14],[15,16,17,18],[21,22,23,24]],[[25,26,27,28],[31,32,33,34],[35,36,37,38]]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11, 12, 13],\n",
       "         [14, 15, 16],\n",
       "         [17, 18, 21],\n",
       "         [22, 23, 24]],\n",
       "\n",
       "        [[25, 26, 27],\n",
       "         [28, 31, 32],\n",
       "         [33, 34, 35],\n",
       "         [36, 37, 38]]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(-1, 4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (4) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [3, 4].  Tensor sizes: [4, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-221-bb66786bc5c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (4) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [3, 4].  Tensor sizes: [4, 3]"
     ]
    }
   ],
   "source": [
    "for i in range(len(a)):\n",
    "    a[i] = a[i].T\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11, 12, 13, 14],\n",
       "         [15, 16, 17, 18],\n",
       "         [21, 22, 23, 24]],\n",
       "\n",
       "        [[25, 26, 27, 28],\n",
       "         [31, 32, 33, 34],\n",
       "         [35, 36, 37, 38]]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11, 15, 21],\n",
       "         [12, 16, 22],\n",
       "         [13, 17, 23],\n",
       "         [14, 18, 24]],\n",
       "\n",
       "        [[25, 31, 35],\n",
       "         [26, 32, 36],\n",
       "         [27, 33, 37],\n",
       "         [28, 34, 38]]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
