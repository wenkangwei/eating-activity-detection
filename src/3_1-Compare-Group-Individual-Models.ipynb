{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3_1 Compare performance of group model and individual models\n",
    "1. Load and split training set, validation set and testing set\n",
    "2. write Cross validation function\n",
    "3. train and save individual models with cross-validation\n",
    "4. train and save group model without cross-validation\n",
    "5. print performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "# Load my models and functions\n",
    "from dataset import create_train_test_file_list, Person_MealsDataset, balance_data_indices\n",
    "from utils import *\n",
    "from model import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Turn off TensorFlow logging\n",
    "import tensorflow.compat.v1 as tf # maintain compatibility with TensorFlow 2.2.0\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "import keras\n",
    "# from tensorflow.compat.v1.keras import backend as K # changed for compatibility with TensorFlow 2.2.0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "import loadfile\n",
    "import addons\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l1\n",
    "from keras.models import load_model, save_model\n",
    "\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "shimmer_global_mean = [-0.012359981,-0.0051663737,0.011612018,\n",
    "                        0.05796114,0.1477952,-0.034395125 ]\n",
    "\n",
    "shimmer_global_stddev = [0.05756385,0.040893298,0.043825723,\n",
    "                        17.199743,15.311142,21.229317 ]\n",
    "\n",
    "shimmer_trended_mean = [-0.000002,-0.000002,-0.000000,\n",
    "                0.058144,0.147621,-0.033260 ]\n",
    "\n",
    "shimmer_trended_stddev = [0.037592,0.034135,0.032263,\n",
    "                17.209038,15.321441,21.242532 ]\n",
    "\n",
    "all_zero_means = [0,0,0,0,0,0]\n",
    "\n",
    "meanvals = all_zero_means\n",
    "stdvals = shimmer_trended_stddev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda Device Count:  2 Device Name:  Tesla V100-PCIE-16GB\n",
      "Torch version: 1.7.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device: \",device,\"Device Count: \", torch.cuda.device_count(), \"Device Name: \",torch.cuda.get_device_name()  )\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "random_seed  = 1000\n",
    "batch_size = 64\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/20/2020, 19:54:18\n",
      "WindowLength: 3.00 min (2700 datum)\tSlide: 15 (225 datum)\tEpochs:30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "winmin = 3 \n",
    "stridesec = 15\n",
    "\n",
    "print_settings(winmin,stridesec, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.10/10.10.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.12/10.12.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.14/10.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.16/10.16.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.19/10.19.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/afternoon_2hr33min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/evening_2hr_20min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.21/10.21.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.26/10.26.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.28/10.28.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/evening-2hr_goodDinnerTemplate_CFAmeal/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.4/10.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.6/10.6.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.8/10.8.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.11/11.11.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.14/11.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.15/11.15.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.2/11.2.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.3/11.3.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.4/11.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.23/9.23_13hr.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.25/9.25_1-46.shm\n"
     ]
    }
   ],
   "source": [
    "person = \"lawler\"\n",
    "meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 31896, with 1438 positive samples and 30458 negative samples\n",
      "Test set size: 7974, with 359 positive samples and 7615 negative samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# method 1 to split train, test dataset\n",
    "samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "\n",
    "# split train set and test set\n",
    "train_indices, test_indices = split_train_test_indices(X= [i for i in range(len(labels))],\n",
    "                                                        y = labels, test_size = 0.2,\n",
    "                                                       random_seed = random_seed)\n",
    "# balance train set\n",
    "trainset_labels = labels[train_indices]\n",
    "train_indices_balanced = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "\n",
    "# split validation set\n",
    "# balanced_trainset_labels = labels[train_indices_balanced]\n",
    "# train_indices, valid_indices = split_train_test_indices(X= train_indices_balanced,\n",
    "#                                                         y = balanced_trainset_labels, test_size = 0.2,\n",
    "#                                                        random_seed = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set(train_indices)\n",
    "b = set(test_indices)\n",
    "a.intersection(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2 to split train, test dataset\n",
    "# # load\n",
    "# trainingsamples,traininglabels =  meal_data_train.data_indices, meal_data_train.labels\n",
    "# testsamples,testlabels =  meal_data_test.data_indices, meal_data_test.labels\n",
    "\n",
    "# train_shuffledUnderSampledBalancedIndices = balance_data_indices(traininglabels,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "# test_shuffledUnderSampledBalancedIndices = balance_data_indices(testlabels,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "# print(\"Train set size: %d, Test set size: %d\"%(len(train_shuffledUnderSampledBalancedIndices),len(test_shuffledUnderSampledBalancedIndices)))\n",
    "# train_set_balanced = torch.utils.data.Subset(meal_data_train, train_indices)\n",
    "# valid_set_balanced = torch.utils.data.Subset(meal_data_train, valid_indices)\n",
    "# # test_set_balanced = torch.utils.data.Subset(meal_data_test, test_shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "# valid_loader = torch.utils.data.DataLoader(valid_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "# # test_loader = torch.utils.data.DataLoader(test_set_balanced ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "# print(\"Data Loader Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loader Created\n"
     ]
    }
   ],
   "source": [
    "train_set_balanced = torch.utils.data.Subset(meal_data, train_indices)\n",
    "# valid_set_balanced = torch.utils.data.Subset(meal_data, valid_indices)\n",
    "\n",
    "test_set = torch.utils.data.Subset(meal_data, test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "# valid_loader = torch.utils.data.DataLoader(valid_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "print(\"Data Loader Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(dataset, data_indices, model,n_epochs=30,k=5, device=\"cpu\", random_state = 1000, checkpoint_path = \"./\"  ):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "    best_val_score = 0\n",
    "    overall_best_model = None\n",
    "    best_fold = None\n",
    "    all_loss_ls = []\n",
    "    all_train_acc_ls = []\n",
    "    all_valid_acc_ls = []\n",
    "    data_indices = np.array(data_indices)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k)\n",
    "    \n",
    "    labels = dataset.labels[data_indices]\n",
    "    np.random.seed(random_state)\n",
    "    seeds = np.random.randint(low=0, high=1000,size=k)\n",
    "    \n",
    "    \n",
    "    for fold_ind, (train_fold, valid_fold) in enumerate(skf.split(data_indices, labels)):\n",
    "        torch.manual_seed(seeds[fold_ind])\n",
    "        \n",
    "        print(\"===========================> Running Fold: %d\"%(fold_ind))\n",
    "        print()\n",
    "        train_indices = data_indices[train_fold]\n",
    "        valid_indices = data_indices[valid_fold]\n",
    "        # Train set    \n",
    "        train_set_fold = torch.utils.data.Subset(dataset, train_indices)\n",
    "        train_loader_fold = torch.utils.data.DataLoader(train_set_fold,batch_size=32, shuffle=True)\n",
    "\n",
    "        # validation set\n",
    "        valid_set_fold = torch.utils.data.Subset(dataset, valid_indices)\n",
    "        valid_loader_fold = torch.utils.data.DataLoader(valid_set_fold,batch_size=32, shuffle=True)\n",
    "          \n",
    "        # Re-initialize models\n",
    "        cv_model = model\n",
    "        # Since I use a dynamic created layer in network, need to input a sample to initialize the model first\n",
    "        cv_model.apply(weights_init)\n",
    "        cv_model.to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        optimizer = optim.Adam(cv_model.parameters(),lr=0.01,  weight_decay=0.1)\n",
    "        lrscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',patience= 2, factor = 0.1,threshold=0.01)\n",
    "\n",
    "\n",
    "        dataloader = (train_loader_fold,valid_loader_fold )\n",
    "        cv_model, best_model,val_score,loss_ls, train_acc_ls, valid_acc_ls = train_model(cv_model,dataloader, optimizer, criterion, \n",
    "                                                                                      lrscheduler, device= device,\n",
    "                                                                            n_epochs=n_epochs, patience = 5, l1_enabled=False,\n",
    "                                                                            checkpoint_name =checkpoint_path+\"cross_valid_checkpoint_\"+str(fold_ind)+\".pt\")\n",
    "        best_model.eval()\n",
    "        valid_acc, recall = eval_model(best_model, valid_loader_fold,device)\n",
    "        \n",
    "        all_valid_acc_ls.append(valid_acc)\n",
    "        \n",
    "        print(\"Fold %d Completed\"%(fold_ind))\n",
    "    print(\"Cross Validation Completed，score is %.4f %%\"%( np.mean(all_valid_acc_ls)))\n",
    "    \n",
    "    return all_valid_acc_ls\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================> Running Fold: 0\n",
      "\n",
      "Training set batch amounts: 72\n",
      "Test set : 18\n",
      "Start Training..\n",
      "Epoch: 0,  Epoch_Loss: 38.4488, Train Acc: 55.3043 %, Train Recall: 0.5591, Validation Acc:  61.1111 %,  Validation Recall: 0.5000  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 1,  Epoch_Loss: 6.0468, Train Acc: 62.0870 %, Train Recall: 0.6191, Validation Acc:  65.4514 %,  Validation Recall: 0.7812  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 2,  Epoch_Loss: 2.0216, Train Acc: 66.2609 %, Train Recall: 0.6583, Validation Acc:  55.3819 %,  Validation Recall: 0.8576  \n",
      "\n",
      "\n",
      "Epoch: 3,  Epoch_Loss: 1.0348, Train Acc: 68.4783 %, Train Recall: 0.6922, Validation Acc:  54.8611 %,  Validation Recall: 0.1354  \n",
      "\n",
      "\n",
      "Epoch: 4,  Epoch_Loss: 0.6697, Train Acc: 72.2609 %, Train Recall: 0.7183, Validation Acc:  67.5347 %,  Validation Recall: 0.6806  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 5,  Epoch_Loss: 0.5622, Train Acc: 73.5217 %, Train Recall: 0.7374, Validation Acc:  70.3125 %,  Validation Recall: 0.8646  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 6,  Epoch_Loss: 0.4196, Train Acc: 80.8261 %, Train Recall: 0.8113, Validation Acc:  76.7361 %,  Validation Recall: 0.8264  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 7,  Epoch_Loss: 0.4113, Train Acc: 81.7391 %, Train Recall: 0.8183, Validation Acc:  73.4375 %,  Validation Recall: 0.7083  \n",
      "\n",
      "\n",
      "Epoch: 8,  Epoch_Loss: 0.3947, Train Acc: 82.7391 %, Train Recall: 0.8296, Validation Acc:  74.8264 %,  Validation Recall: 0.7500  \n",
      "\n",
      "\n",
      "Epoch: 9,  Epoch_Loss: 0.3744, Train Acc: 84.7391 %, Train Recall: 0.8470, Validation Acc:  76.0417 %,  Validation Recall: 0.8090  \n",
      "\n",
      "\n",
      "Epoch: 10,  Epoch_Loss: 0.3691, Train Acc: 85.1304 %, Train Recall: 0.8574, Validation Acc:  75.6944 %,  Validation Recall: 0.7778  \n",
      "\n",
      "\n",
      "Epoch: 11,  Epoch_Loss: 0.3689, Train Acc: 84.9130 %, Train Recall: 0.8470, Validation Acc:  76.0417 %,  Validation Recall: 0.8160  \n",
      "\n",
      "\n",
      "Epoch: 12,  Epoch_Loss: 0.3646, Train Acc: 84.7391 %, Train Recall: 0.8626, Validation Acc:  75.6944 %,  Validation Recall: 0.7917  \n",
      "\n",
      "\n",
      "Epoch: 13,  Epoch_Loss: 0.3621, Train Acc: 84.5652 %, Train Recall: 0.8635, Validation Acc:  75.1736 %,  Validation Recall: 0.7743  \n",
      "\n",
      "\n",
      "Epoch: 14,  Epoch_Loss: 0.3679, Train Acc: 84.3478 %, Train Recall: 0.8565, Validation Acc:  75.3472 %,  Validation Recall: 0.7812  \n",
      "\n",
      "\n",
      "Epoch: 15,  Epoch_Loss: 0.3658, Train Acc: 85.1739 %, Train Recall: 0.8548, Validation Acc:  75.6944 %,  Validation Recall: 0.7986  \n",
      "\n",
      "\n",
      "Epoch: 16,  Epoch_Loss: 0.3658, Train Acc: 85.3913 %, Train Recall: 0.8548, Validation Acc:  76.3889 %,  Validation Recall: 0.8056  \n",
      "\n",
      "\n",
      "Epoch: 17,  Epoch_Loss: 0.3596, Train Acc: 85.3913 %, Train Recall: 0.8652, Validation Acc:  75.5208 %,  Validation Recall: 0.7743  \n",
      "\n",
      "\n",
      "Epoch: 18,  Epoch_Loss: 0.3669, Train Acc: 84.7391 %, Train Recall: 0.8548, Validation Acc:  76.7361 %,  Validation Recall: 0.8194  \n",
      "\n",
      "\n",
      "Epoch: 19,  Epoch_Loss: 0.3638, Train Acc: 85.0435 %, Train Recall: 0.8583, Validation Acc:  75.6944 %,  Validation Recall: 0.7847  \n",
      "\n",
      "\n",
      "Load Best Model.\n",
      "Training completed\n",
      "Fold 0 Completed\n",
      "===========================> Running Fold: 1\n",
      "\n",
      "Training set batch amounts: 72\n",
      "Test set : 18\n",
      "Start Training..\n",
      "Epoch: 0,  Epoch_Loss: 19.1095, Train Acc: 57.7140 %, Train Recall: 0.5778, Validation Acc:  62.9565 %,  Validation Recall: 0.8537  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 1,  Epoch_Loss: 3.5361, Train Acc: 63.3638 %, Train Recall: 0.6386, Validation Acc:  60.1739 %,  Validation Recall: 0.6585  \n",
      "\n",
      "\n",
      "Epoch: 2,  Epoch_Loss: 1.5745, Train Acc: 63.8853 %, Train Recall: 0.6360, Validation Acc:  61.7391 %,  Validation Recall: 0.7770  \n",
      "\n",
      "\n",
      "Epoch: 3,  Epoch_Loss: 0.8790, Train Acc: 67.1013 %, Train Recall: 0.6751, Validation Acc:  62.0870 %,  Validation Recall: 0.4077  \n",
      "\n",
      "\n",
      "Epoch: 4,  Epoch_Loss: 0.7206, Train Acc: 67.4924 %, Train Recall: 0.6829, Validation Acc:  70.4348 %,  Validation Recall: 0.7561  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 5,  Epoch_Loss: 0.4868, Train Acc: 76.4016 %, Train Recall: 0.7628, Validation Acc:  70.0870 %,  Validation Recall: 0.6655  \n",
      "\n",
      "\n",
      "Epoch: 6,  Epoch_Loss: 0.4482, Train Acc: 79.2699 %, Train Recall: 0.7837, Validation Acc:  70.6087 %,  Validation Recall: 0.7596  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 7,  Epoch_Loss: 0.4428, Train Acc: 79.0091 %, Train Recall: 0.7811, Validation Acc:  71.3043 %,  Validation Recall: 0.7003  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 8,  Epoch_Loss: 0.4105, Train Acc: 82.0078 %, Train Recall: 0.8063, Validation Acc:  71.3043 %,  Validation Recall: 0.6864  \n",
      "\n",
      "\n",
      "Epoch: 9,  Epoch_Loss: 0.4189, Train Acc: 80.5737 %, Train Recall: 0.8010, Validation Acc:  70.9565 %,  Validation Recall: 0.6585  \n",
      "\n",
      "\n",
      "Epoch: 10,  Epoch_Loss: 0.4080, Train Acc: 82.1817 %, Train Recall: 0.8132, Validation Acc:  72.1739 %,  Validation Recall: 0.6655  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 11,  Epoch_Loss: 0.4099, Train Acc: 81.8774 %, Train Recall: 0.8054, Validation Acc:  71.3043 %,  Validation Recall: 0.6481  \n",
      "\n",
      "\n",
      "Epoch: 12,  Epoch_Loss: 0.4130, Train Acc: 81.6167 %, Train Recall: 0.8036, Validation Acc:  71.6522 %,  Validation Recall: 0.6899  \n",
      "\n",
      "\n",
      "Epoch: 13,  Epoch_Loss: 0.4086, Train Acc: 82.1382 %, Train Recall: 0.8141, Validation Acc:  71.6522 %,  Validation Recall: 0.6899  \n",
      "\n",
      "\n",
      "Epoch: 14,  Epoch_Loss: 0.4090, Train Acc: 81.4863 %, Train Recall: 0.8019, Validation Acc:  71.1304 %,  Validation Recall: 0.6899  \n",
      "\n",
      "\n",
      "Epoch: 15,  Epoch_Loss: 0.4177, Train Acc: 81.3125 %, Train Recall: 0.8028, Validation Acc:  71.8261 %,  Validation Recall: 0.6655  \n",
      "\n",
      "\n",
      "Epoch: 16,  Epoch_Loss: 0.4114, Train Acc: 81.7036 %, Train Recall: 0.8071, Validation Acc:  71.3043 %,  Validation Recall: 0.6725  \n",
      "\n",
      "\n",
      "Epoch: 17,  Epoch_Loss: 0.4098, Train Acc: 82.6163 %, Train Recall: 0.8149, Validation Acc:  71.6522 %,  Validation Recall: 0.6551  \n",
      "\n",
      "\n",
      "Epoch: 18,  Epoch_Loss: 0.4087, Train Acc: 82.0947 %, Train Recall: 0.8123, Validation Acc:  71.8261 %,  Validation Recall: 0.6760  \n",
      "\n",
      "\n",
      "Epoch: 19,  Epoch_Loss: 0.4039, Train Acc: 82.2686 %, Train Recall: 0.8115, Validation Acc:  70.7826 %,  Validation Recall: 0.6620  \n",
      "\n",
      "\n",
      "Load Best Model.\n",
      "Training completed\n",
      "Fold 1 Completed\n",
      "===========================> Running Fold: 2\n",
      "\n",
      "Training set batch amounts: 72\n",
      "Test set : 18\n",
      "Start Training..\n",
      "Epoch: 0,  Epoch_Loss: 28.2127, Train Acc: 52.9770 %, Train Recall: 0.5222, Validation Acc:  61.2174 %,  Validation Recall: 0.8711  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 1,  Epoch_Loss: 4.1804, Train Acc: 61.4950 %, Train Recall: 0.6134, Validation Acc:  67.1304 %,  Validation Recall: 0.7979  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 2,  Epoch_Loss: 1.5641, Train Acc: 68.1443 %, Train Recall: 0.6872, Validation Acc:  60.3478 %,  Validation Recall: 0.3240  \n",
      "\n",
      "\n",
      "Epoch: 3,  Epoch_Loss: 0.7832, Train Acc: 71.5341 %, Train Recall: 0.7081, Validation Acc:  71.6522 %,  Validation Recall: 0.6516  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 4,  Epoch_Loss: 0.6282, Train Acc: 71.7080 %, Train Recall: 0.7237, Validation Acc:  72.3478 %,  Validation Recall: 0.6516  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 5,  Epoch_Loss: 0.4991, Train Acc: 76.4450 %, Train Recall: 0.7689, Validation Acc:  72.3478 %,  Validation Recall: 0.8084  \n",
      "\n",
      "\n",
      "Epoch: 6,  Epoch_Loss: 0.4306, Train Acc: 81.4863 %, Train Recall: 0.8262, Validation Acc:  78.6087 %,  Validation Recall: 0.8502  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 7,  Epoch_Loss: 0.4056, Train Acc: 82.0078 %, Train Recall: 0.8306, Validation Acc:  77.9130 %,  Validation Recall: 0.8153  \n",
      "\n",
      "\n",
      "Epoch: 8,  Epoch_Loss: 0.3813, Train Acc: 84.4850 %, Train Recall: 0.8619, Validation Acc:  78.4348 %,  Validation Recall: 0.7909  \n",
      "\n",
      "\n",
      "Epoch: 9,  Epoch_Loss: 0.3635, Train Acc: 84.7023 %, Train Recall: 0.8497, Validation Acc:  78.7826 %,  Validation Recall: 0.8258  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 10,  Epoch_Loss: 0.3590, Train Acc: 85.3542 %, Train Recall: 0.8723, Validation Acc:  79.3043 %,  Validation Recall: 0.8293  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 11,  Epoch_Loss: 0.3648, Train Acc: 84.8327 %, Train Recall: 0.8705, Validation Acc:  78.0870 %,  Validation Recall: 0.8014  \n",
      "\n",
      "\n",
      "Epoch: 12,  Epoch_Loss: 0.3552, Train Acc: 85.6584 %, Train Recall: 0.8445, Validation Acc:  79.3043 %,  Validation Recall: 0.7979  \n",
      "\n",
      "\n",
      "Epoch: 13,  Epoch_Loss: 0.3585, Train Acc: 86.0061 %, Train Recall: 0.8697, Validation Acc:  78.9565 %,  Validation Recall: 0.7909  \n",
      "\n",
      "\n",
      "Epoch: 14,  Epoch_Loss: 0.3594, Train Acc: 85.7888 %, Train Recall: 0.8688, Validation Acc:  78.7826 %,  Validation Recall: 0.7909  \n",
      "\n",
      "\n",
      "Epoch: 15,  Epoch_Loss: 0.3571, Train Acc: 85.2673 %, Train Recall: 0.8723, Validation Acc:  78.4348 %,  Validation Recall: 0.7805  \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16,  Epoch_Loss: 0.3548, Train Acc: 85.8322 %, Train Recall: 0.8749, Validation Acc:  78.2609 %,  Validation Recall: 0.8258  \n",
      "\n",
      "\n",
      "Epoch: 17,  Epoch_Loss: 0.3596, Train Acc: 86.1365 %, Train Recall: 0.8801, Validation Acc:  78.4348 %,  Validation Recall: 0.8328  \n",
      "\n",
      "\n",
      "Epoch: 18,  Epoch_Loss: 0.3574, Train Acc: 85.3977 %, Train Recall: 0.8671, Validation Acc:  78.7826 %,  Validation Recall: 0.8049  \n",
      "\n",
      "\n",
      "Epoch: 19,  Epoch_Loss: 0.3534, Train Acc: 86.0061 %, Train Recall: 0.8775, Validation Acc:  80.0000 %,  Validation Recall: 0.8293  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Load Best Model.\n",
      "Training completed\n",
      "Fold 2 Completed\n",
      "===========================> Running Fold: 3\n",
      "\n",
      "Training set batch amounts: 72\n",
      "Test set : 18\n",
      "Start Training..\n",
      "Epoch: 0,  Epoch_Loss: 25.5756, Train Acc: 56.1930 %, Train Recall: 0.5565, Validation Acc:  57.5652 %,  Validation Recall: 0.8194  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 1,  Epoch_Loss: 5.8759, Train Acc: 61.3212 %, Train Recall: 0.6165, Validation Acc:  57.3913 %,  Validation Recall: 0.4722  \n",
      "\n",
      "\n",
      "Epoch: 2,  Epoch_Loss: 1.7779, Train Acc: 66.2321 %, Train Recall: 0.6652, Validation Acc:  59.8261 %,  Validation Recall: 0.5903  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 3,  Epoch_Loss: 1.0137, Train Acc: 66.7970 %, Train Recall: 0.6652, Validation Acc:  57.7391 %,  Validation Recall: 0.6319  \n",
      "\n",
      "\n",
      "Epoch: 4,  Epoch_Loss: 0.5673, Train Acc: 74.5328 %, Train Recall: 0.7591, Validation Acc:  61.0435 %,  Validation Recall: 0.6215  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 5,  Epoch_Loss: 0.5148, Train Acc: 75.4020 %, Train Recall: 0.7626, Validation Acc:  62.0870 %,  Validation Recall: 0.6111  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 6,  Epoch_Loss: 0.4961, Train Acc: 76.8796 %, Train Recall: 0.7765, Validation Acc:  61.9130 %,  Validation Recall: 0.6354  \n",
      "\n",
      "\n",
      "Epoch: 7,  Epoch_Loss: 0.4728, Train Acc: 78.2703 %, Train Recall: 0.7939, Validation Acc:  61.7391 %,  Validation Recall: 0.6111  \n",
      "\n",
      "\n",
      "Epoch: 8,  Epoch_Loss: 0.4704, Train Acc: 77.5750 %, Train Recall: 0.7991, Validation Acc:  62.0870 %,  Validation Recall: 0.5833  \n",
      "\n",
      "\n",
      "Epoch: 9,  Epoch_Loss: 0.4706, Train Acc: 78.0530 %, Train Recall: 0.7974, Validation Acc:  61.2174 %,  Validation Recall: 0.5868  \n",
      "\n",
      "\n",
      "Epoch: 10,  Epoch_Loss: 0.4673, Train Acc: 77.7488 %, Train Recall: 0.7896, Validation Acc:  60.5217 %,  Validation Recall: 0.5938  \n",
      "\n",
      "\n",
      "Epoch: 11,  Epoch_Loss: 0.4677, Train Acc: 77.9226 %, Train Recall: 0.7974, Validation Acc:  60.8696 %,  Validation Recall: 0.5868  \n",
      "\n",
      "\n",
      "Epoch: 12,  Epoch_Loss: 0.4627, Train Acc: 78.7484 %, Train Recall: 0.7983, Validation Acc:  61.2174 %,  Validation Recall: 0.5903  \n",
      "\n",
      "\n",
      "Epoch: 13,  Epoch_Loss: 0.4666, Train Acc: 78.5311 %, Train Recall: 0.7930, Validation Acc:  61.0435 %,  Validation Recall: 0.5938  \n",
      "\n",
      "\n",
      "Epoch: 14,  Epoch_Loss: 0.4608, Train Acc: 78.6180 %, Train Recall: 0.7991, Validation Acc:  61.0435 %,  Validation Recall: 0.5868  \n",
      "\n",
      "\n",
      "Epoch: 15,  Epoch_Loss: 0.4625, Train Acc: 79.0091 %, Train Recall: 0.8070, Validation Acc:  61.7391 %,  Validation Recall: 0.5972  \n",
      "\n",
      "\n",
      "Epoch: 16,  Epoch_Loss: 0.4624, Train Acc: 78.4007 %, Train Recall: 0.7878, Validation Acc:  61.3913 %,  Validation Recall: 0.6007  \n",
      "\n",
      "\n",
      "Epoch: 17,  Epoch_Loss: 0.4543, Train Acc: 78.6180 %, Train Recall: 0.7930, Validation Acc:  61.5652 %,  Validation Recall: 0.5938  \n",
      "\n",
      "\n",
      "Epoch: 18,  Epoch_Loss: 0.4616, Train Acc: 78.3572 %, Train Recall: 0.7922, Validation Acc:  62.0870 %,  Validation Recall: 0.5972  \n",
      "\n",
      "\n",
      "Epoch: 19,  Epoch_Loss: 0.4597, Train Acc: 78.9222 %, Train Recall: 0.8017, Validation Acc:  61.5652 %,  Validation Recall: 0.5972  \n",
      "\n",
      "\n",
      "Load Best Model.\n",
      "Training completed\n",
      "Fold 3 Completed\n",
      "===========================> Running Fold: 4\n",
      "\n",
      "Training set batch amounts: 72\n",
      "Test set : 18\n",
      "Start Training..\n",
      "Epoch: 0,  Epoch_Loss: 27.0979, Train Acc: 53.8462 %, Train Recall: 0.5496, Validation Acc:  57.2174 %,  Validation Recall: 0.4861  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 1,  Epoch_Loss: 4.5048, Train Acc: 62.7553 %, Train Recall: 0.6183, Validation Acc:  59.3043 %,  Validation Recall: 0.4306  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 2,  Epoch_Loss: 1.4662, Train Acc: 64.5372 %, Train Recall: 0.6478, Validation Acc:  58.4348 %,  Validation Recall: 0.5069  \n",
      "\n",
      "\n",
      "Epoch: 3,  Epoch_Loss: 0.7928, Train Acc: 66.4059 %, Train Recall: 0.6687, Validation Acc:  63.6522 %,  Validation Recall: 0.4861  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 4,  Epoch_Loss: 0.5649, Train Acc: 72.5771 %, Train Recall: 0.7339, Validation Acc:  65.2174 %,  Validation Recall: 0.6354  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 5,  Epoch_Loss: 0.5377, Train Acc: 73.2725 %, Train Recall: 0.7400, Validation Acc:  66.2609 %,  Validation Recall: 0.6910  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 6,  Epoch_Loss: 0.5350, Train Acc: 74.2286 %, Train Recall: 0.7383, Validation Acc:  66.6087 %,  Validation Recall: 0.6667  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 7,  Epoch_Loss: 0.5217, Train Acc: 74.6197 %, Train Recall: 0.7765, Validation Acc:  67.6522 %,  Validation Recall: 0.6771  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 8,  Epoch_Loss: 0.5259, Train Acc: 74.4894 %, Train Recall: 0.7687, Validation Acc:  66.9565 %,  Validation Recall: 0.6458  \n",
      "\n",
      "\n",
      "Epoch: 9,  Epoch_Loss: 0.5131, Train Acc: 75.4889 %, Train Recall: 0.7713, Validation Acc:  67.4783 %,  Validation Recall: 0.6667  \n",
      "\n",
      "\n",
      "Epoch: 10,  Epoch_Loss: 0.5155, Train Acc: 75.4020 %, Train Recall: 0.7748, Validation Acc:  66.7826 %,  Validation Recall: 0.6667  \n",
      "\n",
      "\n",
      "Epoch: 11,  Epoch_Loss: 0.5115, Train Acc: 75.0978 %, Train Recall: 0.7687, Validation Acc:  66.7826 %,  Validation Recall: 0.6806  \n",
      "\n",
      "\n",
      "Epoch: 12,  Epoch_Loss: 0.5191, Train Acc: 74.4894 %, Train Recall: 0.7617, Validation Acc:  66.6087 %,  Validation Recall: 0.6771  \n",
      "\n",
      "\n",
      "Epoch: 13,  Epoch_Loss: 0.5134, Train Acc: 75.0109 %, Train Recall: 0.7617, Validation Acc:  67.3043 %,  Validation Recall: 0.6667  \n",
      "\n",
      "\n",
      "Epoch: 14,  Epoch_Loss: 0.5195, Train Acc: 74.1851 %, Train Recall: 0.7617, Validation Acc:  66.7826 %,  Validation Recall: 0.6910  \n",
      "\n",
      "\n",
      "Epoch: 15,  Epoch_Loss: 0.5133, Train Acc: 75.3151 %, Train Recall: 0.7652, Validation Acc:  67.8261 %,  Validation Recall: 0.6875  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 16,  Epoch_Loss: 0.5153, Train Acc: 74.8370 %, Train Recall: 0.7696, Validation Acc:  67.1304 %,  Validation Recall: 0.6667  \n",
      "\n",
      "\n",
      "Epoch: 17,  Epoch_Loss: 0.5112, Train Acc: 75.3151 %, Train Recall: 0.7704, Validation Acc:  66.0870 %,  Validation Recall: 0.6597  \n",
      "\n",
      "\n",
      "Epoch: 18,  Epoch_Loss: 0.5138, Train Acc: 75.2282 %, Train Recall: 0.7696, Validation Acc:  66.6087 %,  Validation Recall: 0.6771  \n",
      "\n",
      "\n",
      "Epoch: 19,  Epoch_Loss: 0.5160, Train Acc: 75.0109 %, Train Recall: 0.7713, Validation Acc:  66.7826 %,  Validation Recall: 0.6667  \n",
      "\n",
      "\n",
      "Load Best Model.\n",
      "Training completed\n",
      "Fold 4 Completed\n",
      "Cross Validation Completed，score is 71.7646 %\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "input_shape = (train_set_balanced[0][0].shape[0], train_set_balanced[0][0].shape[1])\n",
    "model = Discriminator_ResNet( ngpu=1, input_shape =input_shape , out_fea = 1)\n",
    "_ = model(torch.rand((1, input_shape[0],input_shape[1])))\n",
    "model.to(device)\n",
    "score = cross_validation(meal_data, train_indices_balanced,model,\n",
    "                n_epochs=20,k=5, device=device, random_state = 1000, checkpoint_path = \"../models/torch_models/\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[76.73611111111111,\n",
       " 72.17391304347827,\n",
       " 80.0,\n",
       " 62.08695652173913,\n",
       " 67.82608695652173]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 2300, with 1150 positive samples and 1150 negative samples\n",
      "Test set size: 576, with 288 positive samples and 288 negative samples\n",
      "Training set batch amounts: 499\n",
      "Test set : 9\n",
      "Start Training..\n",
      "Epoch: 0,  Epoch_Loss: 0.2265, Train Acc: 95.3411 %, Train Recall: 0.0035, Validation Acc:  50.0000 %,  Validation Recall: 0.0000  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 1,  Epoch_Loss: 0.1870, Train Acc: 95.4916 %, Train Recall: 0.0000, Validation Acc:  50.0000 %,  Validation Recall: 0.0000  \n",
      "\n",
      "\n",
      "Epoch: 2,  Epoch_Loss: 0.1870, Train Acc: 95.4916 %, Train Recall: 0.0000, Validation Acc:  50.0000 %,  Validation Recall: 0.0000  \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-58760466c7a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mlrscheduler_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m model_1, best_model_1,val_score_1,loss_ls_1, train_acc_ls_1, valid_acc_ls_1 = train_model(model_1,dataloader, optimizer_1, \n\u001b[0m\u001b[1;32m     21\u001b[0m                                                                     \u001b[0mcriterion_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrscheduler_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                                     \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_enabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Thesis-Research/src/utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, criterion, lrscheduler, device, n_epochs, earlystopping, patience, l1_enabled, checkpoint_name)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mFN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecv_handle\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m'''Receive a handle over a local connection.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_UNIX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecvfds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDupFd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecvfds\u001b[0;34m(sock, size)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mbytes_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecvmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMSG_SPACE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# split validation set\n",
    "balanced_trainset_labels = labels[train_indices_balanced]\n",
    "train_indices, valid_indices = split_train_test_indices(X= train_indices_balanced,\n",
    "                                                        y = balanced_trainset_labels, test_size = 0.2,\n",
    "                                                       random_seed = random_seed)\n",
    "\n",
    "valid_set_balanced = torch.utils.data.Subset(meal_data, valid_indices)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "# train model\n",
    "input_shape = (train_set_balanced[0][0].shape[0], train_set_balanced[0][0].shape[1])\n",
    "model_1 = Discriminator_ResNet( ngpu=1, input_shape =input_shape , out_fea = 1)\n",
    "_ = model_1(torch.rand((1, input_shape[0],input_shape[1])))\n",
    "model_1.to(device)\n",
    "criterion_1 = nn.BCEWithLogitsLoss()\n",
    "optimizer_1 = optim.Adam(model_1.parameters(),lr=0.01,  weight_decay=0.1)\n",
    "lrscheduler_1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_1, mode='min',patience= 2, factor = 0.1,threshold=0.01)\n",
    "dataloader = (train_loader, valid_loader)\n",
    "model_1, best_model_1,val_score_1,loss_ls_1, train_acc_ls_1, valid_acc_ls_1 = train_model(model_1,dataloader, optimizer_1, \n",
    "                                                                    criterion_1, lrscheduler_1, device= device,\n",
    "                                                                    n_epochs=30, patience = 5, l1_enabled=False,\n",
    "                                                                    checkpoint_name =\"../models/lawler_models/checkpoint_model_resnet.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_1.eval()\n",
    "acc, recall = eval_model(best_model_1, test_loader,device)\n",
    "print(\"Test Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/adam-data/09-22-2020/09-22-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/10-28-2020/10-28-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-09-2020/11-09-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-11-2020/11-11-2020-1.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-12-2020/11-12-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-16-2020/11-16-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-17-2020/11-17-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-24-2020/11-24-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-25-2020/11-25-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-26-2020/11-26-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-30-2020/11-30-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-01-2020/12-01-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-02-2020/12-02-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-03-2020/12-03-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-04-2020/12-04-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-07-2020/12-07-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-08-2020/12-08-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-09-2020/12-09-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-10-2020/12-10-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-11-2020/12-11-2020.shm\n",
      "Train set size: 32253, with 887 positive samples and 31366 negative samples\n",
      "Test set size: 8064, with 222 positive samples and 7842 negative samples\n"
     ]
    }
   ],
   "source": [
    "person = \"adam\"\n",
    "meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "# split train set and test set\n",
    "train_indices, test_indices = split_train_test_indices(X= [i for i in range(len(labels))],\n",
    "                                                        y = labels, test_size = 0.2,\n",
    "                                                       random_seed = random_seed)\n",
    "# balance train set\n",
    "trainset_labels = labels[train_indices]\n",
    "train_indices_balanced = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loader Created\n"
     ]
    }
   ],
   "source": [
    "train_set_balanced = torch.utils.data.Subset(meal_data, train_indices)\n",
    "\n",
    "\n",
    "test_set = torch.utils.data.Subset(meal_data, test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "print(\"Data Loader Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1419, with 709 positive samples and 710 negative samples\n",
      "Test set size: 355, with 178 positive samples and 177 negative samples\n",
      "Training set batch amounts: 504\n",
      "Test set : 6\n",
      "Start Training..\n",
      "Epoch: 0,  Epoch_Loss: 0.1595, Train Acc: 97.1786 %, Train Recall: 0.0011, Validation Acc:  49.8592 %,  Validation Recall: 0.0000  \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 1,  Epoch_Loss: 0.1258, Train Acc: 97.2499 %, Train Recall: 0.0000, Validation Acc:  49.8592 %,  Validation Recall: 0.0000  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "# split validation set\n",
    "balanced_trainset_labels = labels[train_indices_balanced]\n",
    "train_indices, valid_indices = split_train_test_indices(X= train_indices_balanced,\n",
    "                                                        y = balanced_trainset_labels, test_size = 0.2,\n",
    "                                                       random_seed = random_seed)\n",
    "\n",
    "valid_set_balanced = torch.utils.data.Subset(meal_data, valid_indices)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "# train model\n",
    "input_shape = (train_set_balanced[0][0].shape[0], train_set_balanced[0][0].shape[1])\n",
    "model_2 = Discriminator_ResNet( ngpu=1, input_shape =input_shape , out_fea = 1)\n",
    "_ = model_2(torch.rand((1, input_shape[0],input_shape[1])))\n",
    "model_2.to(device)\n",
    "criterion_2 = nn.BCEWithLogitsLoss()\n",
    "optimizer_2 = optim.Adam(model_2.parameters(),lr=0.01,  weight_decay=0.1)\n",
    "lrscheduler_2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_2, mode='min',patience= 2, factor = 0.1,threshold=0.01)\n",
    "dataloader = (train_loader, valid_loader)\n",
    "model_2, best_model_2,val_score_2,loss_ls_2, train_acc_ls_2, valid_acc_ls_2 = train_model(model_2,dataloader, optimizer_2, \n",
    "                                                                    criterion_2, lrscheduler_2, device= device,\n",
    "                                                                    n_epochs=30, patience = 5, l1_enabled=False,\n",
    "                                                                    checkpoint_name =\"../models/adam_models/checkpoint_model_resnet.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set batch amounts: 499\n",
      "Test set : 9\n",
      "Start Training..\n",
      "Epoch: 0,  Epoch_Loss: 0.2262, Train Acc: 95.3380 %, Train Recall: 0.0035 \n",
      "Validation Acc:  50.0000 %,  Validation Recall: 0.0000 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-34f0f41f34ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mlrscheduler_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m model_4, best_model_4,val_score_4,loss_ls_4, train_acc_ls_4, valid_acc_ls_4 = train_model(model_4,dataloader, optimizer_4, \n\u001b[0m\u001b[1;32m     17\u001b[0m                                                                     \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrscheduler_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                                     \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_enabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Thesis-Research/src/utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, criterion, lrscheduler, device, n_epochs, earlystopping, patience, l1_enabled, checkpoint_name)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mTP\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                     \u001b[0mFN\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from utils import *\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "input_shape = (train_set_balanced[0][0].shape[0], train_set_balanced[0][0].shape[1])\n",
    "model_4 = Discriminator_ResNet( ngpu=1, input_shape =input_shape , out_fea = 1)\n",
    "# Since I use a dynamic created layer in network, need to input a sample to initialize the model first\n",
    "_ = model_4(torch.rand((1, input_shape[0],input_shape[1])))\n",
    "model_4.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_4 = optim.Adam(model_4.parameters(),lr=0.01,  weight_decay=0.1)\n",
    "lrscheduler_4 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_4, mode='min',patience= 2, factor = 0.1,threshold=0.01)\n",
    "dataloader = (train_loader, valid_loader)\n",
    "model_4, best_model_4,val_score_4,loss_ls_4, train_acc_ls_4, valid_acc_ls_4 = train_model(model_4,dataloader, optimizer_4, \n",
    "                                                                    criterion, lrscheduler_4, device= device,\n",
    "                                                                    n_epochs=30, patience = 5, l1_enabled=False,\n",
    "                                                                    checkpoint_name =\"../models/l_models/checkpoint_model_resnet.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
