{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Individual Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from dataset import create_train_test_file_list, Person_MealsDataset, balance_data_indices\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device: \",device,\"Device Count: \", torch.cuda.device_count(), \"Device Name: \",torch.cuda.get_device_name()  )\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "### imports\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Turn off TensorFlow logging\n",
    "import tensorflow.compat.v1 as tf # maintain compatibility with TensorFlow 2.2.0\n",
    "\n",
    "import keras\n",
    "# from tensorflow.compat.v1.keras import backend as K # changed for compatibility with TensorFlow 2.2.0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "import loadfile\n",
    "import addons\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l1\n",
    "from keras.models import load_model, save_model\n",
    "\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "shimmer_global_mean = [-0.012359981,-0.0051663737,0.011612018,\n",
    "                        0.05796114,0.1477952,-0.034395125 ]\n",
    "\n",
    "shimmer_global_stddev = [0.05756385,0.040893298,0.043825723,\n",
    "                        17.199743,15.311142,21.229317 ]\n",
    "\n",
    "shimmer_trended_mean = [-0.000002,-0.000002,-0.000000,\n",
    "                0.058144,0.147621,-0.033260 ]\n",
    "\n",
    "shimmer_trended_stddev = [0.037592,0.034135,0.032263,\n",
    "                17.209038,15.321441,21.242532 ]\n",
    "\n",
    "all_zero_means = [0,0,0,0,0,0]\n",
    "\n",
    "meanvals = all_zero_means\n",
    "stdvals = shimmer_trended_stddev\n",
    "\n",
    "\n",
    "random_seed  = 1000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_settings(winmin,stridesec, EPOCHS):\n",
    "    outfile = sys.stdout\n",
    "\n",
    "    winlength = int(winmin * 60 * 15)\n",
    "    step = int(stridesec * 15)\n",
    "    start_time = datetime.now()\n",
    "    arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "          \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "          \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "\n",
    "    [os.system(cmd) for cmd in arr]\n",
    "    print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "    print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "    print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/18/2020, 00:17:50\n",
      "WindowLength: 1.00 min (900 datum)\tSlide: 1 (15 datum)\tEpochs:30\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.19/10.19.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/afternoon_2hr33min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/evening_2hr_20min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.21/10.21.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.26/10.26.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.28/10.28.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/evening-2hr_goodDinnerTemplate_CFAmeal/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.4/10.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.6/10.6.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.8/10.8.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.11/11.11.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.14/11.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.15/11.15.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.2/11.2.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.3/11.3.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.4/11.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.23/9.23_13hr.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.25/9.25_1-46.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.10/10.10.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.12/10.12.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.14/10.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.16/10.16.shm\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "winmin = 1 \n",
    "stridesec = 1\n",
    "\n",
    "print_settings(winmin,stridesec, EPOCHS)\n",
    "# Load the dataset\n",
    "meal_data_train = Person_MealsDataset(person_name= \"lawler\", file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "meal_data_test = Person_MealsDataset(person_name= \"lawler\", file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.seed(random_seed)\n",
    "\n",
    "# load\n",
    "trainingsamples,traininglabels =  meal_data_train.data_indices, meal_data_train.labels\n",
    "testsamples,testlabels =  meal_data_test.data_indices, meal_data_test.labels\n",
    "\n",
    "train_shuffledUnderSampledBalancedIndices = balance_data_indices(traininglabels,mode=\"under\", shuffle=True,random_state = random_seed)\n",
    "test_shuffledUnderSampledBalancedIndices = balance_data_indices(testlabels,mode=\"under\", shuffle=True,random_state = random_seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loader Created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_set_balanced = torch.utils.data.Subset(meal_data_train, train_shuffledUnderSampledBalancedIndices)\n",
    "test_set_balanced = torch.utils.data.Subset(meal_data_test, test_shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set_balanced ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "print(\"Data Loader Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to check if random generated indices can be reproducted\n",
    "# ls = balance_data_indices(traininglabels,mode=\"under\", shuffle=True,random_state = random_seed)\n",
    "# ls ==train_shuffledUnderSampledBalancedIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set batch amounts: 357\n",
      "Test set : 70\n",
      "Start Training..\n",
      "Epoch: 0,  Epoch_Loss: 0.6271, Train Acc: 68.7224 %, Train Recall: 0.6884 \n",
      "Validation Acc:  62.9306 %,  Validation Recall: 0.5790 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 1,  Epoch_Loss: 0.5568, Train Acc: 72.2921 %, Train Recall: 0.6852 \n",
      "Validation Acc:  60.7606 %,  Validation Recall: 0.3725 \n",
      "\n",
      "\n",
      "Epoch: 2,  Epoch_Loss: 0.5497, Train Acc: 72.6315 %, Train Recall: 0.6837 \n",
      "Validation Acc:  51.3535 %,  Validation Recall: 0.0300 \n",
      "\n",
      "\n",
      "Epoch: 3,  Epoch_Loss: 0.5552, Train Acc: 72.0030 %, Train Recall: 0.6676 \n",
      "Validation Acc:  52.5280 %,  Validation Recall: 0.8284 \n",
      "\n",
      "\n",
      "Epoch: 4,  Epoch_Loss: 0.5523, Train Acc: 72.2154 %, Train Recall: 0.6776 \n",
      "Validation Acc:  57.6622 %,  Validation Recall: 0.1935 \n",
      "\n",
      "\n",
      "Epoch: 5,  Epoch_Loss: 0.5547, Train Acc: 72.3118 %, Train Recall: 0.6834 \n",
      "Validation Acc:  50.1119 %,  Validation Recall: 0.0022 \n",
      "\n",
      "\n",
      "Epoch: 6,  Epoch_Loss: 0.5605, Train Acc: 71.2934 %, Train Recall: 0.6712 \n",
      "Validation Acc:  50.0000 %,  Validation Recall: 0.0000 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-78c658a62bc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlrscheduler_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m model_4, best_model_4,val_score_4,loss_ls_4, train_acc_ls_4, valid_acc_ls_4 = train_model(model_4,dataloader, optimizer_4, \n\u001b[0m\u001b[1;32m     15\u001b[0m                                                                     \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrscheduler_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                                                     \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_enabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Thesis-Research/src/utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, criterion, lrscheduler, device, n_epochs, earlystopping, patience, l1_enabled, checkpoint_name)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# reshape samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m#print(\"Output: \", outputs, \"label: \", labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Thesis-Research/src/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    254\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 256\u001b[0;31m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    257\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from utils import *\n",
    "\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "input_shape = (train_set_balanced[0][0].shape[1], train_set_balanced[0][0].shape[0])\n",
    "model_4 = Discriminator_ResNet( ngpu=1, input_shape =input_shape , out_fea = 1).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_4 = optim.Adam(model_4.parameters(),lr=0.01,  weight_decay=0.1)\n",
    "lrscheduler_4 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_4, mode='min',patience= 2, factor = 0.1,threshold=0.01)\n",
    "dataloader = (train_loader, test_loader)\n",
    "model_4, best_model_4,val_score_4,loss_ls_4, train_acc_ls_4, valid_acc_ls_4 = train_model(model_4,dataloader, optimizer_4, \n",
    "                                                                    criterion, lrscheduler_4, device= device,\n",
    "                                                                    n_epochs=30, patience = 5, l1_enabled=False,\n",
    "                                                                    checkpoint_name =\"../models/lawler_models/checkpoint_model_resnet.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from utils import *\n",
    "\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "input_shape = (train_set_balanced[0][0].shape[1], train_set_balanced[0][0].shape[0])\n",
    "model_4 = Discriminator_ResNet( ngpu=1, input_shape =input_shape , out_fea = 1).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_4 = optim.Adam(model_4.parameters(),lr=0.01,  weight_decay=0.1)\n",
    "lrscheduler_4 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_4, mode='min',patience= 2, factor = 0.1,threshold=0.01)\n",
    "dataloader = (train_loader, test_loader)\n",
    "model_4, best_model_4,val_score_4,loss_ls_4, train_acc_ls_4, valid_acc_ls_4 = train_model(model_4,dataloader, optimizer_4, \n",
    "                                                                    criterion, lrscheduler_4, device= device,\n",
    "                                                                    n_epochs=30, patience = 5, l1_enabled=False,\n",
    "                                                                    checkpoint_name =\"../models/wenkanw_models/checkpoint_model_resnet.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
