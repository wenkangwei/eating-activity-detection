{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.8 µs ± 158 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "41.8 µs ± 571 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "The slowest run took 11.62 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1.99 µs ± 2.68 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "58.1 µs ± 175 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@nb.jit()\n",
    "def nb_sum(a):\n",
    "    Sum = 0\n",
    "    for i in range(len(a)):\n",
    "        Sum += a[i]\n",
    "    return Sum\n",
    "\n",
    "# 没用numba加速的求和函数\n",
    "def py_sum(a):\n",
    "    Sum = 0\n",
    "    for i in range(len(a)):\n",
    "        Sum += a[i]\n",
    "        \n",
    "    return Sum\n",
    "\n",
    "import numpy as np\n",
    "a = np.linspace(0,100,100) # 创建一个长度为100的数组\n",
    "\n",
    "%timeit np.sum(a) # numpy自带的求和函数\n",
    "%timeit sum(a) # python自带的求和函数\n",
    "%timeit nb_sum(a) # numba加速的求和函数\n",
    "%timeit py_sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9456139 , 0.75524947, 0.67912952, 0.22281563, 0.75102626,\n",
       "       0.03629475, 0.4868519 , 0.64976584, 0.9920782 , 0.84184192])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.94 ms ± 144 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame({\"data\": [i for i in range(10000)]})\n",
    "import random\n",
    "@nb.jit\n",
    "def func(df):\n",
    "    for i in range(len(df)):\n",
    "        df[i] = 0.1* np.random.sample(1)[0]*100\n",
    "    \n",
    "    return df\n",
    "%timeit new_df = func(df['data'].to_numpy())\n",
    "# df['data'].to_numpy()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor\t: 0\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 26\n",
      "model name\t: Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz\n",
      "stepping\t: 5\n",
      "microcode\t: 0x19\n",
      "cpu MHz\t\t: 2382.246\n",
      "cache size\t: 8192 KB\n",
      "physical id\t: 1\n",
      "siblings\t: 4\n",
      "core id\t\t: 0\n",
      "cpu cores\t: 4\n",
      "apicid\t\t: 16\n",
      "initial apicid\t: 16\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 11\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\n",
      "bogomips\t: 4521.58\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 40 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 1\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 26\n",
      "model name\t: Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz\n",
      "stepping\t: 5\n",
      "microcode\t: 0x19\n",
      "cpu MHz\t\t: 2515.259\n",
      "cache size\t: 8192 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 4\n",
      "core id\t\t: 0\n",
      "cpu cores\t: 4\n",
      "apicid\t\t: 0\n",
      "initial apicid\t: 0\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 11\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\n",
      "bogomips\t: 4521.23\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 40 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 2\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 26\n",
      "model name\t: Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz\n",
      "stepping\t: 5\n",
      "microcode\t: 0x19\n",
      "cpu MHz\t\t: 2459.379\n",
      "cache size\t: 8192 KB\n",
      "physical id\t: 1\n",
      "siblings\t: 4\n",
      "core id\t\t: 1\n",
      "cpu cores\t: 4\n",
      "apicid\t\t: 18\n",
      "initial apicid\t: 18\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 11\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\n",
      "bogomips\t: 4521.58\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 40 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 3\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 26\n",
      "model name\t: Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz\n",
      "stepping\t: 5\n",
      "microcode\t: 0x19\n",
      "cpu MHz\t\t: 2358.832\n",
      "cache size\t: 8192 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 4\n",
      "core id\t\t: 1\n",
      "cpu cores\t: 4\n",
      "apicid\t\t: 2\n",
      "initial apicid\t: 2\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 11\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\n",
      "bogomips\t: 4521.23\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 40 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 4\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 26\n",
      "model name\t: Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz\n",
      "stepping\t: 5\n",
      "microcode\t: 0x19\n",
      "cpu MHz\t\t: 2387.639\n",
      "cache size\t: 8192 KB\n",
      "physical id\t: 1\n",
      "siblings\t: 4\n",
      "core id\t\t: 2\n",
      "cpu cores\t: 4\n",
      "apicid\t\t: 20\n",
      "initial apicid\t: 20\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 11\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\n",
      "bogomips\t: 4521.58\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 40 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 5\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 26\n",
      "model name\t: Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz\n",
      "stepping\t: 5\n",
      "microcode\t: 0x19\n",
      "cpu MHz\t\t: 2385.375\n",
      "cache size\t: 8192 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 4\n",
      "core id\t\t: 2\n",
      "cpu cores\t: 4\n",
      "apicid\t\t: 4\n",
      "initial apicid\t: 4\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 11\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\n",
      "bogomips\t: 4521.23\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 40 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 6\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 26\n",
      "model name\t: Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz\n",
      "stepping\t: 5\n",
      "microcode\t: 0x19\n",
      "cpu MHz\t\t: 2458.829\n",
      "cache size\t: 8192 KB\n",
      "physical id\t: 1\n",
      "siblings\t: 4\n",
      "core id\t\t: 3\n",
      "cpu cores\t: 4\n",
      "apicid\t\t: 22\n",
      "initial apicid\t: 22\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 11\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\n",
      "bogomips\t: 4521.58\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 40 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 7\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 26\n",
      "model name\t: Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz\n",
      "stepping\t: 5\n",
      "microcode\t: 0x19\n",
      "cpu MHz\t\t: 2516.104\n",
      "cache size\t: 8192 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 4\n",
      "core id\t\t: 3\n",
      "cpu cores\t: 4\n",
      "apicid\t\t: 6\n",
      "initial apicid\t: 6\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 11\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm pti tpr_shadow vnmi flexpriority ept vpid dtherm ida\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\n",
      "bogomips\t: 4521.23\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 40 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Note:\n",
    "1. Default python  version is using /software/jupyterhub/env/jupyterhub-user/bin/python3.8, which is 3.8.5\n",
    "2. When add conda module, the python in conda (for example: conda use 3.6 python) will overlap the default python version, then python becomes 3.6\n",
    "3. Only after add conda module, Can I activate myenv\n",
    "4. After activate myenv conda environment, the python 3.7.7 in myenv will overlap the python 3.6 in conda again. \n",
    "\n",
    "5. So Make SURE that \n",
    "    + **add conda module (python in conda) first**\n",
    "    + **activate myenv (python 3.7.7 in myenv)**\n",
    "    + **DO NOT add conda module again in myenv!**\n",
    "    \n",
    "6. **CAN Not Use pd.to_csv() to  save cleaned dataset directly**, since it converts data into string and replace most of data with \"...\" string. It LOSES information!\n",
    "Even if we can use csv to store data without converting to string, it will take about 40G to store data again!\n",
    "7. **CAN Not Use pd.to_json() to save cleaned dataset directly as well**, although it stores array correctlly without converting it to string, it takes a long time to load data as well and large space (40G) to store data as well.\n",
    "\n",
    "8. **Final Solution:**\n",
    "    + Use the API I write to load batch of data rather than full data. \n",
    "    + Randomly sample part of them as training set, test set and save it into a json file for **Quick Use**\n",
    "    + For full data, **Use an API to load batch of them (recommanded for testing)** or **all of them into pandas dataframe(could take some time)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy to Load data quicker\n",
    "0. base case: use the load function I wrote to load data into array directly, but slow\n",
    "1. Use the load functions i wrote to load preprocessed data directly with **MPI to load faster**\n",
    "2. Save preprocessed data (data in list format, not array) to csv file. Then use literal_eval, or Spark UDF to load csv to parse **String data into list data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with my Loader Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 354/354 [03:18<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating balanced training data array\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119024/119024 [14:10<00:00, 139.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Testing with modules\n",
    "\n",
    "from data_loader import load_dataset, load_PreprocessData,all_zero_means, shimmer_trended_stddev\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "ModelNum = 3 #int(sys.argv[1])\n",
    "EPOCHS = 30 #int(sys.argv[2])\n",
    "winmin = 6 #int(sys.argv[1])\n",
    "stridesec = 15 # int(sys.argv[4])\n",
    "\n",
    "winlength = int(winmin * 60 * 15)\n",
    "step = int(stridesec * 15)\n",
    "start_time = datetime.now()\n",
    "# data_path ='data-list.txt'\n",
    "data_path ='batch-unix.txt'\n",
    "meanvals, stdvals =  all_zero_means, shimmer_trended_stddev\n",
    "NumFiles, data, samples_indices, labels_array = load_PreprocessData(winlength,\n",
    "                                                                            step,\n",
    "                                                                            meanvals, \n",
    "                                                                            stdvals,\n",
    "                                                                            files_list=data_path, \n",
    "                                                                            removerest=0,\n",
    "                                                                            removewalk=0,\n",
    "                                                                            removebias=0)\n",
    "\n",
    "train_set = load_dataset(data, samples_indices, labels_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2206243e-02,  2.0161411e-02,  3.6075068e-01,  4.2981387e-04,\n",
       "        -1.6510699e-04, -3.7603444e-04],\n",
       "       [-1.2820247e-02,  2.4278400e-02,  3.5645226e-01,  4.7764552e-04,\n",
       "        -5.1265582e-05, -9.0299050e-05],\n",
       "       [-1.7977927e-02,  2.4803881e-02,  3.4717363e-01,  5.2298151e-04,\n",
       "        -5.0981984e-05, -6.2657236e-05],\n",
       "       ...,\n",
       "       [ 5.5302966e-01, -1.6028321e+00, -2.9467770e-01, -4.9050752e-02,\n",
       "         2.6008642e-01,  5.5196112e-01],\n",
       "       [ 1.6226400e-01, -1.2037127e+00,  2.8320214e-01,  1.9215938e-01,\n",
       "        -1.1539631e-01,  5.6213886e-01],\n",
       "       [-3.7249854e-01, -1.5326660e+00,  7.1742004e-01, -2.5402227e-02,\n",
       "        -2.6570490e-01,  3.4866059e-01]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-0.14022358, 1.0515205, 0.34656662, 0.057980...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-0.103657156, 0.03466896, -0.046756677, 0.07...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-0.1583497, -0.7498793, 0.12545471, -0.02409...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[1.5225868, -0.39115846, 1.5109606, 0.9389003...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[-0.13768347, 0.16851205, 0.027480043, -0.001...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119014</th>\n",
       "      <td>[[-0.62476885, 0.039178997, 0.26083758, -0.335...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119015</th>\n",
       "      <td>[[-1.2598703, 1.0170987, -1.6936212, -0.089855...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119018</th>\n",
       "      <td>[[-0.16487487, -0.5883155, -0.21024181, 0.1673...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119022</th>\n",
       "      <td>[[-0.33617634, 0.0396387, 0.27841455, 0.048238...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119023</th>\n",
       "      <td>[[-0.28601098, -0.0031736863, -0.0065734955, 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59512 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     data label\n",
       "1       [[-0.14022358, 1.0515205, 0.34656662, 0.057980...     1\n",
       "2       [[-0.103657156, 0.03466896, -0.046756677, 0.07...     1\n",
       "3       [[-0.1583497, -0.7498793, 0.12545471, -0.02409...     1\n",
       "4       [[1.5225868, -0.39115846, 1.5109606, 0.9389003...     1\n",
       "7       [[-0.13768347, 0.16851205, 0.027480043, -0.001...     1\n",
       "...                                                   ...   ...\n",
       "119014  [[-0.62476885, 0.039178997, 0.26083758, -0.335...     1\n",
       "119015  [[-1.2598703, 1.0170987, -1.6936212, -0.089855...     1\n",
       "119018  [[-0.16487487, -0.5883155, -0.21024181, 0.1673...     1\n",
       "119022  [[-0.33617634, 0.0396387, 0.27841455, 0.048238...     1\n",
       "119023  [[-0.28601098, -0.0031736863, -0.0065734955, 0...     1\n",
       "\n",
       "[59512 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[train_set.label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.iloc[0:5000].to_json(\"./data/cleaned_data_\"+str(0)+\".json\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 8/17 [00:04<00:05,  1.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-61f077d80941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mundersampling\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                          \u001b[0mtest_split_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                          winmin = 6, stridesec = 15 )\n\u001b[0m",
      "\u001b[0;32m~/Thesis-Research/mywork/data_loader.py\u001b[0m in \u001b[0;36mload_train_test_data\u001b[0;34m(data_path, ratio_dataset, undersampling, test_split_ratio, winmin, stridesec)\u001b[0m\n\u001b[1;32m    497\u001b[0m                                                                                     \u001b[0mremovebias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                                                                                     \u001b[0mshx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                                                                                      gtperc = 0.5)\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         train_set,test_set = load_dataset(data, samples_indices, labels_array,undersampling= undersampling,\n",
      "\u001b[0;32m~/Thesis-Research/mywork/data_loader.py\u001b[0m in \u001b[0;36mload_PreprocessData\u001b[0;34m(winlength, step, meanvals, stdvals, ratio_dataset, files_list, removerest, removewalk, removebias, shx, gtperc)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m#################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;31m# smoothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mSmoothed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRawData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# remove trend of series data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Thesis-Research/mywork/data_loader.py\u001b[0m in \u001b[0;36msmooth\u001b[0;34m(RawData, WINDOW_SIZE, SIG)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Convolution followed by discarding of extra values after boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mSmoothed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRawData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRawData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Copy first 15 values from Rawdata to Smoothed. np.convolve doesn't do this.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mSmoothed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRawData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconvolve\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mconvolve\u001b[0;34m(a, v, mode)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'v cannot be empty'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mode_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from data_loader import *\n",
    "\n",
    "train_data, test_data= load_train_test_data(data_path ='batch-unix.txt',\n",
    "                         ratio_dataset=0.05, \n",
    "                        undersampling= True,\n",
    "                         test_split_ratio = 0.3,\n",
    "                         winmin = 6, stridesec = 15 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47285986,  0.55677134, -0.2330321 , -0.20383409,  0.00852659,\n",
       "         0.2438388 ],\n",
       "       [-0.6472508 ,  0.26777586, -0.07266016, -0.2460811 ,  0.07531058,\n",
       "         0.28094402],\n",
       "       [-0.7517754 ,  0.62782884,  0.33681288, -0.21922588,  0.04757515,\n",
       "         0.2828293 ],\n",
       "       ...,\n",
       "       [ 0.02423186,  0.01421984,  0.49951476, -0.04769883,  0.07232166,\n",
       "         0.07123255],\n",
       "       [ 0.10916266, -0.11873381,  0.5735638 , -0.02974646,  0.04825988,\n",
       "         0.06456374],\n",
       "       [ 0.17115285, -0.14832962,  0.6142865 , -0.02937665,  0.05272228,\n",
       "         0.0725497 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['data'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data/cleaned_data_0.json\",compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[ 1.5942797e-01  9.0304792e-02  1.4607425e-01 -6.4893484e-02\\n   4.0591914e-02 -4.3611377e-02]\\n [ 1.7132924e-01  7.3759265e-02  1.3082972e-01 -6.0138237e-02\\n   5.7521898e-02  1.1591701e-03]\\n [ 1.7425483e-01  3.0503687e-02  1.5181470e-01 -6.2486194e-02\\n   5.7846423e-02 -1.6662143e-02]\\n ...\\n [-4.6580547e-01  3.2085699e-01  1.6851538e-01  1.7772797e-02\\n  -1.8031813e-03 -4.5005791e-04]\\n [-4.5697862e-01  2.9397491e-01  1.7543517e-01  1.3826254e-02\\n  -3.5399494e-03  2.9436119e-03]\\n [-4.6172807e-01  2.8711575e-01  1.7303370e-01  8.2457634e-03\\n  -3.1789739e-03 -6.6284148e-04]]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_df = pd.read_csv(\"cleaned_all_day_data.csv\")\n",
    "csv_df['data'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of object (2) does not match with length of fields (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6d44839a2af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                         \u001b[0;34m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArrayType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArrayType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFloatType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                             ,StructField(\"label\", IntegerType(), True)])\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mspark_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;31m# Create a DataFrame from pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[0;32m--> 601\u001b[0;31m                 data, schema, samplingRatio, verifySchema)\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;31m# make sure data could consumed multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                 \u001b[0mverify_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverify_nullability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m             \u001b[0mverify_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1385\u001b[0m                     raise ValueError(\n\u001b[1;32m   1386\u001b[0m                         new_msg(\"Length of object (%d) does not match with \"\n\u001b[0;32m-> 1387\u001b[0;31m                                 \"length of fields (%d)\" % (len(obj), len(verifiers))))\n\u001b[0m\u001b[1;32m   1388\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifier\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m                     \u001b[0mverifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of object (2) does not match with length of fields (3)"
     ]
    }
   ],
   "source": [
    "# Test with python spark\n",
    "import os\n",
    "os.environ[\"PYTHON\"] = \"~/.conda/envs/myenv/bin/python3.7\" \n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"~/.conda/envs/myenv/bin/python3.7\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"~/.conda/envs/myenv/bin/python3.7\"\n",
    "from pyspark.sql import SparkSession \n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\")\\\n",
    ".appName(\"Data\")\\\n",
    ".config(\"spark.some.config.option\", \"some-value\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "# spark_df= spark.read.options(header=True).csv(\"cleaned_data.csv\")\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "mySchema = StructType([ StructField(\"index\", IntegerType(), True)\\\n",
    "                        ,StructField(\"data\", ArrayType(ArrayType(FloatType())), True)\\\n",
    "                            ,StructField(\"label\", IntegerType(), True)])\n",
    "spark_df = spark.createDataFrame(train_data, mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## using literal_eval to convert string to list\n",
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"test_data.csv\")\n",
    "# from ast import literal_eval\n",
    "# df['data'] = df['data'].apply(literal_eval)\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHON\"] = \"~/.conda/envs/myenv/bin/python3.7\" \n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"~/.conda/envs/myenv/bin/python3.7\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"~/.conda/envs/myenv/bin/python3.7\"\n",
    "from pyspark.sql import SparkSession \n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\")\\\n",
    ".appName(\"Data\")\\\n",
    ".config(\"spark.some.config.option\", \"some-value\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "spark_df= spark.read.options(header=True).csv(\"test_data.csv\")\n",
    "spark_df.printSchema()\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "convert = udf(lambda x: literal_eval(x), ArrayType(ArrayType(FloatType())))\n",
    "new_df = spark_df.withColumn(\"data\", convert(spark_df.data))\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Loader functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_loader.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numba as nb\n",
    "\n",
    "######################################\n",
    "# Note: \n",
    "#   1. load_PreprocessData is to load all raw data and do smoothing, de-trend processing\n",
    "#         and then segment series data using lists of indices.\n",
    "#   2. load_dataset is to return the formatted trainable dataset in pandas dataframe format.\n",
    "#       It should be run after load_PreProcessData\n",
    "#   3. loadshmfile is to load .shm file directly and return numpy time series data \n",
    "#      There are 6 axis gyroscope, accelerator data in .shm file\n",
    "#   4. date time information of data is in -events.txt  files, which indicates start,end, eating time period\n",
    "#      We can use it for more feature extraction\n",
    "###################################\n",
    "\n",
    "## Global settings for current dataset\n",
    "ACC_THRESH = 0.008   # sum(acc) (G) max value for stddev rest\n",
    "GYRO_THRESH = 0.04 * 180.0/ math.pi\n",
    "shimmer_global_mean = [-0.012359981,-0.0051663737,0.011612018,\n",
    "                0.05796114,0.1477952,-0.034395125 ]\n",
    "\n",
    "shimmer_global_stddev = [0.05756385,0.040893298,0.043825723,\n",
    "                17.199743,15.311142,21.229317 ]\n",
    "\n",
    "shimmer_trended_mean = [-0.000002,-0.000002,-0.000000,\n",
    "                0.058144,0.147621,-0.033260 ]\n",
    "\n",
    "shimmer_trended_stddev = [0.037592,0.034135,0.032263,\n",
    "                17.209038,15.321441,21.242532 ]\n",
    "\n",
    "all_zero_means = [0,0,0,0,0,0]\n",
    "\n",
    "actigraph_global_means = [ 0.010016, -0.254719, 0.016803, \n",
    "                0.430628, 0.097660, 0.359574 ]\n",
    "\n",
    "actigraph_trended_means = [ -0.000001, 0.000022, -0.000002, \n",
    "                0.430628, 0.097660, 0.359574 ]\n",
    "meanvals = all_zero_means\n",
    "stdvals = shimmer_trended_stddev\n",
    "\n",
    "\n",
    "\n",
    "def loadshmfile(File_Name):\n",
    "    \"\"\"\n",
    "    Load shm data file\n",
    "    Output:\n",
    "        6-axis gyroscope, accelerator time series data in numpy format\n",
    "    \"\"\"\n",
    "    MB = 1024 * 1024\n",
    "    RawData = np.fromfile(File_Name, dtype=np.dtype(\"6f4\")) \n",
    "    #print(f\"File {File_Name} Loaded\")\n",
    "    #print(sys.getsizeof(RawData)/MB)\n",
    "    # Swap gyroscope axis. Remember python always uses variables with reference.\n",
    "    # Swap Acceleromter\n",
    "    Temp = np.copy(RawData[:,5])\n",
    "    Temp2 = np.copy(RawData[:,3])\n",
    "    Temp3 = np.copy(RawData[:,4])\n",
    "    RawData[:,3], RawData[:,4],RawData[:,5] = Temp,Temp2, Temp3\n",
    "    del Temp\n",
    "    del Temp2\n",
    "    del Temp3\n",
    "    \n",
    "    return RawData\n",
    "\n",
    "\n",
    "def smooth(RawData, WINDOW_SIZE = 15,SIG = 10.0 ):\n",
    "    \"\"\"\n",
    "    Smooth 6 axis raw data by convolution using a window\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create kernel\n",
    "    # size of window\n",
    "    r_array = np.linspace(14,0, 15)\n",
    "    Kernel = np.exp((0-np.square(r_array))/(2 * SIG * SIG))\n",
    "    deno = sum(Kernel)\n",
    "    Kernel = Kernel / deno\n",
    "    del r_array\n",
    "    del deno\n",
    "    #Clone (deep copy) the variable, instead of reference. We don't want to change RawData\n",
    "    Smoothed = np.copy(RawData) \n",
    "    r,c = RawData.shape\n",
    "    for x in range(c):\n",
    "        # Convolution followed by discarding of extra values after boundary\n",
    "        Smoothed[:,x] = np.convolve(RawData[:,x], Kernel)[0:len(RawData)]\n",
    "    # Copy first 15 values from Rawdata to Smoothed. np.convolve doesn't do this.\n",
    "    Smoothed[:15, :] = RawData[:15,:]\n",
    "    return Smoothed\n",
    "\n",
    "\n",
    "\n",
    "def loadEvents(filename):\n",
    "    \"\"\"\n",
    "    loads events data given the .shm filename\n",
    "    and parse the event.txt file to obtain meal duration\n",
    "    Input: \n",
    "        filename:  <filename>-events.txt name of label file we want to load\n",
    "    output:\n",
    "        TotalEvents: amount of event loaded\n",
    "        EventStart: a list of starting moment of meals\n",
    "        EventEnd: a list of ending moment of meals\n",
    "        EventNames: name of meal in string\n",
    "    \"\"\"\n",
    "    # Load the meals file to get any triaged meals.\n",
    "    SkippedMeals = []\n",
    "    mealsfile = open(\"meals-shimmer.txt\", \"r\") \n",
    "    for line in mealsfile:\n",
    "        #print(line)\n",
    "        data = line.split()\n",
    "        #print(data[0], data[1], data[13])\n",
    "        if(int(data[13]) == 0):\n",
    "            Mdata = [data[0][-9:], data[1], int(data[13])]\n",
    "            SkippedMeals.append(Mdata)\n",
    "    \n",
    "    EventsFileName = filename.replace(\".shm\",\"-events.txt\")\n",
    "    \n",
    "    # Load the meals\n",
    "    EventNames = []\n",
    "    EventStart = (np.zeros((100))).astype(int)\n",
    "    EventEnd = (np.zeros((100))).astype(int)\n",
    "    TotalEvents = 0\n",
    "    TimeOffset = 0\n",
    "    file = open(EventsFileName, \"r\") \n",
    "    #print(filename)\n",
    "    for lines in file:\n",
    "        #print(lines)\n",
    "        words = lines.split()\n",
    "        if(len(words) == 0): continue # Skip empty lines\n",
    "        # Convert Start time to offset\n",
    "        if(words[0] == \"START\"): # Get Start Time (TimeOffset) from file\n",
    "            #print(words)\n",
    "            hours = int(words[2].split(\":\")[0])\n",
    "            minutes = int(words[2].split(\":\")[1])\n",
    "            seconds = int(words[2].split(\":\")[2])\n",
    "            #print(\"{}h:{}m:{}s\".format(hours, minutes,seconds))\n",
    "            TimeOffset = (hours * 60 * 60) + (minutes * 60) + seconds\n",
    "            continue\n",
    "        if(words[0] == \"END\"):\n",
    "            #print(words)\n",
    "            continue\n",
    "        for x in range(1,3): # Process Events Data\n",
    "            hours = int(words[x].split(\":\")[0])\n",
    "            minutes = int(words[x].split(\":\")[1])\n",
    "            seconds = int(words[x].split(\":\")[2])\n",
    "            EventTime = (hours * 60 * 60) + (minutes * 60) + seconds\n",
    "            EventTime = EventTime - TimeOffset\n",
    "            if(x == 1): EventStart[TotalEvents] = EventTime * 15\n",
    "            if(x == 2): EventEnd[TotalEvents] = EventTime * 15\n",
    "        if(TotalEvents>0):\n",
    "            if(EventStart[TotalEvents]<EventStart[TotalEvents-1]):\n",
    "                EventStart[TotalEvents] = EventStart[TotalEvents] + (24*60*60*15)\n",
    "            if(EventEnd[TotalEvents]<EventEnd[TotalEvents-1]):\n",
    "                EventEnd[TotalEvents] = EventEnd[TotalEvents] + (24*60*60*15)\n",
    "        #print(TotalEvents)\n",
    "        \n",
    "        # Check if meal was triaged out for too much walking or rest\n",
    "        ename = words[0]\n",
    "        fname = filename[-9:]\n",
    "        skipmeal = 0\n",
    "        #print(fname, ename)\n",
    "        for skippedmeal in SkippedMeals:\n",
    "            Pname, EventName, Keep = skippedmeal\n",
    "            if(Pname == fname and ename == EventName):\n",
    "                #print(Pname, EventName, Keep, ename, fname, Pname == fname, ename == EventName)\n",
    "                skipmeal = 1\n",
    "                break\n",
    "        \n",
    "        if(skipmeal == 1): continue\n",
    "        TotalEvents = TotalEvents + 1\n",
    "        EventNames.append(ename)\n",
    "    return TotalEvents, EventStart, EventEnd, EventNames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_PreprocessData(winlength, step, meanvals, stdvals,ratio_dataset=1, files_list='batch-unix.txt', \n",
    "                 removerest=1, removewalk=0, removebias=1, shx=1, gtperc = 0.5):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        winlength: number of data point in a window\n",
    "        step: number of data point to skip during moving the window forward\n",
    "        removerest / removewalk: flag to indicate if remove rest/walk period or not\n",
    "        gtperc:  ground truth percentage:  the ratio of true label in a window to the total window size\n",
    "                indicating how many data points label = 1 in a window can be regarded as eating d=label\n",
    "        shx：flag to indicate load shx file or not\n",
    "        \n",
    "        ratio_dataset: the ratio of number of days to sample over all days(354 days).\n",
    "                        It controls the size of dataset. It should be in [0,1]. Otherwise return 0 sample\n",
    "        \n",
    "    Output:\n",
    "        len(df[\"Filenames\"]) : amount of day of dataset\n",
    "        AllNormalized: normalized, smoothed dataset\n",
    "        samples_array: list of indices of sample frame in dataset, \n",
    "                        format: [(i^th day, start time of window, end time of window),... ]\n",
    "        labels_array: labels corresponding to each segmentation/window\n",
    "        \n",
    "    \"\"\"\n",
    "    ### Load data, make samples \n",
    "\n",
    "    samples = []\n",
    "    labels = []\n",
    "    AllNormalized = []\n",
    "    AllIndices = []\n",
    "    totaleatingrest = 0\n",
    "    totaleatingwalk = 0\n",
    "    flag = 0\n",
    "    # load index file of all dataset\n",
    "    df = pd.read_csv(files_list, names=[\"Filenames\"])\n",
    "    \n",
    "    #if ratio is 1, then do nothing\n",
    "    if ratio_dataset != 1.0:\n",
    "        if ratio_dataset<1 and ratio_dataset>=0:\n",
    "            # sample days without replacement\n",
    "            num_days = int(ratio_dataset* len(df))\n",
    "            df = df.sample(num_days,replace=False)\n",
    "            # reorder index\n",
    "            df.index = [i for i in range(len(df))]\n",
    "        else:\n",
    "            df = df.sample(0,replace=False)\n",
    "        \n",
    "    \n",
    "    \n",
    "    print(\"Loading Dataset ...\")\n",
    "    # using  tqdm package to visualize process\n",
    "    for x in tqdm(range(len(df[\"Filenames\"]))):\n",
    "        fileeatingrest = 0\n",
    "        fileeatingwalk = 0\n",
    "        filesamples = []\n",
    "        filelabels = []\n",
    "        File_Name = \"../Data/\" + df[\"Filenames\"][x]\n",
    "        RawData = loadshmfile(File_Name)\n",
    "        \n",
    "        ##################################\n",
    "        #Data preprocessing, Smoothing here\n",
    "        #################################            \n",
    "        # smoothing\n",
    "        Smoothed = smooth(RawData)\n",
    "        \n",
    "        # remove trend of series data\n",
    "        # Option:  remove acceleration bias or not using a slide window to compute mean\n",
    "        if(removebias):\n",
    "            # Remove acceleration bias\n",
    "            TREND_WINDOW = 150\n",
    "            mean = []\n",
    "            for j in range(3):\n",
    "                dat = pd.Series(Smoothed[:,j]).rolling(window=TREND_WINDOW).mean()\n",
    "                dat[:TREND_WINDOW-1] = 0\n",
    "                mean.append(dat)\n",
    "            mean2 = np.roll(np.asarray(mean).transpose(), -((TREND_WINDOW//2)-1)*3) # Shift to the left to center the values\n",
    "            # The last value in mean [-75] does not match that of phoneview, but an error in one datum is acceptable\n",
    "            # The phone view code calculates mean from -Window/2 to <Window/2 instead of including it.\n",
    "            Smoothed[:,0:3]-=mean2\n",
    "            del mean2, mean, dat\n",
    "        \n",
    "        # Normalization: z-normalization\n",
    "        Normalized = np.empty_like(Smoothed)\n",
    "        for i in range(6):\n",
    "            Normalized[:,i] = (Smoothed[:,i] - meanvals[i]) / stdvals[i]\n",
    "        # Stick this Normalized data to the Full Array\n",
    "        AllNormalized.append(np.copy(Normalized))\n",
    "\n",
    "        \n",
    "        if(removerest != 0):\n",
    "        # remove labels for the class of rest \n",
    "            std = []\n",
    "            for j in range(6):\n",
    "                dat = pd.Series(Smoothed[:,j]).rolling(window=15).std(ddof=0)\n",
    "                dat[:14] = 0\n",
    "                std.append(dat)\n",
    "            # Above doesn't center window. Left Shift all values to the left by 7 datum (6 sensors)\n",
    "            std2 = np.roll(np.asarray(std).transpose(), -7*6) \n",
    "            accstd = np.sum(std2[:,:3], axis=1)\n",
    "            gyrostd = np.sum(std2[:,-3:], axis=1)\n",
    "            datrest = (accstd < ACC_THRESH) & (gyrostd < GYRO_THRESH)\n",
    "            mrest = datrest.copy()\n",
    "\n",
    "            for i in range(8,len(datrest)-7):\n",
    "                if(datrest[i]==True):\n",
    "                    mrest[i-7:i+8] = True\n",
    "            \n",
    "            del dat, datrest, gyrostd, accstd, std2, std\n",
    "        \n",
    "        if(removewalk!=0):\n",
    "        # remove labels for the class of walking\n",
    "        # remove walk period if enabled\n",
    "            minv = np.zeros((3,1))\n",
    "            maxv = np.zeros((3,1))\n",
    "            zerocross = np.zeros((len(Smoothed),1)).astype(int)\n",
    "            for j in range(3):\n",
    "                minv[j]=float('inf')\n",
    "                maxv[j]= -float('inf')\n",
    "\n",
    "            for t in range(len(Smoothed)-1):\n",
    "                for j in range(3):\n",
    "                    if (Smoothed[t][j+3] < minv[j]):\n",
    "                        minv[j]=Smoothed[t][j+3]\n",
    "                    if (Smoothed[t][j+3] > maxv[j]):\n",
    "                        maxv[j]=Smoothed[t][j+3]\n",
    "                    if ((Smoothed[t][j+3] < 0.0)  and  (Smoothed[t+1][j+3] > 0.0)  and  (minv[j] < -5.0)):\n",
    "                        zerocross[t]+=(1<<j)\n",
    "                        minv[j]=float('inf')\n",
    "                        maxv[j]=-float('inf')\n",
    "                    if ((Smoothed[t][j+3] > 0.0)  and  (Smoothed[t+1][j+3] < 0.0)  and  (maxv[j] > 5.0)):\n",
    "                        zerocross[t]+=(1<<(j+3))\n",
    "                        minv[j]=float('inf')\n",
    "                        maxv[j]= -float('inf')\n",
    "\n",
    "            zc = [0 if i==0 else 1 for i in zerocross]\n",
    "            del minv, maxv, zerocross\n",
    "        \n",
    "        del RawData, Smoothed\n",
    "\n",
    "        \n",
    "        ###################################\n",
    "        # Generating labels here\n",
    "        ###################################\n",
    "        # Identify things as GT\n",
    "        [TotalEvents, EventStart, EventEnd, EventNames] = loadEvents(File_Name)\n",
    "        GT = np.zeros((len(Normalized))).astype(int)\n",
    "        for i in range(TotalEvents):\n",
    "            #print(EventStart[i], EventStart[i], type(EventStart[i]))\n",
    "            GT[EventStart[i]: EventEnd[i]+1] = 1\n",
    "        \n",
    "        # Generate labels \n",
    "        MaxData = len(Normalized)\n",
    "        for t in range(0, MaxData, step):\n",
    "            # x: the x^th day data in dataset\n",
    "            # t: starting time of eating\n",
    "            # t+ winlength:  end time of eating\n",
    "            sample = [x, t, t+winlength]\n",
    "            label = int((np.sum(GT[t:t+winlength])/winlength)>=gtperc)\n",
    "            \n",
    "            #Change labels if the flag of removerest or removewalk is enabled\n",
    "            if(label and removerest!=0): # Only ignore if in eating\n",
    "                isrest = int((np.sum(mrest[t:t+winlength])/winlength)>=0.65)\n",
    "                if(isrest and removerest==1): continue; # Do not consider this sample at all. Comment this if you want to move the sample to non-eating.\n",
    "                elif(isrest and removerest==2): label = 0;\n",
    "                else: label = 1    \n",
    "            if(label and removewalk!=0): # Only ignore if in eating\n",
    "                iswalk = int((np.sum(zc[t:t+winlength])/winlength)>=0.15)\n",
    "                if(iswalk and removewalk==1): continue;\n",
    "                elif(iswalk and removewalk==2): label=0;\n",
    "                else: label = 1\n",
    "#                fileeatingwalk+=1\n",
    "#                continue # Do not append this sample to the dataset\n",
    "                \n",
    "            if(t+winlength < MaxData): # Ignore last small window. Not ignoring results in a list rather than a numpy array.\n",
    "                filesamples.append(sample)\n",
    "                filelabels.append(label)\n",
    "                \n",
    "        #merge two lists\n",
    "        samples += filesamples\n",
    "        labels += filelabels\n",
    "        numsamples = len(filesamples)\n",
    "        totaleatingwalk += fileeatingwalk\n",
    "\n",
    "    samples_array = np.asarray(samples)\n",
    "    labels_array = np.asarray(labels)\n",
    "    #print(\"Total {:d} walking in eating\\n\".format(fileeatingwalk))\n",
    "    return len(df[\"Filenames\"]), AllNormalized, samples_array, labels_array\n",
    "\n",
    "\n",
    "\n",
    "def normalizeData(data, meanvals, stdvals):\n",
    "    \"\"\"\n",
    "    Normalize data using given mean values and std variance values\n",
    "    Input:\n",
    "         data: time series data set to normalize\n",
    "         meanvals: global mean value of all days dataset used to smooth per day time series data\n",
    "         stdvals: global standard variance.\n",
    "    Output:\n",
    "        smoothed data\n",
    "    \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for x in range(len(data)):\n",
    "        Smoothed = data[x]\n",
    "        Normalized = np.empty_like(Smoothed)\n",
    "        # Normalize\n",
    "        for i in range(6):\n",
    "            Normalized[:,i] = (Smoothed[:,i] - meanvals[i]) / stdvals[i]\n",
    "        \n",
    "        # Stick this Normalized data to the Full Array\n",
    "        AllNormalized.append(np.copy(Normalized))\n",
    "    \n",
    "    return AllNormalized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(data, samples_indices, labels_array, shuffle_flag = True, undersampling= True, test_split_ratio =0.3):\n",
    "    \"\"\"\n",
    "    Note: need to run load_PreprocessData to get processed time series data and segmentation data\n",
    "            before running this function\n",
    "    Input:\n",
    "        data: normalized smoothed time series data.\n",
    "\n",
    "        sample_indices: a list of tuples containing start and end indices of each window in data\n",
    "                Format:   there are n rows in data, each represents sensor data for one day\n",
    "                        in each row.\n",
    "                        There are m x k data, where m = amount of data point along time\n",
    "                        K = amount of features in training set\n",
    "        labels_array: label of each segmentation in dataset\n",
    "    Output:\n",
    "        pandas data frame with columns \"data\" and \"label\"\n",
    "        used to train model\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    import sys\n",
    "    outfile = sys.stdout\n",
    "    \n",
    "    data_indices = samples_indices\n",
    "    label_indices = labels_array\n",
    "\n",
    "    # Balance Data here\n",
    "    #undersample the training dataset\n",
    "    eatingindices = [i for i, e in enumerate(label_indices) if e >= 0.5]\n",
    "    noneatingindices = [i for i, e in enumerate(label_indices) if e < 0.5]\n",
    "    \n",
    "    #split training set and test set\n",
    "    eat_len = len(eatingindices)\n",
    "    noneat_len = len(noneatingindices)\n",
    "    # Test set indices\n",
    "    test_noneating = noneatingindices[0: int( noneat_len*test_split_ratio)]\n",
    "    test_eating  =  eatingindices[0: int(eat_len*test_split_ratio)]  \n",
    "    test_indices = test_eating + test_noneating\n",
    "    # Train set indices\n",
    "    eatingindices = eatingindices[int(eat_len *test_split_ratio):]\n",
    "    noneatingindices = noneatingindices[int( noneat_len*test_split_ratio): ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # training set \n",
    "    if undersampling:\n",
    "        underSampledNoneatingIndices = random.sample(noneatingindices,len(eatingindices))\n",
    "        underSampledBalancedIndices = eatingindices + underSampledNoneatingIndices\n",
    "        shuffledUnderSampledBalancedIndices = underSampledBalancedIndices.copy()\n",
    "    else:\n",
    "        shuffledUnderSampledBalancedIndices = eatingindices+noneatingindices\n",
    "    \n",
    "    if shuffle_flag:\n",
    "        random.shuffle(shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "    print(\"Creating balanced training data array\", file=outfile, flush=True)\n",
    "    train_set = pd.DataFrame(columns=['data','label'])\n",
    "    test_set = pd.DataFrame(columns=['data','label'])\n",
    "\n",
    "    # use process bar to show the remaining items to handle\n",
    "    print(\"Loading training set...\")\n",
    "    for _, i in enumerate(tqdm(shuffledUnderSampledBalancedIndices)):\n",
    "        f = data_indices[i,0]\n",
    "        t1, t2 = data_indices[i,1], data_indices[i,2]\n",
    "        sample,label = data[f][t1:t2], label_indices[i]\n",
    "        df = pd.DataFrame({'data':[sample], 'label':label})\n",
    "        train_set = train_set.append(df,ignore_index=True)\n",
    "    \n",
    "    print(\"Loading test set...\")\n",
    "    for _, i in enumerate(tqdm(test_indices)):\n",
    "        f = data_indices[i,0]\n",
    "        t1, t2 = data_indices[i,1], data_indices[i,2]\n",
    "        sample,label = data[f][t1:t2], label_indices[i]\n",
    "        df = pd.DataFrame({'data':[sample], 'label':label})\n",
    "        test_set = test_set.append(df,ignore_index=True)\n",
    "    \n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "def load_train_test_data(data_path ='batch-unix.txt',\n",
    "                         ratio_dataset=1, undersampling= True,\n",
    "                         test_split_ratio = 0.3,\n",
    "                         winmin = 6, stridesec = 15 ):\n",
    "#         from datetime import datetime\n",
    "        ModelNum = 3 \n",
    "        \n",
    "        winlength = int(winmin * 60 * 15)\n",
    "        step = int(stridesec * 15)\n",
    "#         start_time = datetime.now()\n",
    "        meanvals, stdvals =  all_zero_means, shimmer_trended_stddev\n",
    "#     winlength, step, meanvals, stdvals,ratio_dataset=1, files_list='batch-unix.txt', \n",
    "#                  removerest=1, removewalk=0, removebias=1, shx=1, gtperc = 0.5\n",
    "        NumFiles, data, samples_indices, labels_array = load_PreprocessData(winlength,\n",
    "                                                                                    step,\n",
    "                                                                                    meanvals, \n",
    "                                                                                    stdvals,\n",
    "                                                                                    ratio_dataset =ratio_dataset,\n",
    "                                                                                    files_list=data_path, \n",
    "                                                                                    removerest=0,\n",
    "                                                                                    removewalk=0,\n",
    "                                                                                    removebias=0,\n",
    "                                                                                    shx=1,\n",
    "                                                                                     gtperc = 0.5)\n",
    "\n",
    "        train_set,test_set = load_dataset(data, samples_indices, labels_array,undersampling= undersampling,\n",
    "                                          test_split_ratio=test_split_ratio)\n",
    "\n",
    "\n",
    "        return train_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filenames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ShimmerData/P2116/P2116.shm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ShimmerData/P2189/P2189.shm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ShimmerData/P2410/P2410.shm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ShimmerData/P2334/P2334.shm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ShimmerData/P2479/P2479.shm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ShimmerData/P2289/P2289.shm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ShimmerData/P2217/P2217.shm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ShimmerData/P2262/P2262.shm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ShimmerData/P2302/P2302.shm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ShimmerData/P2276/P2276.shm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Filenames\n",
       "0  ShimmerData/P2116/P2116.shm\n",
       "1  ShimmerData/P2189/P2189.shm\n",
       "2  ShimmerData/P2410/P2410.shm\n",
       "3  ShimmerData/P2334/P2334.shm\n",
       "4  ShimmerData/P2479/P2479.shm\n",
       "5  ShimmerData/P2289/P2289.shm\n",
       "6  ShimmerData/P2217/P2217.shm\n",
       "7  ShimmerData/P2262/P2262.shm\n",
       "8  ShimmerData/P2302/P2302.shm\n",
       "9  ShimmerData/P2276/P2276.shm"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"batch-unix.txt\", names=[\"Filenames\"])\n",
    "df = df.sample(10,replace=False)\n",
    "df.index = [i for i in range(len(df))]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Load function written by one PhD student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "ACC_THRESH = 0.008   # sum(acc) (G) max value for stddev rest\n",
    "GYRO_THRESH = 0.04 * 180.0/ math.pi\n",
    "\n",
    "def loadshmfile(File_Name):\n",
    "    MB = 1024 * 1024\n",
    "    RawData = np.fromfile(File_Name, dtype=np.dtype(\"6f4\")) \n",
    "    #print(f\"File {File_Name} Loaded\")\n",
    "    #print(sys.getsizeof(RawData)/MB)\n",
    "    # Swap gyroscope axis. Remember python always uses variables with reference.\n",
    "    # Swap Acceleromter\n",
    "    Temp = np.copy(RawData[:,5])\n",
    "    Temp2 = np.copy(RawData[:,3])\n",
    "    Temp3 = np.copy(RawData[:,4])\n",
    "    RawData[:,3], RawData[:,4],RawData[:,5] = Temp,Temp2, Temp3\n",
    "    del Temp\n",
    "    del Temp2\n",
    "    del Temp3\n",
    "    \n",
    "    return RawData\n",
    "\n",
    "\n",
    "def smooth(RawData, WINDOW_SIZE = 15,SIG = 10.0 ):\n",
    "    \"\"\"\n",
    "    Smooth 6 axis raw data by convolution using a window\n",
    "    \"\"\"\n",
    "    # Create kernel\n",
    "    # size of window\n",
    "    r_array = np.linspace(14,0, 15)\n",
    "    Kernel = np.exp((0-np.square(r_array))/(2 * SIG * SIG))\n",
    "    deno = sum(Kernel)\n",
    "    Kernel = Kernel / deno\n",
    "    del r_array\n",
    "    del deno\n",
    "    #Clone (deep copy) the variable, instead of reference. We don't want to change RawData\n",
    "    Smoothed = np.copy(RawData) \n",
    "    r,c = RawData.shape\n",
    "    for x in range(c):\n",
    "        # Convolution followed by discarding of extra values after boundary\n",
    "        Smoothed[:,x] = np.convolve(RawData[:,x], Kernel)[0:len(RawData)]\n",
    "    # Copy first 15 values from Rawdata to Smoothed. np.convolve doesn't do this.\n",
    "    Smoothed[:15, :] = RawData[:15,:]\n",
    "    return Smoothed\n",
    "\n",
    "\n",
    "\n",
    "def loadEvents(filename):\n",
    "    \"\"\"\n",
    "    loads events data given the .shm filename\n",
    "    and parse the event.txt file to obtain meal duration\n",
    "    Input: \n",
    "        filename:  <filename>-events.txt name of label file we want to load\n",
    "    output:\n",
    "        TotalEvents: amount of event loaded\n",
    "        EventStart: a list of starting moment of meals\n",
    "        EventEnd: a list of ending moment of meals\n",
    "        EventNames: name of meal in string\n",
    "    \"\"\"\n",
    "    # Load the meals file to get any triaged meals.\n",
    "    SkippedMeals = []\n",
    "    mealsfile = open(\"meals-shimmer.txt\", \"r\") \n",
    "    for line in mealsfile:\n",
    "        #print(line)\n",
    "        data = line.split()\n",
    "        #print(data[0], data[1], data[13])\n",
    "        if(int(data[13]) == 0):\n",
    "            Mdata = [data[0][-9:], data[1], int(data[13])]\n",
    "            SkippedMeals.append(Mdata)\n",
    "    \n",
    "    EventsFileName = filename.replace(\".shm\",\"-events.txt\")\n",
    "    \n",
    "    # Load the meals\n",
    "    EventNames = []\n",
    "    EventStart = (np.zeros((100))).astype(int)\n",
    "    EventEnd = (np.zeros((100))).astype(int)\n",
    "    TotalEvents = 0\n",
    "    TimeOffset = 0\n",
    "    file = open(EventsFileName, \"r\") \n",
    "    #print(filename)\n",
    "    for lines in file:\n",
    "        #print(lines)\n",
    "        words = lines.split()\n",
    "        if(len(words) == 0): continue # Skip empty lines\n",
    "        # Convert Start time to offset\n",
    "        if(words[0] == \"START\"): # Get Start Time (TimeOffset) from file\n",
    "            #print(words)\n",
    "            hours = int(words[2].split(\":\")[0])\n",
    "            minutes = int(words[2].split(\":\")[1])\n",
    "            seconds = int(words[2].split(\":\")[2])\n",
    "            #print(\"{}h:{}m:{}s\".format(hours, minutes,seconds))\n",
    "            TimeOffset = (hours * 60 * 60) + (minutes * 60) + seconds\n",
    "            continue\n",
    "        if(words[0] == \"END\"):\n",
    "            #print(words)\n",
    "            continue\n",
    "        for x in range(1,3): # Process Events Data\n",
    "            hours = int(words[x].split(\":\")[0])\n",
    "            minutes = int(words[x].split(\":\")[1])\n",
    "            seconds = int(words[x].split(\":\")[2])\n",
    "            EventTime = (hours * 60 * 60) + (minutes * 60) + seconds\n",
    "            EventTime = EventTime - TimeOffset\n",
    "            if(x == 1): EventStart[TotalEvents] = EventTime * 15\n",
    "            if(x == 2): EventEnd[TotalEvents] = EventTime * 15\n",
    "        if(TotalEvents>0):\n",
    "            if(EventStart[TotalEvents]<EventStart[TotalEvents-1]):\n",
    "                EventStart[TotalEvents] = EventStart[TotalEvents] + (24*60*60*15)\n",
    "            if(EventEnd[TotalEvents]<EventEnd[TotalEvents-1]):\n",
    "                EventEnd[TotalEvents] = EventEnd[TotalEvents] + (24*60*60*15)\n",
    "        #print(TotalEvents)\n",
    "        \n",
    "        # Check if meal was triaged out for too much walking or rest\n",
    "        ename = words[0]\n",
    "        fname = filename[-9:]\n",
    "        skipmeal = 0\n",
    "        #print(fname, ename)\n",
    "        for skippedmeal in SkippedMeals:\n",
    "            Pname, EventName, Keep = skippedmeal\n",
    "            if(Pname == fname and ename == EventName):\n",
    "                #print(Pname, EventName, Keep, ename, fname, Pname == fname, ename == EventName)\n",
    "                skipmeal = 1\n",
    "                break\n",
    "        \n",
    "        if(skipmeal == 1): continue\n",
    "        TotalEvents = TotalEvents + 1\n",
    "        EventNames.append(ename)\n",
    "    return TotalEvents, EventStart, EventEnd, EventNames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loadAllData2(winlength, step, meanvals, stdvals, files_list='batch-unix.txt', \n",
    "                 removerest=1, removewalk=0, removebias=1, shx=1, gtperc = 0.5):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        winlength: number of data point in a window\n",
    "        step: number of data point to skip during moving the window forward\n",
    "        removerest / removewalk: flag to indicate if remove rest/walk period or not\n",
    "        gtperc:  ground truth percentage:  the ratio of true label in a window to the total window size\n",
    "                indicating how many data points label = 1 in a window can be regarded as eating d=label\n",
    "        shx：flag to indicate load shx file or not\n",
    "    Output:\n",
    "        len(df[\"Filenames\"]) : amount of day of dataset\n",
    "        AllNormalized: normalized, smoothed dataset\n",
    "        samples_array: list of indices of sample frame in dataset, \n",
    "                        format: [(i^th day, start time of window, end time of window),... ]\n",
    "        labels_array: labels corresponding to each segmentation/window\n",
    "        \n",
    "    \"\"\"\n",
    "    ### Load data, make samples \n",
    "\n",
    "    samples = []\n",
    "    labels = []\n",
    "    AllNormalized = []\n",
    "    AllIndices = []\n",
    "    totaleatingrest = 0\n",
    "    totaleatingwalk = 0\n",
    "    flag = 0\n",
    "    # load index file of all dataset\n",
    "    df = pd.read_csv(files_list, names=[\"Filenames\"])\n",
    "    # using  tqdm package to visualize process\n",
    "    for x in tqdm(range(len(df[\"Filenames\"]))):\n",
    "        fileeatingrest = 0\n",
    "        fileeatingwalk = 0\n",
    "        filesamples = []\n",
    "        filelabels = []\n",
    "        File_Name = \"../Data/\" + df[\"Filenames\"][x]\n",
    "        RawData = loadshmfile(File_Name)\n",
    "        \n",
    "        ##################################\n",
    "        #Data preprocessing, Smoothing here\n",
    "        #################################            \n",
    "        # smoothing\n",
    "        Smoothed = smooth(RawData)\n",
    "        \n",
    "        # remove trend of series data\n",
    "        # Option:  remove acceleration bias or not using a slide window to compute mean\n",
    "        if(removebias):\n",
    "            # Remove acceleration bias\n",
    "            TREND_WINDOW = 150\n",
    "            mean = []\n",
    "            for j in range(3):\n",
    "                dat = pd.Series(Smoothed[:,j]).rolling(window=TREND_WINDOW).mean()\n",
    "                dat[:TREND_WINDOW-1] = 0\n",
    "                mean.append(dat)\n",
    "            mean2 = np.roll(np.asarray(mean).transpose(), -((TREND_WINDOW//2)-1)*3) # Shift to the left to center the values\n",
    "            # The last value in mean [-75] does not match that of phoneview, but an error in one datum is acceptable\n",
    "            # The phone view code calculates mean from -Window/2 to <Window/2 instead of including it.\n",
    "            Smoothed[:,0:3]-=mean2\n",
    "            del mean2, mean, dat\n",
    "        \n",
    "        # Normalization: z-normalization\n",
    "        Normalized = np.empty_like(Smoothed)\n",
    "        for i in range(6):\n",
    "            Normalized[:,i] = (Smoothed[:,i] - meanvals[i]) / stdvals[i]\n",
    "        # Stick this Normalized data to the Full Array\n",
    "        AllNormalized.append(np.copy(Normalized))\n",
    "\n",
    "        \n",
    "        if(removerest != 0):\n",
    "        # remove labels for the class of rest \n",
    "            std = []\n",
    "            for j in range(6):\n",
    "                dat = pd.Series(Smoothed[:,j]).rolling(window=15).std(ddof=0)\n",
    "                dat[:14] = 0\n",
    "                std.append(dat)\n",
    "            # Above doesn't center window. Left Shift all values to the left by 7 datum (6 sensors)\n",
    "            std2 = np.roll(np.asarray(std).transpose(), -7*6) \n",
    "            accstd = np.sum(std2[:,:3], axis=1)\n",
    "            gyrostd = np.sum(std2[:,-3:], axis=1)\n",
    "            datrest = (accstd < ACC_THRESH) & (gyrostd < GYRO_THRESH)\n",
    "            mrest = datrest.copy()\n",
    "\n",
    "            for i in range(8,len(datrest)-7):\n",
    "                if(datrest[i]==True):\n",
    "                    mrest[i-7:i+8] = True\n",
    "            \n",
    "            del dat, datrest, gyrostd, accstd, std2, std\n",
    "        \n",
    "        if(removewalk!=0):\n",
    "        # remove labels for the class of walking\n",
    "        # remove walk period if enabled\n",
    "            minv = np.zeros((3,1))\n",
    "            maxv = np.zeros((3,1))\n",
    "            zerocross = np.zeros((len(Smoothed),1)).astype(int)\n",
    "            for j in range(3):\n",
    "                minv[j]=float('inf')\n",
    "                maxv[j]= -float('inf')\n",
    "\n",
    "            for t in range(len(Smoothed)-1):\n",
    "                for j in range(3):\n",
    "                    if (Smoothed[t][j+3] < minv[j]):\n",
    "                        minv[j]=Smoothed[t][j+3]\n",
    "                    if (Smoothed[t][j+3] > maxv[j]):\n",
    "                        maxv[j]=Smoothed[t][j+3]\n",
    "                    if ((Smoothed[t][j+3] < 0.0)  and  (Smoothed[t+1][j+3] > 0.0)  and  (minv[j] < -5.0)):\n",
    "                        zerocross[t]+=(1<<j)\n",
    "                        minv[j]=float('inf')\n",
    "                        maxv[j]=-float('inf')\n",
    "                    if ((Smoothed[t][j+3] > 0.0)  and  (Smoothed[t+1][j+3] < 0.0)  and  (maxv[j] > 5.0)):\n",
    "                        zerocross[t]+=(1<<(j+3))\n",
    "                        minv[j]=float('inf')\n",
    "                        maxv[j]= -float('inf')\n",
    "\n",
    "            zc = [0 if i==0 else 1 for i in zerocross]\n",
    "            del minv, maxv, zerocross\n",
    "        \n",
    "        del RawData, Smoothed\n",
    "\n",
    "        \n",
    "        ###################################\n",
    "        # Generating labels here\n",
    "        ###################################\n",
    "        # Identify things as GT\n",
    "        [TotalEvents, EventStart, EventEnd, EventNames] = loadEvents(File_Name)\n",
    "        GT = np.zeros((len(Normalized))).astype(int)\n",
    "        for i in range(TotalEvents):\n",
    "            #print(EventStart[i], EventStart[i], type(EventStart[i]))\n",
    "            GT[EventStart[i]: EventEnd[i]+1] = 1\n",
    "        \n",
    "        # Generate labels \n",
    "        MaxData = len(Normalized)\n",
    "        for t in range(0, MaxData, step):\n",
    "            # x: the x^th day data in dataset\n",
    "            # t: starting time of eating\n",
    "            # t+ winlength:  end time of eating\n",
    "            sample = [x, t, t+winlength]\n",
    "            label = int((np.sum(GT[t:t+winlength])/winlength)>=gtperc)\n",
    "            \n",
    "            #Change labels if the flag of removerest or removewalk is enabled\n",
    "            if(label and removerest!=0): # Only ignore if in eating\n",
    "                isrest = int((np.sum(mrest[t:t+winlength])/winlength)>=0.65)\n",
    "                if(isrest and removerest==1): continue; # Do not consider this sample at all. Comment this if you want to move the sample to non-eating.\n",
    "                elif(isrest and removerest==2): label = 0;\n",
    "                else: label = 1    \n",
    "            if(label and removewalk!=0): # Only ignore if in eating\n",
    "                iswalk = int((np.sum(zc[t:t+winlength])/winlength)>=0.15)\n",
    "                if(iswalk and removewalk==1): continue;\n",
    "                elif(iswalk and removewalk==2): label=0;\n",
    "                else: label = 1\n",
    "#                fileeatingwalk+=1\n",
    "#                continue # Do not append this sample to the dataset\n",
    "                \n",
    "            if(t+winlength < MaxData): # Ignore last small window. Not ignoring results in a list rather than a numpy array.\n",
    "                filesamples.append(sample)\n",
    "                filelabels.append(label)\n",
    "                \n",
    "        #merge two lists\n",
    "        samples += filesamples\n",
    "        labels += filelabels\n",
    "        numsamples = len(filesamples)\n",
    "        totaleatingwalk += fileeatingwalk\n",
    "\n",
    "    samples_array = np.asarray(samples)\n",
    "    labels_array = np.asarray(labels)\n",
    "    #print(\"Total {:d} walking in eating\\n\".format(fileeatingwalk))\n",
    "    return len(df[\"Filenames\"]), AllNormalized, samples_array, labels_array\n",
    "\n",
    "\n",
    "\n",
    "def normalizeData(AllSmoothed, meanvals, stdvals):\n",
    "    AllNormalized = []\n",
    "    #for x in tqdm(range(len(AllSmoothed))):\n",
    "    for x in range(len(AllSmoothed)):\n",
    "        Smoothed = AllSmoothed[x]\n",
    "        Normalized = np.empty_like(Smoothed)\n",
    "        # Normalize\n",
    "        for i in range(6):\n",
    "            Normalized[:,i] = (Smoothed[:,i] - meanvals[i]) / stdvals[i]\n",
    "        \n",
    "        # Stick this Normalized data to the Full Array\n",
    "        AllNormalized.append(np.copy(Normalized))\n",
    "    \n",
    "    return AllNormalized\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original  Test example of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/354 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(730802, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 354/354 [02:34<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using every file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "### imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "shimmer_global_mean = [-0.012359981,-0.0051663737,0.011612018,\n",
    "                0.05796114,0.1477952,-0.034395125 ]\n",
    "\n",
    "shimmer_global_stddev = [0.05756385,0.040893298,0.043825723,\n",
    "                17.199743,15.311142,21.229317 ]\n",
    "\n",
    "shimmer_trended_mean = [-0.000002,-0.000002,-0.000000,\n",
    "                0.058144,0.147621,-0.033260 ]\n",
    "\n",
    "shimmer_trended_stddev = [0.037592,0.034135,0.032263,\n",
    "                17.209038,15.321441,21.242532 ]\n",
    "\n",
    "all_zero_means = [0,0,0,0,0,0]\n",
    "\n",
    "actigraph_global_means = [ 0.010016, -0.254719, 0.016803, \n",
    "                0.430628, 0.097660, 0.359574 ]\n",
    "\n",
    "actigraph_trended_means = [ -0.000001, 0.000022, -0.000002, \n",
    "                0.430628, 0.097660, 0.359574 ]\n",
    "\n",
    "\n",
    "meanvals = all_zero_means\n",
    "stdvals = shimmer_trended_stddev\n",
    "\n",
    "outfile = sys.stdout\n",
    "\n",
    "\n",
    "ModelNum = 3 #int(sys.argv[1])\n",
    "EPOCHS = 30 #int(sys.argv[2])\n",
    "winmin = 6 #int(sys.argv[1])\n",
    "stridesec = 15 # int(sys.argv[4])\n",
    "\n",
    "winlength = int(winmin * 60 * 15)\n",
    "step = int(stridesec * 15)\n",
    "start_time = datetime.now()\n",
    "\n",
    "NumFiles, AllNormalized, samples_array, labels_array = loadAllData2(winlength,\n",
    "                                                                            step,\n",
    "                                                                            meanvals, stdvals,\n",
    "                                                                            removerest=0,\n",
    "                                                                            removewalk=0,\n",
    "                                                                            removebias=0)\n",
    "print(\"Training using every file\\n\",file=outfile, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadshmfile(File_Name= \"../Data/ShimmerData/P2001/P2001.shm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(730802, 6)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87375"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='rbf')\n",
    "x = train_set['data'].to_list()\n",
    "ls=[]\n",
    "num = 800\n",
    "for i in range(num):\n",
    "    ls.append(x[i].ravel()) \n",
    "# ls[0]\n",
    "clf.fit(ls, train_set['label'].iloc[:num].tolist())\n",
    "clf.score(ls,  train_set['label'].iloc[:num].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set['label'].iloc[:4].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32400"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119024 entries, 0 to 119023\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  119024 non-null  int64 \n",
      " 1   data        119024 non-null  object\n",
      " 2   label       119024 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"cleaned_all_day_data.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[ 1.5942797e-01  9.0304792e-02  1.4607425e-01...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[ 0.15128006 -0.08849697 -0.1524664  -0.00511...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[[-0.3320187  -0.8915681  -0.8468274   0.13487...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[-0.52839035  0.26718208  1.2749779   0.18268...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[[ 0.8798676  -0.14580043  1.1407169   0.33000...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               data  label\n",
       "0           0  [[ 1.5942797e-01  9.0304792e-02  1.4607425e-01...      1\n",
       "1           1  [[ 0.15128006 -0.08849697 -0.1524664  -0.00511...      1\n",
       "2           2  [[-0.3320187  -0.8915681  -0.8468274   0.13487...      0\n",
       "3           3  [[-0.52839035  0.26718208  1.2749779   0.18268...      0\n",
       "4           4  [[ 0.8798676  -0.14580043  1.1407169   0.33000...      1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    [[ 0.15128006 -0.08849697 -0.1524664  -0.00511...\n",
       "2    [[-0.3320187  -0.8915681  -0.8468274   0.13487...\n",
       "3    [[-0.52839035  0.26718208  1.2749779   0.18268...\n",
       "4    [[ 0.8798676  -0.14580043  1.1407169   0.33000...\n",
       "Name: data, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df[\"data\"].iloc[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My default: Python3.7.7",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
