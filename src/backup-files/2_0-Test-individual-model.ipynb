{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Individual Model 1-wenkanw data\n",
    "## Hardware settings: 4cpus, 120GB,  1 gpus, gpu:V100, interconnection: 25ge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda Device Count:  2 Device Name:  Tesla V100-PCIE-16GB\n",
      "Torch version: 1.7.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from dataset import create_train_test_file_list, Person_MealsDataset, balance_data_indices\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device: \",device,\"Device Count: \", torch.cuda.device_count(), \"Device Name: \",torch.cuda.get_device_name()  )\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "### imports\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Turn off TensorFlow logging\n",
    "import tensorflow.compat.v1 as tf # maintain compatibility with TensorFlow 2.2.0\n",
    "\n",
    "import keras\n",
    "# from tensorflow.compat.v1.keras import backend as K # changed for compatibility with TensorFlow 2.2.0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "import loadfile\n",
    "import addons\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l1\n",
    "from keras.models import load_model, save_model\n",
    "\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "shimmer_global_mean = [-0.012359981,-0.0051663737,0.011612018,\n",
    "                        0.05796114,0.1477952,-0.034395125 ]\n",
    "\n",
    "shimmer_global_stddev = [0.05756385,0.040893298,0.043825723,\n",
    "                        17.199743,15.311142,21.229317 ]\n",
    "\n",
    "shimmer_trended_mean = [-0.000002,-0.000002,-0.000000,\n",
    "                0.058144,0.147621,-0.033260 ]\n",
    "\n",
    "shimmer_trended_stddev = [0.037592,0.034135,0.032263,\n",
    "                17.209038,15.321441,21.242532 ]\n",
    "\n",
    "all_zero_means = [0,0,0,0,0,0]\n",
    "\n",
    "meanvals = all_zero_means\n",
    "stdvals = shimmer_trended_stddev\n",
    "\n",
    "\n",
    "random_seed  = 1000\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_settings(winmin,stridesec, EPOCHS):\n",
    "    outfile = sys.stdout\n",
    "\n",
    "    winlength = int(winmin * 60 * 15)\n",
    "    step = int(stridesec * 15)\n",
    "    start_time = datetime.now()\n",
    "    arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "          \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "          \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "\n",
    "    [os.system(cmd) for cmd in arr]\n",
    "    print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "    print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "    print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/19/2020, 19:37:40\n",
      "WindowLength: 1.00 min (900 datum)\tSlide: 1 (15 datum)\tEpochs:30\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-14-20/10-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-3-20/10-3-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-9-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-14-20/11-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-16-20/11-16-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-18-20/11-18-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/lunch/lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Lunch/Lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-17-20/9-17-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-27-20/9-27-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-29-20/9-29-20.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-10-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-12-20/10-12-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-13-20/10-13-20.shm\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "winmin = 1 \n",
    "stridesec = 1\n",
    "\n",
    "print_settings(winmin,stridesec, EPOCHS)\n",
    "# Load the dataset\n",
    "meal_data_train = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "meal_data_test = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41878, 13520)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random.seed(random_seed)\n",
    "\n",
    "# load\n",
    "trainingsamples,traininglabels =  meal_data_train.data_indices, meal_data_train.labels\n",
    "testsamples,testlabels =  meal_data_test.data_indices, meal_data_test.labels\n",
    "\n",
    "train_shuffledUnderSampledBalancedIndices = balance_data_indices(traininglabels,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "test_shuffledUnderSampledBalancedIndices = balance_data_indices(testlabels,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "len(train_shuffledUnderSampledBalancedIndices),len(test_shuffledUnderSampledBalancedIndices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print([item for item, count in collections.Counter(train_shuffledUnderSampledBalancedIndices).items() if count > 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traininglabels[train_shuffledUnderSampledBalancedIndices]\n",
    "testlabels[test_shuffledUnderSampledBalancedIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_indices(X, y, test_size, random_seed = None):\n",
    "    \"\"\"\n",
    "    This function is to split the training set indices into validation set indices and training set indices\n",
    "    \n",
    "    X: indices of subset of the whole dataset\n",
    "    y: labels of the whole dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, train_test_split\n",
    "\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    if test_size ==0:\n",
    "        train_indices = X\n",
    "    elif test_size == 1:\n",
    "        test_indices = X\n",
    "    elif test_size >0 and test_size <1:\n",
    "        labels = y[X]\n",
    "        \n",
    "        train_indices, test_indices, _,_ = train_test_split(X,labels ,\n",
    "                                                            stratify=labels, \n",
    "                                                            test_size=test_size,random_state = random_seed)\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid split ratio: %.3f\"%(test_size))\n",
    "    if len(train_indices)>0:\n",
    "        print(\"Train set size: %d, with %d positive samples and %d negative samples\"%(len(y[train_indices]),sum(y[train_indices]==1),\n",
    "                                                                          sum(y[train_indices]==0)))\n",
    "    if len(test_indices)>0:\n",
    "        print(\"Test set size: %d, with %d positive samples and %d negative samples\"%(len(y[test_indices]),\n",
    "                                                                          sum(y[test_indices]==1),\n",
    "                                                                           sum(y[test_indices]==0)))\n",
    "    \n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 33502, with 16751 positive samples and 16751 negative samples\n",
      "Test set size: 8376, with 4188 positive samples and 4188 negative samples\n"
     ]
    }
   ],
   "source": [
    "train_indices, valid_indices = split_train_test_indices(X= train_shuffledUnderSampledBalancedIndices,\n",
    "                                                        y = traininglabels, test_size = 0.2,\n",
    "                                                       random_seed = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set(train_indices)\n",
    "b = set(valid_indices)\n",
    "c = a.intersection(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loader Created\n"
     ]
    }
   ],
   "source": [
    "train_set_balanced = torch.utils.data.Subset(meal_data_train, train_indices)\n",
    "valid_set_balanced = torch.utils.data.Subset(meal_data_train, valid_indices)\n",
    "\n",
    "test_set_balanced = torch.utils.data.Subset(meal_data_test, test_shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set_balanced ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "print(\"Data Loader Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set batch amounts: 524\n",
      "Test set : 131\n",
      "Start Training..\n",
      "Epoch: 0,  Epoch_Loss: 0.4655, Train Acc: 80.4758 %, Train Recall: 0.7556 \n",
      "Validation Acc:  82.1275 %,  Validation Recall: 0.8009 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 1,  Epoch_Loss: 0.4356, Train Acc: 82.1772 %, Train Recall: 0.7726 \n",
      "Validation Acc:  82.8558 %,  Validation Recall: 0.8665 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 2,  Epoch_Loss: 0.4375, Train Acc: 81.8548 %, Train Recall: 0.7711 \n",
      "Validation Acc:  82.3663 %,  Validation Recall: 0.7104 \n",
      "\n",
      "\n",
      "Epoch: 3,  Epoch_Loss: 0.4359, Train Acc: 82.0250 %, Train Recall: 0.7686 \n",
      "Validation Acc:  62.7507 %,  Validation Recall: 0.2569 \n",
      "\n",
      "\n",
      "Epoch: 4,  Epoch_Loss: 0.4326, Train Acc: 82.2578 %, Train Recall: 0.7720 \n",
      "Validation Acc:  83.0468 %,  Validation Recall: 0.7844 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 5,  Epoch_Loss: 0.4364, Train Acc: 82.2070 %, Train Recall: 0.7735 \n",
      "Validation Acc:  82.4379 %,  Validation Recall: 0.7151 \n",
      "\n",
      "\n",
      "Epoch: 6,  Epoch_Loss: 0.4328, Train Acc: 82.1235 %, Train Recall: 0.7692 \n",
      "Validation Acc:  80.0979 %,  Validation Recall: 0.8835 \n",
      "\n",
      "\n",
      "Epoch: 7,  Epoch_Loss: 0.3710, Train Acc: 85.6516 %, Train Recall: 0.8101 \n",
      "Validation Acc:  84.2646 %,  Validation Recall: 0.9164 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 8,  Epoch_Loss: 0.3577, Train Acc: 86.2098 %, Train Recall: 0.8152 \n",
      "Validation Acc:  87.6671 %,  Validation Recall: 0.8462 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 9,  Epoch_Loss: 0.3501, Train Acc: 86.6665 %, Train Recall: 0.8225 \n",
      "Validation Acc:  84.3959 %,  Validation Recall: 0.9222 \n",
      "\n",
      "\n",
      "Epoch: 10,  Epoch_Loss: 0.3209, Train Acc: 88.5290 %, Train Recall: 0.8537 \n",
      "Validation Acc:  89.7206 %,  Validation Recall: 0.8584 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 11,  Epoch_Loss: 0.3024, Train Acc: 89.3439 %, Train Recall: 0.8638 \n",
      "Validation Acc:  89.9713 %,  Validation Recall: 0.8532 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 12,  Epoch_Loss: 0.2918, Train Acc: 89.7230 %, Train Recall: 0.8674 \n",
      "Validation Acc:  90.0907 %,  Validation Recall: 0.8481 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 13,  Epoch_Loss: 0.2800, Train Acc: 90.3767 %, Train Recall: 0.8792 \n",
      "Validation Acc:  90.9742 %,  Validation Recall: 0.8766 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 14,  Epoch_Loss: 0.2777, Train Acc: 90.3856 %, Train Recall: 0.8787 \n",
      "Validation Acc:  91.1055 %,  Validation Recall: 0.8830 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 15,  Epoch_Loss: 0.2771, Train Acc: 90.4304 %, Train Recall: 0.8781 \n",
      "Validation Acc:  91.0578 %,  Validation Recall: 0.8856 \n",
      "\n",
      "\n",
      "Epoch: 16,  Epoch_Loss: 0.2762, Train Acc: 90.4692 %, Train Recall: 0.8798 \n",
      "Validation Acc:  91.1772 %,  Validation Recall: 0.8861 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 17,  Epoch_Loss: 0.2749, Train Acc: 90.6543 %, Train Recall: 0.8807 \n",
      "Validation Acc:  91.0817 %,  Validation Recall: 0.8794 \n",
      "\n",
      "\n",
      "Epoch: 18,  Epoch_Loss: 0.2751, Train Acc: 90.6274 %, Train Recall: 0.8810 \n",
      "Validation Acc:  91.1294 %,  Validation Recall: 0.8964 \n",
      "\n",
      "\n",
      "Epoch: 19,  Epoch_Loss: 0.2750, Train Acc: 90.6513 %, Train Recall: 0.8808 \n",
      "Validation Acc:  91.1772 %,  Validation Recall: 0.8937 \n",
      "\n",
      "\n",
      "Epoch: 20,  Epoch_Loss: 0.2748, Train Acc: 90.5498 %, Train Recall: 0.8813 \n",
      "Validation Acc:  91.2249 %,  Validation Recall: 0.8887 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 21,  Epoch_Loss: 0.2737, Train Acc: 90.6543 %, Train Recall: 0.8803 \n",
      "Validation Acc:  91.2727 %,  Validation Recall: 0.8883 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 22,  Epoch_Loss: 0.2742, Train Acc: 90.6274 %, Train Recall: 0.8815 \n",
      "Validation Acc:  91.1294 %,  Validation Recall: 0.8809 \n",
      "\n",
      "\n",
      "Epoch: 23,  Epoch_Loss: 0.2759, Train Acc: 90.6304 %, Train Recall: 0.8804 \n",
      "Validation Acc:  91.0936 %,  Validation Recall: 0.8809 \n",
      "\n",
      "\n",
      "Epoch: 24,  Epoch_Loss: 0.2740, Train Acc: 90.6006 %, Train Recall: 0.8805 \n",
      "Validation Acc:  91.1175 %,  Validation Recall: 0.8811 \n",
      "\n",
      "\n",
      "Epoch: 25,  Epoch_Loss: 0.2745, Train Acc: 90.6632 %, Train Recall: 0.8823 \n",
      "Validation Acc:  91.1294 %,  Validation Recall: 0.8926 \n",
      "\n",
      "\n",
      "Epoch: 26,  Epoch_Loss: 0.2753, Train Acc: 90.6244 %, Train Recall: 0.8814 \n",
      "Validation Acc:  91.2130 %,  Validation Recall: 0.8897 \n",
      "\n",
      "\n",
      "Epoch: 27,  Epoch_Loss: 0.2736, Train Acc: 90.6752 %, Train Recall: 0.8818 \n",
      "Validation Acc:  91.1294 %,  Validation Recall: 0.8823 \n",
      "\n",
      "\n",
      "Epoch: 28,  Epoch_Loss: 0.2746, Train Acc: 90.6125 %, Train Recall: 0.8802 \n",
      "Validation Acc:  91.1652 %,  Validation Recall: 0.8921 \n",
      "\n",
      "\n",
      "Epoch: 29,  Epoch_Loss: 0.2748, Train Acc: 90.5409 %, Train Recall: 0.8798 \n",
      "Validation Acc:  91.1772 %,  Validation Recall: 0.8887 \n",
      "\n",
      "\n",
      "Load Best Model.\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from utils import *\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "input_shape = (train_set_balanced[0][0].shape[0], train_set_balanced[0][0].shape[1])\n",
    "model_4 = Discriminator_ResNet( ngpu=1, input_shape =input_shape , out_fea = 1)\n",
    "# Since I use a dynamic created layer in network, need to input a sample to initialize the model first\n",
    "_ = model_4(torch.rand((1, input_shape[0],input_shape[1])))\n",
    "model_4.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_4 = optim.Adam(model_4.parameters(),lr=0.01,  weight_decay=0.1)\n",
    "lrscheduler_4 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_4, mode='min',patience= 2, factor = 0.1,threshold=0.01)\n",
    "dataloader = (train_loader, valid_loader)\n",
    "model_4, best_model_4,val_score_4,loss_ls_4, train_acc_ls_4, valid_acc_ls_4 = train_model(model_4,dataloader, optimizer_4, \n",
    "                                                                    criterion, lrscheduler_4, device= device,\n",
    "                                                                    n_epochs=30, patience = 5, l1_enabled=False,\n",
    "                                                                    checkpoint_name =\"../models/wenkanw_models/checkpoint_model_resnet.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.25739644970415\n"
     ]
    }
   ],
   "source": [
    "best_model_4.eval()\n",
    "acc, recall = eval_model(best_model_4, test_loader,device)\n",
    "print(\"Test Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.77 s, sys: 283 ms, total: 2.06 s\n",
      "Wall time: 6.08 s\n"
     ]
    }
   ],
   "source": [
    "%time acc, recall = eval_model(best_model_4, test_loader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.4 s, sys: 13.4 s, total: 42.8 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%time acc, recall = eval_model(best_model_4.to(\"cpu\"), test_loader,\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
