{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Test Model with different Window Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from dataset import create_train_test_file_list, Person_MealsDataset, balance_data_indices\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "### imports\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Turn off TensorFlow logging\n",
    "import tensorflow.compat.v1 as tf # maintain compatibility with TensorFlow 2.2.0\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "import keras\n",
    "# from tensorflow.compat.v1.keras import backend as K # changed for compatibility with TensorFlow 2.2.0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "import loadfile\n",
    "import addons\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l1\n",
    "from keras.models import load_model, save_model\n",
    "\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shimmer_global_mean = [-0.012359981,-0.0051663737,0.011612018,\n",
    "                        0.05796114,0.1477952,-0.034395125 ]\n",
    "\n",
    "shimmer_global_stddev = [0.05756385,0.040893298,0.043825723,\n",
    "                        17.199743,15.311142,21.229317 ]\n",
    "\n",
    "shimmer_trended_mean = [-0.000002,-0.000002,-0.000000,\n",
    "                0.058144,0.147621,-0.033260 ]\n",
    "\n",
    "shimmer_trended_stddev = [0.037592,0.034135,0.032263,\n",
    "                17.209038,15.321441,21.242532 ]\n",
    "\n",
    "all_zero_means = [0,0,0,0,0,0]\n",
    "\n",
    "meanvals = all_zero_means\n",
    "stdvals = shimmer_trended_stddev\n",
    "winmin = 6\n",
    "pathtemp = \"../models/ActiModels/M_F_\"\n",
    "modelpath = pathtemp + \"{:f}Min.h5\".format(winmin)\n",
    "jsonpath = pathtemp + \"{:f}Min.json\".format(winmin)\n",
    "\n",
    "random_seed  = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = {\"model\":[],\"win(sec)\":[], \"acc\":[],\"recall\":[], \"auc\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 ActiModel with window of 10 sec , slide of 1 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 17:06:10\n",
      "WindowLength: 0.17 min (150 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-14-20/10-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-3-20/10-3-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-9-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-14-20/11-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-16-20/11-16-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-18-20/11-18-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/lunch/lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Lunch/Lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-17-20/9-17-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-27-20/9-27-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-29-20/9-29-20.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-10-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-12-20/10-12-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-13-20/10-13-20.shm\n",
      "Data loaded\n",
      "Dataset ready\n"
     ]
    }
   ],
   "source": [
    "ModelNum = 3\n",
    "EPOCHS = 10\n",
    "winmin = 1/6 \n",
    "stridesec = 1\n",
    "\n",
    "outfile = sys.stdout\n",
    "\n",
    "winlength = int(winmin * 60 * 15)\n",
    "step = int(stridesec * 15)\n",
    "start_time = datetime.now()\n",
    "arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "      \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "      \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "[os.system(cmd) for cmd in arr]\n",
    "print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "meal_data_train = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "meal_data_test = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "# meal_data_train = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "\n",
    "print(\"Data loaded\", file=outfile, flush=True)\n",
    "print(\"Dataset ready\", file=outfile, flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[333898  20962]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(meal_data_train.labels, return_counts=True)\n",
    "print(unique_elements)\n",
    "print(counts_elements)\n",
    "\n",
    "### Set the seed for randomizing\n",
    "random.seed(random_seed)\n",
    "\n",
    "# load\n",
    "print(\"Training using every file\\n\",file=outfile, flush=True)\n",
    "trainingsamples,traininglabels =  meal_data_train.data_indices, meal_data_train.labels\n",
    "testsamples,testlabels =  meal_data_test.data_indices, meal_data_test.labels\n",
    "\n",
    "print(\"Creating balanced training data array\", file=outfile, flush=True)\n",
    "\n",
    "shuffledUnderSampledBalancedIndices = balance_data_indices(traininglabels,mode=\"under\", shuffle=True)\n",
    "balancedData,balancedLabels = meal_data_train.get_subset( shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "shuffledUnderSampledBalancedIndices = balance_data_indices(testlabels,mode=\"under\", shuffle=True)\n",
    "balancedData_test,balancedLabels_test = meal_data_test.get_subset( shuffledUnderSampledBalancedIndices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Training on 13520 samples of length 150\n",
      "\n",
      "Train on 33539 samples, validate on 8385 samples\n",
      "Epoch 1/10\n",
      "33539/33539 [==============================] - 2s 49us/sample - loss: 1.3804 - accuracy: 0.6802 - val_loss: 0.8150 - val_accuracy: 0.6809\n",
      "Epoch 2/10\n",
      "33539/33539 [==============================] - 2s 46us/sample - loss: 0.6922 - accuracy: 0.7135 - val_loss: 0.6634 - val_accuracy: 0.6745\n",
      "Epoch 3/10\n",
      "33539/33539 [==============================] - 2s 46us/sample - loss: 0.6120 - accuracy: 0.7284 - val_loss: 0.7413 - val_accuracy: 0.6275\n",
      "Epoch 4/10\n",
      "33539/33539 [==============================] - 2s 48us/sample - loss: 0.6295 - accuracy: 0.7217 - val_loss: 0.5864 - val_accuracy: 0.7420\n",
      "Epoch 5/10\n",
      "33539/33539 [==============================] - 2s 48us/sample - loss: 0.5802 - accuracy: 0.7467 - val_loss: 0.5727 - val_accuracy: 0.7485\n",
      "Epoch 6/10\n",
      "33539/33539 [==============================] - 2s 47us/sample - loss: 0.5673 - accuracy: 0.7555 - val_loss: 0.6882 - val_accuracy: 0.6547\n",
      "Epoch 7/10\n",
      "33539/33539 [==============================] - 2s 46us/sample - loss: 0.5884 - accuracy: 0.7438 - val_loss: 0.5674 - val_accuracy: 0.7556\n",
      "Epoch 8/10\n",
      "33539/33539 [==============================] - 2s 47us/sample - loss: 0.5570 - accuracy: 0.7600 - val_loss: 0.5507 - val_accuracy: 0.7643\n",
      "Epoch 9/10\n",
      "33539/33539 [==============================] - 2s 47us/sample - loss: 0.5478 - accuracy: 0.7681 - val_loss: 0.5715 - val_accuracy: 0.7481\n",
      "Epoch 10/10\n",
      "33539/33539 [==============================] - 2s 46us/sample - loss: 0.5483 - accuracy: 0.7670 - val_loss: 0.5489 - val_accuracy: 0.7628\n",
      "Max value:  0.76806104  at epoch 9\n"
     ]
    }
   ],
   "source": [
    "print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "print(\"Training on {:d} samples of length {:d}\\n\".format(len(shuffledUnderSampledBalancedIndices), len(balancedData[0])), file=outfile, flush=True)\n",
    "\n",
    "\n",
    "K.get_session().close()\n",
    "K.set_session(tf.Session())\n",
    "K.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "mcp_save = keras.callbacks.ModelCheckpoint(modelpath, save_best_only=True, monitor='accuracy')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(10, 44, strides=2,activation='relu', input_shape=(winlength, 6)))\n",
    "model.add(Conv1D(10, 20, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "model.add(Conv1D(10, 4, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "H = model.fit(x=balancedData, y = balancedLabels,\n",
    "#                validation_data=[undersampledTestData, undersampledTestLabels],\n",
    "            epochs = EPOCHS, batch_size=256, verbose=1, validation_split=0.2,\n",
    "            callbacks=[mcp_save]) # removed addons.LossHistory(jsonpath) for compatibility with TensorFlow 2.2.0, needs to be re-added at some point\n",
    "\n",
    "print(\"Max value: \", max(H.history['accuracy']), \" at epoch\", np.argmax(H.history['accuracy']) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6937130177514793\n",
      "Recall Accuracy: 0.6891521016900187\n",
      "AUC Score: 0.6938257097808892\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "predictions = model.predict(x=balancedData_test)\n",
    "threshold = 0.5\n",
    "acc_10s =  accuracy_score(predictions>threshold,balancedLabels_test)\n",
    "recall_10s = recall_score(predictions>threshold,balancedLabels_test)\n",
    "auc_10s = roc_auc_score(predictions>threshold,balancedLabels_test)\n",
    "print(\"Test Accuracy:\", acc_10s)\n",
    "print(\"Recall Accuracy:\", recall_10s)\n",
    "print(\"AUC Score:\", auc_10s)\n",
    "\n",
    "perf[\"model\"].append(\"ActiModel\")\n",
    "perf[\"win(sec)\"].append(10)\n",
    "perf[\"acc\"].append(acc_10s)\n",
    "perf[\"recall\"].append(recall_10s)\n",
    "perf[\"auc\"].append(auc_10s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelNum = 3\n",
    "EPOCHS = 10\n",
    "winmin = 3/6 \n",
    "stridesec = 1\n",
    "\n",
    "winlength = int(winmin * 60 * 15)\n",
    "step = int(stridesec * 15)\n",
    "start_time = datetime.now()\n",
    "arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "      \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "      \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "[os.system(cmd) for cmd in arr]\n",
    "print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "meal_data_train = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "meal_data_test = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "# meal_data_train = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "print(\"Data loaded\", file=outfile, flush=True)\n",
    "print(\"Dataset ready\", file=outfile, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 ActiModel with window of 30 sec , slide of 1sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 17:19:36\n",
      "WindowLength: 0.50 min (450 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-14-20/10-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-3-20/10-3-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-9-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-14-20/11-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-16-20/11-16-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-18-20/11-18-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/lunch/lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Lunch/Lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-17-20/9-17-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-27-20/9-27-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-29-20/9-29-20.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-10-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-12-20/10-12-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-13-20/10-13-20.shm\n",
      "Data loaded\n",
      "Dataset ready\n"
     ]
    }
   ],
   "source": [
    "ModelNum = 3\n",
    "EPOCHS = 10\n",
    "winmin = 3/6 \n",
    "stridesec = 1\n",
    "\n",
    "winlength = int(winmin * 60 * 15)\n",
    "step = int(stridesec * 15)\n",
    "start_time = datetime.now()\n",
    "arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "      \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "      \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "[os.system(cmd) for cmd in arr]\n",
    "print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "meal_data_train = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "meal_data_test = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "# meal_data_train = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "print(\"Data loaded\", file=outfile, flush=True)\n",
    "print(\"Dataset ready\", file=outfile, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[333626  20954]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(meal_data_train.labels, return_counts=True)\n",
    "print(unique_elements)\n",
    "print(counts_elements)\n",
    "\n",
    "### Set the seed for randomizing\n",
    "random.seed(random_seed)\n",
    "\n",
    "# load\n",
    "print(\"Training using every file\\n\",file=outfile, flush=True)\n",
    "trainingsamples,traininglabels =  meal_data_train.data_indices, meal_data_train.labels\n",
    "testsamples,testlabels =  meal_data_test.data_indices, meal_data_test.labels\n",
    "\n",
    "print(\"Creating balanced training data array\", file=outfile, flush=True)\n",
    "\n",
    "shuffledUnderSampledBalancedIndices = balance_data_indices(traininglabels,mode=\"under\", shuffle=True)\n",
    "balancedData,balancedLabels = meal_data_train.get_subset( shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "shuffledUnderSampledBalancedIndices = balance_data_indices(testlabels,mode=\"under\", shuffle=True)\n",
    "balancedData_test,balancedLabels_test = meal_data_test.get_subset( shuffledUnderSampledBalancedIndices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Training on 13520 samples of length 450\n",
      "\n",
      "Train on 33526 samples, validate on 8382 samples\n",
      "Epoch 1/10\n",
      "33526/33526 [==============================] - 6s 179us/sample - loss: 1.3119 - accuracy: 0.7147 - val_loss: 0.7001 - val_accuracy: 0.7848\n",
      "Epoch 2/10\n",
      "33526/33526 [==============================] - 6s 177us/sample - loss: 0.5997 - accuracy: 0.7822 - val_loss: 0.5219 - val_accuracy: 0.7963\n",
      "Epoch 3/10\n",
      "33526/33526 [==============================] - 6s 176us/sample - loss: 0.5170 - accuracy: 0.7944 - val_loss: 0.5003 - val_accuracy: 0.8010\n",
      "Epoch 4/10\n",
      "33526/33526 [==============================] - 6s 177us/sample - loss: 0.4964 - accuracy: 0.8056 - val_loss: 0.4829 - val_accuracy: 0.8144\n",
      "Epoch 5/10\n",
      "33526/33526 [==============================] - 6s 177us/sample - loss: 0.4858 - accuracy: 0.8081 - val_loss: 0.4770 - val_accuracy: 0.8085\n",
      "Epoch 6/10\n",
      "33526/33526 [==============================] - 6s 176us/sample - loss: 0.4715 - accuracy: 0.8158 - val_loss: 0.4574 - val_accuracy: 0.8178\n",
      "Epoch 7/10\n",
      "33526/33526 [==============================] - 6s 177us/sample - loss: 0.4533 - accuracy: 0.8264 - val_loss: 0.4477 - val_accuracy: 0.8263\n",
      "Epoch 8/10\n",
      "33526/33526 [==============================] - 6s 178us/sample - loss: 0.4448 - accuracy: 0.8311 - val_loss: 0.4317 - val_accuracy: 0.8386\n",
      "Epoch 9/10\n",
      "33526/33526 [==============================] - 6s 176us/sample - loss: 0.4333 - accuracy: 0.8378 - val_loss: 0.4271 - val_accuracy: 0.8416\n",
      "Epoch 10/10\n",
      "33526/33526 [==============================] - 6s 178us/sample - loss: 0.4263 - accuracy: 0.8422 - val_loss: 0.4206 - val_accuracy: 0.8428\n",
      "Max value:  0.84218216  at epoch 10\n"
     ]
    }
   ],
   "source": [
    "print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "print(\"Training on {:d} samples of length {:d}\\n\".format(len(shuffledUnderSampledBalancedIndices), len(balancedData[0])), file=outfile, flush=True)\n",
    "\n",
    "K.get_session().close()\n",
    "K.set_session(tf.Session())\n",
    "K.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "mcp_save = keras.callbacks.ModelCheckpoint(modelpath, save_best_only=True, monitor='accuracy')\n",
    "model = Sequential()\n",
    "model.add(Conv1D(10, 44, strides=2,activation='relu', input_shape=(winlength, 6)))\n",
    "model.add(Conv1D(10, 20, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "model.add(Conv1D(10, 4, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "H = model.fit(x=balancedData, y = balancedLabels,\n",
    "#                validation_data=[undersampledTestData, undersampledTestLabels],\n",
    "            epochs = EPOCHS, batch_size=256, verbose=1, validation_split=0.2,\n",
    "            callbacks=[mcp_save]) # removed addons.LossHistory(jsonpath) for compatibility with TensorFlow 2.2.0, needs to be re-added at some point\n",
    "\n",
    "print(\"Max value: \", max(H.history['accuracy']), \" at epoch\", np.argmax(H.history['accuracy']) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7502958579881657\n",
      "Recall Accuracy: 0.8199697428139183\n",
      "AUC Score: 0.7627545507072507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "predictions = model.predict(x=balancedData_test)\n",
    "threshold = 0.5\n",
    "acc_30s =  accuracy_score(predictions>threshold,balancedLabels_test)\n",
    "recall_30s = recall_score(predictions>threshold,balancedLabels_test)\n",
    "auc_30s = roc_auc_score(predictions>threshold,balancedLabels_test)\n",
    "print(\"Test Accuracy:\", acc_30s)\n",
    "print(\"Recall Accuracy:\", recall_30s)\n",
    "print(\"AUC Score:\", auc_30s)\n",
    "\n",
    "perf[\"model\"].append(\"ActiModel\")\n",
    "perf[\"win(sec)\"].append(30)\n",
    "perf[\"acc\"].append(acc_30s)\n",
    "perf[\"recall\"].append(recall_30s)\n",
    "perf[\"auc\"].append(auc_30s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 ActiModel with window of 60 sec , slide of 1sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 17:24:47\n",
      "WindowLength: 1.00 min (900 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-14-20/10-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-3-20/10-3-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-9-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-14-20/11-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-16-20/11-16-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-18-20/11-18-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/lunch/lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Lunch/Lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-17-20/9-17-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-27-20/9-27-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-29-20/9-29-20.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-10-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-12-20/10-12-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-13-20/10-13-20.shm\n",
      "Data loaded\n",
      "Dataset ready\n"
     ]
    }
   ],
   "source": [
    "ModelNum = 3\n",
    "EPOCHS = 10\n",
    "winmin = 1\n",
    "stridesec = 1\n",
    "\n",
    "winlength = int(winmin * 60 * 15)\n",
    "step = int(stridesec * 15)\n",
    "start_time = datetime.now()\n",
    "arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "      \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "      \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "[os.system(cmd) for cmd in arr]\n",
    "print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "meal_data_train = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "meal_data_test = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "# meal_data_train = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "print(\"Data loaded\", file=outfile, flush=True)\n",
    "print(\"Dataset ready\", file=outfile, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[333221  20939]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(meal_data_train.labels, return_counts=True)\n",
    "print(unique_elements)\n",
    "print(counts_elements)\n",
    "\n",
    "### Set the seed for randomizing\n",
    "random.seed(random_seed)\n",
    "\n",
    "# load\n",
    "print(\"Training using every file\\n\",file=outfile, flush=True)\n",
    "trainingsamples,traininglabels =  meal_data_train.data_indices, meal_data_train.labels\n",
    "testsamples,testlabels =  meal_data_test.data_indices, meal_data_test.labels\n",
    "\n",
    "print(\"Creating balanced training data array\", file=outfile, flush=True)\n",
    "\n",
    "shuffledUnderSampledBalancedIndices = balance_data_indices(traininglabels,mode=\"under\", shuffle=True)\n",
    "balancedData,balancedLabels = meal_data_train.get_subset( shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "shuffledUnderSampledBalancedIndices = balance_data_indices(testlabels,mode=\"under\", shuffle=True)\n",
    "balancedData_test,balancedLabels_test = meal_data_test.get_subset( shuffledUnderSampledBalancedIndices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Training on 13520 samples of length 900\n",
      "\n",
      "Train on 33502 samples, validate on 8376 samples\n",
      "Epoch 1/10\n",
      "33502/33502 [==============================] - 10s 298us/sample - loss: 1.2869 - accuracy: 0.8111 - val_loss: 0.6700 - val_accuracy: 0.8432\n",
      "Epoch 2/10\n",
      "33502/33502 [==============================] - 10s 296us/sample - loss: 0.5350 - accuracy: 0.8577 - val_loss: 0.4510 - val_accuracy: 0.8637\n",
      "Epoch 3/10\n",
      "33502/33502 [==============================] - 10s 294us/sample - loss: 0.4213 - accuracy: 0.8722 - val_loss: 0.3992 - val_accuracy: 0.8739\n",
      "Epoch 4/10\n",
      "33502/33502 [==============================] - 10s 295us/sample - loss: 0.3847 - accuracy: 0.8817 - val_loss: 0.3759 - val_accuracy: 0.8838\n",
      "Epoch 5/10\n",
      "33502/33502 [==============================] - 10s 295us/sample - loss: 0.3657 - accuracy: 0.8887 - val_loss: 0.3758 - val_accuracy: 0.8764\n",
      "Epoch 6/10\n",
      "33502/33502 [==============================] - 10s 293us/sample - loss: 0.3514 - accuracy: 0.8929 - val_loss: 0.3487 - val_accuracy: 0.8912\n",
      "Epoch 7/10\n",
      "33502/33502 [==============================] - 10s 294us/sample - loss: 0.3393 - accuracy: 0.8960 - val_loss: 0.3337 - val_accuracy: 0.8989\n",
      "Epoch 8/10\n",
      "33502/33502 [==============================] - 10s 297us/sample - loss: 0.3280 - accuracy: 0.9017 - val_loss: 0.3350 - val_accuracy: 0.8983\n",
      "Epoch 9/10\n",
      "33502/33502 [==============================] - 10s 297us/sample - loss: 0.3221 - accuracy: 0.9032 - val_loss: 0.3202 - val_accuracy: 0.9031\n",
      "Epoch 10/10\n",
      "33502/33502 [==============================] - 10s 295us/sample - loss: 0.3136 - accuracy: 0.9065 - val_loss: 0.3257 - val_accuracy: 0.8985\n",
      "Max value:  0.9065429  at epoch 10\n"
     ]
    }
   ],
   "source": [
    "print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "print(\"Training on {:d} samples of length {:d}\\n\".format(len(shuffledUnderSampledBalancedIndices), len(balancedData[0])), file=outfile, flush=True)\n",
    "\n",
    "K.get_session().close()\n",
    "K.set_session(tf.Session())\n",
    "K.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "mcp_save = keras.callbacks.ModelCheckpoint(modelpath, save_best_only=True, monitor='accuracy')\n",
    "model = Sequential()\n",
    "model.add(Conv1D(10, 44, strides=2,activation='relu', input_shape=(winlength, 6)))\n",
    "model.add(Conv1D(10, 20, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "model.add(Conv1D(10, 4, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "H = model.fit(x=balancedData, y = balancedLabels,\n",
    "#                validation_data=[undersampledTestData, undersampledTestLabels],\n",
    "            epochs = EPOCHS, batch_size=256, verbose=1, validation_split=0.2,\n",
    "            callbacks=[mcp_save]) # removed addons.LossHistory(jsonpath) for compatibility with TensorFlow 2.2.0, needs to be re-added at some point\n",
    "\n",
    "print(\"Max value: \", max(H.history['accuracy']), \" at epoch\", np.argmax(H.history['accuracy']) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7937130177514793\n",
      "Recall Accuracy: 0.9034749034749034\n",
      "AUC Score: 0.8171869226061573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "predictions = model.predict(x=balancedData_test)\n",
    "threshold = 0.5\n",
    "acc_60s =  accuracy_score(predictions>threshold,balancedLabels_test)\n",
    "recall_60s = recall_score(predictions>threshold,balancedLabels_test)\n",
    "auc_60s = roc_auc_score(predictions>threshold,balancedLabels_test)\n",
    "print(\"Test Accuracy:\", acc_60s)\n",
    "print(\"Recall Accuracy:\", recall_60s)\n",
    "print(\"AUC Score:\", auc_60s)\n",
    "\n",
    "perf[\"model\"].append(\"ActiModel\")\n",
    "perf[\"win(sec)\"].append(60)\n",
    "perf[\"acc\"].append(acc_60s)\n",
    "perf[\"recall\"].append(recall_60s)\n",
    "perf[\"auc\"].append(auc_60s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>win(sec)</th>\n",
       "      <th>acc</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>10</td>\n",
       "      <td>0.693713</td>\n",
       "      <td>0.689152</td>\n",
       "      <td>0.693826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>30</td>\n",
       "      <td>0.750296</td>\n",
       "      <td>0.819970</td>\n",
       "      <td>0.762755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>60</td>\n",
       "      <td>0.793713</td>\n",
       "      <td>0.903475</td>\n",
       "      <td>0.817187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  win(sec)       acc    recall       auc\n",
       "0  ActiModel        10  0.693713  0.689152  0.693826\n",
       "1  ActiModel        30  0.750296  0.819970  0.762755\n",
       "2  ActiModel        60  0.793713  0.903475  0.817187"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_df = pd.DataFrame(perf)\n",
    "perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ActiModel with different window size , slide of 1 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_windowsize(win_ls = [],EPOCHS = 10,stridesec = 1,name = \"wenkanw\"):\n",
    "    \n",
    "    \n",
    "    perf = {\"model\":[],\"win(sec)\":[], \"acc\":[],\"recall\":[], \"auc\":[]}\n",
    "    model_ls = []\n",
    "    hist_ls = []\n",
    "    for winsize in win_ls:\n",
    "        winmin = winsize\n",
    "        winlength = int(winmin * 60 * 15)\n",
    "        step = int(stridesec * 15)\n",
    "        start_time = datetime.now()\n",
    "        arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "              \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "              \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "        [os.system(cmd) for cmd in arr]\n",
    "        print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "        print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "        print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "\n",
    "        # Load the dataset\n",
    "        \n",
    "        meal_data_train = Person_MealsDataset(person_name= name, file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "        meal_data_test = Person_MealsDataset(person_name= name, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "        # meal_data_train = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "        #print(\"Data loaded\", file=outfile, flush=True)\n",
    "        #print(\"Dataset ready\", file=outfile, flush=True)\n",
    "\n",
    "        unique_elements, counts_elements = np.unique(meal_data_train.labels, return_counts=True)\n",
    "        print(unique_elements)\n",
    "        print(counts_elements)\n",
    "\n",
    "        ### Set the seed for randomizing\n",
    "        random.seed(random_seed)\n",
    "\n",
    "        # load\n",
    "        print(\"Training using every file\\n\",file=outfile, flush=True)\n",
    "        trainingsamples,traininglabels =  meal_data_train.data_indices, meal_data_train.labels\n",
    "        testsamples,testlabels =  meal_data_test.data_indices, meal_data_test.labels\n",
    "\n",
    "        print(\"Creating balanced training data array\", file=outfile, flush=True)\n",
    "\n",
    "        shuffledUnderSampledBalancedIndices = balance_data_indices(traininglabels,mode=\"under\", shuffle=True)\n",
    "        balancedData,balancedLabels = meal_data_train.get_subset( shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "        shuffledUnderSampledBalancedIndices = balance_data_indices(testlabels,mode=\"under\", shuffle=True)\n",
    "        balancedData_test,balancedLabels_test = meal_data_test.get_subset( shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "        print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "        print(\"Training on {:d} samples of length {:d}\\n\".format(len(shuffledUnderSampledBalancedIndices), len(balancedData[0])), file=outfile, flush=True)\n",
    "\n",
    "        K.get_session().close()\n",
    "        K.set_session(tf.Session())\n",
    "        K.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "        mcp_save = keras.callbacks.ModelCheckpoint(modelpath, save_best_only=True, monitor='accuracy')\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(10, 44, strides=2,activation='relu', input_shape=(winlength, 6)))\n",
    "        model.add(Conv1D(10, 20, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "        model.add(Conv1D(10, 4, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(200, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        H = model.fit(x=balancedData, y = balancedLabels,\n",
    "        #                validation_data=[undersampledTestData, undersampledTestLabels],\n",
    "                    epochs = EPOCHS, batch_size=256, verbose=1, validation_split=0.2,\n",
    "                    callbacks=[mcp_save]) # removed addons.LossHistory(jsonpath) for compatibility with TensorFlow 2.2.0, needs to be re-added at some point\n",
    "\n",
    "        print(\"Max value: \", max(H.history['accuracy']), \" at epoch\", np.argmax(H.history['accuracy']) + 1)\n",
    "\n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "        predictions = model.predict(x=balancedData_test)\n",
    "        threshold = 0.5\n",
    "        acc =  accuracy_score(predictions>threshold,balancedLabels_test)\n",
    "        recall = recall_score(predictions>threshold,balancedLabels_test)\n",
    "        auc = roc_auc_score(predictions>threshold,balancedLabels_test)\n",
    "        print(\"Test Accuracy:\", acc)\n",
    "        print(\"Recall Accuracy:\", recall)\n",
    "        print(\"AUC Score:\", auc)\n",
    "\n",
    "        perf[\"model\"].append(\"ActiModel\")\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        perf[\"acc\"].append(acc)\n",
    "        perf[\"recall\"].append(recall)\n",
    "        perf[\"auc\"].append(auc)\n",
    "        model_ls.append(model)\n",
    "        hist_ls.append(H)\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    return perf_df, model_ls, hist_ls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 17:48:30\n",
      "WindowLength: 0.50 min (450 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-14-20/10-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-3-20/10-3-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-9-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-14-20/11-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-16-20/11-16-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-18-20/11-18-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/lunch/lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Lunch/Lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-17-20/9-17-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-27-20/9-27-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-29-20/9-29-20.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-10-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-12-20/10-12-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-13-20/10-13-20.shm\n",
      "[0 1]\n",
      "[333626  20954]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 13520 samples of length 450\n",
      "\n",
      "Train on 33526 samples, validate on 8382 samples\n",
      "Epoch 1/10\n",
      "33526/33526 [==============================] - 6s 181us/sample - loss: 1.3164 - accuracy: 0.7348 - val_loss: 0.7291 - val_accuracy: 0.7799\n",
      "Epoch 2/10\n",
      "33526/33526 [==============================] - 6s 178us/sample - loss: 0.6045 - accuracy: 0.8018 - val_loss: 0.5379 - val_accuracy: 0.8040\n",
      "Epoch 3/10\n",
      "33526/33526 [==============================] - 6s 177us/sample - loss: 0.5096 - accuracy: 0.8123 - val_loss: 0.4964 - val_accuracy: 0.8153\n",
      "Epoch 4/10\n",
      "33526/33526 [==============================] - 6s 183us/sample - loss: 0.4793 - accuracy: 0.8210 - val_loss: 0.4807 - val_accuracy: 0.8101\n",
      "Epoch 5/10\n",
      "33526/33526 [==============================] - 6s 176us/sample - loss: 0.4641 - accuracy: 0.8259 - val_loss: 0.4589 - val_accuracy: 0.8232\n",
      "Epoch 6/10\n",
      "33526/33526 [==============================] - 6s 180us/sample - loss: 0.4544 - accuracy: 0.8300 - val_loss: 0.4532 - val_accuracy: 0.8194\n",
      "Epoch 7/10\n",
      "33526/33526 [==============================] - 6s 180us/sample - loss: 0.4462 - accuracy: 0.8323 - val_loss: 0.4389 - val_accuracy: 0.8325\n",
      "Epoch 8/10\n",
      "33526/33526 [==============================] - 6s 178us/sample - loss: 0.4302 - accuracy: 0.8403 - val_loss: 0.4349 - val_accuracy: 0.8317\n",
      "Epoch 9/10\n",
      "33526/33526 [==============================] - 6s 180us/sample - loss: 0.4230 - accuracy: 0.8446 - val_loss: 0.4222 - val_accuracy: 0.8425\n",
      "Epoch 10/10\n",
      "33526/33526 [==============================] - 6s 176us/sample - loss: 0.4174 - accuracy: 0.8475 - val_loss: 0.4187 - val_accuracy: 0.8419\n",
      "Max value:  0.8474617  at epoch 10\n",
      "Test Accuracy: 0.7571005917159763\n",
      "Recall Accuracy: 0.8784843205574913\n",
      "AUC Score: 0.7865763896694267\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 17:50:45\n",
      "WindowLength: 1.00 min (900 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-14-20/10-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-3-20/10-3-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-9-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-14-20/11-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-16-20/11-16-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-18-20/11-18-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/lunch/lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Lunch/Lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-17-20/9-17-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-27-20/9-27-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-29-20/9-29-20.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-10-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-12-20/10-12-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-13-20/10-13-20.shm\n",
      "[0 1]\n",
      "[333221  20939]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 13520 samples of length 900\n",
      "\n",
      "Train on 33502 samples, validate on 8376 samples\n",
      "Epoch 1/10\n",
      "33502/33502 [==============================] - 12s 353us/sample - loss: 1.3016 - accuracy: 0.7772 - val_loss: 0.7049 - val_accuracy: 0.8382\n",
      "Epoch 2/10\n",
      "33502/33502 [==============================] - 12s 348us/sample - loss: 0.5796 - accuracy: 0.8452 - val_loss: 0.4833 - val_accuracy: 0.8508\n",
      "Epoch 3/10\n",
      "33502/33502 [==============================] - 11s 342us/sample - loss: 0.4554 - accuracy: 0.8573 - val_loss: 0.4223 - val_accuracy: 0.8577\n",
      "Epoch 4/10\n",
      "33502/33502 [==============================] - 12s 349us/sample - loss: 0.4102 - accuracy: 0.8656 - val_loss: 0.4023 - val_accuracy: 0.8664\n",
      "Epoch 5/10\n",
      "33502/33502 [==============================] - 12s 349us/sample - loss: 0.3799 - accuracy: 0.8744 - val_loss: 0.3762 - val_accuracy: 0.8751\n",
      "Epoch 6/10\n",
      "33502/33502 [==============================] - 12s 354us/sample - loss: 0.3692 - accuracy: 0.8762 - val_loss: 0.3568 - val_accuracy: 0.8738\n",
      "Epoch 7/10\n",
      "33502/33502 [==============================] - 12s 349us/sample - loss: 0.3495 - accuracy: 0.8847 - val_loss: 0.3459 - val_accuracy: 0.8813\n",
      "Epoch 8/10\n",
      "33502/33502 [==============================] - 12s 349us/sample - loss: 0.3421 - accuracy: 0.8907 - val_loss: 0.3392 - val_accuracy: 0.8948\n",
      "Epoch 9/10\n",
      "33502/33502 [==============================] - 12s 350us/sample - loss: 0.3271 - accuracy: 0.8971 - val_loss: 0.3184 - val_accuracy: 0.9023\n",
      "Epoch 10/10\n",
      "33502/33502 [==============================] - 14s 410us/sample - loss: 0.3136 - accuracy: 0.9033 - val_loss: 0.3387 - val_accuracy: 0.8995\n",
      "Max value:  0.9032894  at epoch 10\n",
      "Test Accuracy: 0.8005917159763314\n",
      "Recall Accuracy: 0.8335521996060407\n",
      "AUC Score: 0.8035558520916579\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 17:54:02\n",
      "WindowLength: 2.00 min (1800 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-14-20/10-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-3-20/10-3-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-9-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-14-20/11-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-16-20/11-16-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-18-20/11-18-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/lunch/lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Lunch/Lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-17-20/9-17-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-27-20/9-27-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-29-20/9-29-20.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-10-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-12-20/10-12-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-13-20/10-13-20.shm\n",
      "[0 1]\n",
      "[332411  20909]\n",
      "Training using every file\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 13520 samples of length 1800\n",
      "\n",
      "Train on 33454 samples, validate on 8364 samples\n",
      "Epoch 1/10\n",
      "33454/33454 [==============================] - 31s 916us/sample - loss: 1.2647 - accuracy: 0.8049 - val_loss: 0.5890 - val_accuracy: 0.8883\n",
      "Epoch 2/10\n",
      "33454/33454 [==============================] - 23s 684us/sample - loss: 0.4680 - accuracy: 0.8894 - val_loss: 0.3678 - val_accuracy: 0.9084\n",
      "Epoch 3/10\n",
      "33454/33454 [==============================] - 23s 693us/sample - loss: 0.3477 - accuracy: 0.9057 - val_loss: 0.3255 - val_accuracy: 0.9121\n",
      "Epoch 4/10\n",
      "33454/33454 [==============================] - 29s 856us/sample - loss: 0.3380 - accuracy: 0.9054 - val_loss: 0.3098 - val_accuracy: 0.9195\n",
      "Epoch 5/10\n",
      "33454/33454 [==============================] - 31s 932us/sample - loss: 0.3115 - accuracy: 0.9148 - val_loss: 0.3033 - val_accuracy: 0.9191\n",
      "Epoch 6/10\n",
      "33454/33454 [==============================] - 33s 979us/sample - loss: 0.2943 - accuracy: 0.9217 - val_loss: 0.2779 - val_accuracy: 0.9316\n",
      "Epoch 7/10\n",
      "33454/33454 [==============================] - 37s 1ms/sample - loss: 0.2890 - accuracy: 0.9240 - val_loss: 0.2689 - val_accuracy: 0.9346\n",
      "Epoch 8/10\n",
      "33454/33454 [==============================] - 37s 1ms/sample - loss: 0.2837 - accuracy: 0.9234 - val_loss: 0.2701 - val_accuracy: 0.9345\n",
      "Epoch 9/10\n",
      "33454/33454 [==============================] - 36s 1ms/sample - loss: 0.2673 - accuracy: 0.9294 - val_loss: 0.2512 - val_accuracy: 0.9382\n",
      "Epoch 10/10\n",
      "33454/33454 [==============================] - 23s 689us/sample - loss: 0.2550 - accuracy: 0.9349 - val_loss: 0.2395 - val_accuracy: 0.9463\n",
      "Max value:  0.9348957  at epoch 10\n",
      "Test Accuracy: 0.8245562130177515\n",
      "Recall Accuracy: 0.9201455381080046\n",
      "AUC Score: 0.8422732993022549\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 18:00:27\n",
      "WindowLength: 3.00 min (2700 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-14-20/10-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-3-20/10-3-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-9-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-14-20/11-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-16-20/11-16-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-18-20/11-18-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/lunch/lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Lunch/Lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-17-20/9-17-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-27-20/9-27-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-29-20/9-29-20.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-10-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-12-20/10-12-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-13-20/10-13-20.shm\n",
      "[0 1]\n",
      "[331615  20865]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 13520 samples of length 2700\n",
      "\n",
      "Train on 33384 samples, validate on 8346 samples\n",
      "Epoch 1/10\n",
      "33384/33384 [==============================] - 47s 1ms/sample - loss: 1.2811 - accuracy: 0.7994 - val_loss: 0.6129 - val_accuracy: 0.8744\n",
      "Epoch 2/10\n",
      "33384/33384 [==============================] - 57s 2ms/sample - loss: 0.4723 - accuracy: 0.9013 - val_loss: 0.3683 - val_accuracy: 0.9155\n",
      "Epoch 3/10\n",
      "33384/33384 [==============================] - 56s 2ms/sample - loss: 0.3587 - accuracy: 0.9153 - val_loss: 0.3245 - val_accuracy: 0.9195\n",
      "Epoch 4/10\n",
      "33384/33384 [==============================] - 56s 2ms/sample - loss: 0.3271 - accuracy: 0.9201 - val_loss: 0.3473 - val_accuracy: 0.9149\n",
      "Epoch 5/10\n",
      "33384/33384 [==============================] - 56s 2ms/sample - loss: 0.3033 - accuracy: 0.9251 - val_loss: 0.2708 - val_accuracy: 0.9340\n",
      "Epoch 6/10\n",
      "33384/33384 [==============================] - 39s 1ms/sample - loss: 0.2788 - accuracy: 0.9311 - val_loss: 0.3005 - val_accuracy: 0.9111\n",
      "Epoch 7/10\n",
      "33384/33384 [==============================] - 34s 1ms/sample - loss: 0.2647 - accuracy: 0.9338 - val_loss: 0.2381 - val_accuracy: 0.9448\n",
      "Epoch 8/10\n",
      "33384/33384 [==============================] - 34s 1ms/sample - loss: 0.2507 - accuracy: 0.9386 - val_loss: 0.2296 - val_accuracy: 0.9497\n",
      "Epoch 9/10\n",
      "33384/33384 [==============================] - 35s 1ms/sample - loss: 0.2376 - accuracy: 0.9431 - val_loss: 0.2703 - val_accuracy: 0.9335\n",
      "Epoch 10/10\n",
      "33384/33384 [==============================] - 34s 1ms/sample - loss: 0.2299 - accuracy: 0.9481 - val_loss: 0.2235 - val_accuracy: 0.9422\n",
      "Max value:  0.94805896  at epoch 10\n",
      "Test Accuracy: 0.8336538461538462\n",
      "Recall Accuracy: 0.9649556792413936\n",
      "AUC Score: 0.8625678153964493\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 18:09:21\n",
      "WindowLength: 4.00 min (3600 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-14-20/10-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-3-20/10-3-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-9-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-14-20/11-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-16-20/11-16-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-18-20/11-18-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/lunch/lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Lunch/Lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-17-20/9-17-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-27-20/9-27-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-29-20/9-29-20.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-10-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-12-20/10-12-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-13-20/10-13-20.shm\n",
      "[0 1]\n",
      "[330862  20778]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 13520 samples of length 3600\n",
      "\n",
      "Train on 33244 samples, validate on 8312 samples\n",
      "Epoch 1/10\n",
      "33244/33244 [==============================] - 47s 1ms/sample - loss: 1.2929 - accuracy: 0.7801 - val_loss: 0.5793 - val_accuracy: 0.8890\n",
      "Epoch 2/10\n",
      "33244/33244 [==============================] - 47s 1ms/sample - loss: 0.4523 - accuracy: 0.8972 - val_loss: 0.3989 - val_accuracy: 0.8949\n",
      "Epoch 3/10\n",
      "33244/33244 [==============================] - 48s 1ms/sample - loss: 0.3458 - accuracy: 0.9041 - val_loss: 0.3178 - val_accuracy: 0.9135\n",
      "Epoch 4/10\n",
      "33244/33244 [==============================] - 47s 1ms/sample - loss: 0.2999 - accuracy: 0.9186 - val_loss: 0.2821 - val_accuracy: 0.9210\n",
      "Epoch 5/10\n",
      "33244/33244 [==============================] - 47s 1ms/sample - loss: 0.2780 - accuracy: 0.9235 - val_loss: 0.2593 - val_accuracy: 0.9266\n",
      "Epoch 6/10\n",
      "33244/33244 [==============================] - 47s 1ms/sample - loss: 0.2549 - accuracy: 0.9288 - val_loss: 0.2467 - val_accuracy: 0.9318\n",
      "Epoch 7/10\n",
      "33244/33244 [==============================] - 47s 1ms/sample - loss: 0.2333 - accuracy: 0.9348 - val_loss: 0.2294 - val_accuracy: 0.9388\n",
      "Epoch 8/10\n",
      "33244/33244 [==============================] - 47s 1ms/sample - loss: 0.2225 - accuracy: 0.9409 - val_loss: 0.2095 - val_accuracy: 0.9465\n",
      "Epoch 9/10\n",
      "33244/33244 [==============================] - 47s 1ms/sample - loss: 0.2162 - accuracy: 0.9444 - val_loss: 0.2060 - val_accuracy: 0.9420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "33244/33244 [==============================] - 47s 1ms/sample - loss: 0.2003 - accuracy: 0.9521 - val_loss: 0.1934 - val_accuracy: 0.9518\n",
      "Max value:  0.95214176  at epoch 10\n",
      "Test Accuracy: 0.8451923076923077\n",
      "Recall Accuracy: 0.9346246973365617\n",
      "AUC Score: 0.8604542944418057\n"
     ]
    }
   ],
   "source": [
    "perf_df, model_ls, hist_ls = test_windowsize(win_ls = [3/6, 1, 2, 3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>win(sec)</th>\n",
       "      <th>acc</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.757101</td>\n",
       "      <td>0.878484</td>\n",
       "      <td>0.786576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.800592</td>\n",
       "      <td>0.833552</td>\n",
       "      <td>0.803556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.824556</td>\n",
       "      <td>0.920146</td>\n",
       "      <td>0.842273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.833654</td>\n",
       "      <td>0.964956</td>\n",
       "      <td>0.862568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.845192</td>\n",
       "      <td>0.934625</td>\n",
       "      <td>0.860454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  win(sec)       acc    recall       auc\n",
       "0  ActiModel      30.0  0.757101  0.878484  0.786576\n",
       "1  ActiModel      60.0  0.800592  0.833552  0.803556\n",
       "2  ActiModel     120.0  0.824556  0.920146  0.842273\n",
       "3  ActiModel     180.0  0.833654  0.964956  0.862568\n",
       "4  ActiModel     240.0  0.845192  0.934625  0.860454"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 18:49:48\n",
      "WindowLength: 0.50 min (450 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/adam-data/11-12-2020/11-12-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-16-2020/11-16-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-17-2020/11-17-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-24-2020/11-24-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-25-2020/11-25-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-26-2020/11-26-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-30-2020/11-30-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-01-2020/12-01-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-02-2020/12-02-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-03-2020/12-03-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-04-2020/12-04-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-07-2020/12-07-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-08-2020/12-08-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-09-2020/12-09-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-10-2020/12-10-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-11-2020/12-11-2020.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/adam-data/09-22-2020/09-22-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/10-28-2020/10-28-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-09-2020/11-09-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-11-2020/11-11-2020-1.shm\n",
      "[0 1]\n",
      "[505103  14121]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 5198 samples of length 450\n",
      "\n",
      "Train on 22593 samples, validate on 5649 samples\n",
      "Epoch 1/10\n",
      "22593/22593 [==============================] - 4s 186us/sample - loss: 1.5217 - accuracy: 0.7648 - val_loss: 0.8100 - val_accuracy: 0.8681\n",
      "Epoch 2/10\n",
      "22593/22593 [==============================] - 4s 179us/sample - loss: 0.6541 - accuracy: 0.8698 - val_loss: 0.5208 - val_accuracy: 0.8910\n",
      "Epoch 3/10\n",
      "22593/22593 [==============================] - 4s 178us/sample - loss: 0.4671 - accuracy: 0.8859 - val_loss: 0.4007 - val_accuracy: 0.8993\n",
      "Epoch 4/10\n",
      "22593/22593 [==============================] - 4s 175us/sample - loss: 0.3762 - accuracy: 0.9000 - val_loss: 0.3467 - val_accuracy: 0.9095\n",
      "Epoch 5/10\n",
      "22593/22593 [==============================] - 4s 174us/sample - loss: 0.3333 - accuracy: 0.9101 - val_loss: 0.3050 - val_accuracy: 0.9235\n",
      "Epoch 6/10\n",
      "22593/22593 [==============================] - 4s 179us/sample - loss: 0.3012 - accuracy: 0.9184 - val_loss: 0.3047 - val_accuracy: 0.9092\n",
      "Epoch 7/10\n",
      "22593/22593 [==============================] - 4s 175us/sample - loss: 0.2907 - accuracy: 0.9223 - val_loss: 0.2796 - val_accuracy: 0.9140\n",
      "Epoch 8/10\n",
      "22593/22593 [==============================] - 4s 177us/sample - loss: 0.2668 - accuracy: 0.9286 - val_loss: 0.2572 - val_accuracy: 0.9333\n",
      "Epoch 9/10\n",
      "22593/22593 [==============================] - 4s 176us/sample - loss: 0.2546 - accuracy: 0.9310 - val_loss: 0.2549 - val_accuracy: 0.9269\n",
      "Epoch 10/10\n",
      "22593/22593 [==============================] - 4s 177us/sample - loss: 0.2455 - accuracy: 0.9343 - val_loss: 0.2367 - val_accuracy: 0.9402\n",
      "Max value:  0.9343159  at epoch 10\n",
      "Test Accuracy: 0.533859176606387\n",
      "Recall Accuracy: 0.5804387568555759\n",
      "AUC Score: 0.5509406259911408\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 18:51:23\n",
      "WindowLength: 1.00 min (900 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/adam-data/11-12-2020/11-12-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-16-2020/11-16-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-17-2020/11-17-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-24-2020/11-24-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-25-2020/11-25-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-26-2020/11-26-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-30-2020/11-30-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-01-2020/12-01-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-02-2020/12-02-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-03-2020/12-03-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-04-2020/12-04-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-07-2020/12-07-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-08-2020/12-08-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-09-2020/12-09-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-10-2020/12-10-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-11-2020/12-11-2020.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/adam-data/09-22-2020/09-22-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/10-28-2020/10-28-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-09-2020/11-09-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-11-2020/11-11-2020-1.shm\n",
      "[0 1]\n",
      "[504638  14106]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 5198 samples of length 900\n",
      "\n",
      "Train on 22569 samples, validate on 5643 samples\n",
      "Epoch 1/10\n",
      "22569/22569 [==============================] - 8s 352us/sample - loss: 1.4910 - accuracy: 0.7781 - val_loss: 0.7937 - val_accuracy: 0.8907\n",
      "Epoch 2/10\n",
      "22569/22569 [==============================] - 8s 343us/sample - loss: 0.5902 - accuracy: 0.9009 - val_loss: 0.4685 - val_accuracy: 0.8981\n",
      "Epoch 3/10\n",
      "22569/22569 [==============================] - 8s 343us/sample - loss: 0.4223 - accuracy: 0.9055 - val_loss: 0.3802 - val_accuracy: 0.9036\n",
      "Epoch 4/10\n",
      "22569/22569 [==============================] - 8s 342us/sample - loss: 0.3535 - accuracy: 0.9138 - val_loss: 0.3303 - val_accuracy: 0.9080\n",
      "Epoch 5/10\n",
      "22569/22569 [==============================] - 8s 346us/sample - loss: 0.3077 - accuracy: 0.9217 - val_loss: 0.3228 - val_accuracy: 0.9224\n",
      "Epoch 6/10\n",
      "22569/22569 [==============================] - 8s 344us/sample - loss: 0.2800 - accuracy: 0.9282 - val_loss: 0.2770 - val_accuracy: 0.9302\n",
      "Epoch 7/10\n",
      "22569/22569 [==============================] - 8s 344us/sample - loss: 0.2548 - accuracy: 0.9362 - val_loss: 0.2499 - val_accuracy: 0.9307\n",
      "Epoch 8/10\n",
      "22569/22569 [==============================] - 8s 347us/sample - loss: 0.2427 - accuracy: 0.9381 - val_loss: 0.2307 - val_accuracy: 0.9401\n",
      "Epoch 9/10\n",
      "22569/22569 [==============================] - 8s 343us/sample - loss: 0.2275 - accuracy: 0.9436 - val_loss: 0.2263 - val_accuracy: 0.9438\n",
      "Epoch 10/10\n",
      "22569/22569 [==============================] - 8s 349us/sample - loss: 0.2161 - accuracy: 0.9467 - val_loss: 0.2347 - val_accuracy: 0.9415\n",
      "Max value:  0.9467411  at epoch 10\n",
      "Test Accuracy: 0.5257791458253175\n",
      "Recall Accuracy: 0.5431701030927835\n",
      "AUC Score: 0.5307732029451849\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 18:53:36\n",
      "WindowLength: 2.00 min (1800 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/adam-data/11-12-2020/11-12-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-16-2020/11-16-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-17-2020/11-17-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-24-2020/11-24-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-25-2020/11-25-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-26-2020/11-26-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-30-2020/11-30-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-01-2020/12-01-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-02-2020/12-02-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-03-2020/12-03-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-04-2020/12-04-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-07-2020/12-07-2020.shm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading File:  ../data/IndividualData/adam-data/12-08-2020/12-08-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-09-2020/12-09-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-10-2020/12-10-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-11-2020/12-11-2020.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/adam-data/09-22-2020/09-22-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/10-28-2020/10-28-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-09-2020/11-09-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-11-2020/11-11-2020-1.shm\n",
      "[0 1]\n",
      "[503708  14076]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 5198 samples of length 1800\n",
      "\n",
      "Train on 22521 samples, validate on 5631 samples\n",
      "Epoch 1/10\n",
      "22521/22521 [==============================] - 16s 696us/sample - loss: 1.5298 - accuracy: 0.7682 - val_loss: 0.7588 - val_accuracy: 0.9243\n",
      "Epoch 2/10\n",
      "22521/22521 [==============================] - 15s 680us/sample - loss: 0.5475 - accuracy: 0.9288 - val_loss: 0.4117 - val_accuracy: 0.9377\n",
      "Epoch 3/10\n",
      "22521/22521 [==============================] - 15s 678us/sample - loss: 0.3514 - accuracy: 0.9394 - val_loss: 0.3094 - val_accuracy: 0.9437\n",
      "Epoch 4/10\n",
      "22521/22521 [==============================] - 15s 676us/sample - loss: 0.2795 - accuracy: 0.9489 - val_loss: 0.2702 - val_accuracy: 0.9542\n",
      "Epoch 5/10\n",
      "22521/22521 [==============================] - 15s 671us/sample - loss: 0.2412 - accuracy: 0.9600 - val_loss: 0.2452 - val_accuracy: 0.9551\n",
      "Epoch 6/10\n",
      "22521/22521 [==============================] - 15s 669us/sample - loss: 0.2174 - accuracy: 0.9647 - val_loss: 0.2152 - val_accuracy: 0.9639\n",
      "Epoch 7/10\n",
      "22521/22521 [==============================] - 15s 683us/sample - loss: 0.2004 - accuracy: 0.9680 - val_loss: 0.1867 - val_accuracy: 0.9732\n",
      "Epoch 8/10\n",
      "22521/22521 [==============================] - 15s 678us/sample - loss: 0.1816 - accuracy: 0.9721 - val_loss: 0.1776 - val_accuracy: 0.9712\n",
      "Epoch 9/10\n",
      "22521/22521 [==============================] - 15s 668us/sample - loss: 0.1761 - accuracy: 0.9718 - val_loss: 0.1734 - val_accuracy: 0.9727\n",
      "Epoch 10/10\n",
      "22521/22521 [==============================] - 15s 677us/sample - loss: 0.1638 - accuracy: 0.9750 - val_loss: 0.1557 - val_accuracy: 0.9776\n",
      "Max value:  0.9750455  at epoch 10\n",
      "Test Accuracy: 0.5448249326664102\n",
      "Recall Accuracy: 0.6050495942290351\n",
      "AUC Score: 0.5667703339205826\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 18:57:05\n",
      "WindowLength: 3.00 min (2700 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/adam-data/11-12-2020/11-12-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-16-2020/11-16-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-17-2020/11-17-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-24-2020/11-24-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-25-2020/11-25-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-26-2020/11-26-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-30-2020/11-30-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-01-2020/12-01-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-02-2020/12-02-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-03-2020/12-03-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-04-2020/12-04-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-07-2020/12-07-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-08-2020/12-08-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-09-2020/12-09-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-10-2020/12-10-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-11-2020/12-11-2020.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/adam-data/09-22-2020/09-22-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/10-28-2020/10-28-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-09-2020/11-09-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-11-2020/11-11-2020-1.shm\n",
      "[0 1]\n",
      "[502778  14046]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 5198 samples of length 2700\n",
      "\n",
      "Train on 22473 samples, validate on 5619 samples\n",
      "Epoch 1/10\n",
      "22473/22473 [==============================] - 23s 1ms/sample - loss: 1.5354 - accuracy: 0.7987 - val_loss: 0.8265 - val_accuracy: 0.9203\n",
      "Epoch 2/10\n",
      "22473/22473 [==============================] - 23s 1ms/sample - loss: 0.6318 - accuracy: 0.9304 - val_loss: 0.5036 - val_accuracy: 0.9365\n",
      "Epoch 3/10\n",
      "22473/22473 [==============================] - 23s 1ms/sample - loss: 0.4547 - accuracy: 0.9407 - val_loss: 0.3864 - val_accuracy: 0.9528\n",
      "Epoch 4/10\n",
      "22473/22473 [==============================] - 23s 1ms/sample - loss: 0.3622 - accuracy: 0.9536 - val_loss: 0.3249 - val_accuracy: 0.9610\n",
      "Epoch 5/10\n",
      "22473/22473 [==============================] - 23s 1ms/sample - loss: 0.3152 - accuracy: 0.9580 - val_loss: 0.2893 - val_accuracy: 0.9649\n",
      "Epoch 6/10\n",
      "22473/22473 [==============================] - 23s 1ms/sample - loss: 0.2846 - accuracy: 0.9608 - val_loss: 0.2706 - val_accuracy: 0.9578\n",
      "Epoch 7/10\n",
      "22473/22473 [==============================] - 23s 1ms/sample - loss: 0.2667 - accuracy: 0.9611 - val_loss: 0.2442 - val_accuracy: 0.9644\n",
      "Epoch 8/10\n",
      "22473/22473 [==============================] - 23s 1ms/sample - loss: 0.2430 - accuracy: 0.9641 - val_loss: 0.2322 - val_accuracy: 0.9653\n",
      "Epoch 9/10\n",
      "22473/22473 [==============================] - 22s 998us/sample - loss: 0.2279 - accuracy: 0.9665 - val_loss: 0.2177 - val_accuracy: 0.9637\n",
      "Epoch 10/10\n",
      "22473/22473 [==============================] - 23s 1ms/sample - loss: 0.2097 - accuracy: 0.9696 - val_loss: 0.2068 - val_accuracy: 0.9705\n",
      "Max value:  0.9695635  at epoch 10\n",
      "Test Accuracy: 0.5548287803001154\n",
      "Recall Accuracy: 0.6533907427341227\n",
      "AUC Score: 0.5933854627233508\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 19:01:50\n",
      "WindowLength: 4.00 min (3600 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/adam-data/11-12-2020/11-12-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-16-2020/11-16-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-17-2020/11-17-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-24-2020/11-24-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-25-2020/11-25-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-26-2020/11-26-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-30-2020/11-30-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-01-2020/12-01-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-02-2020/12-02-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-03-2020/12-03-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-04-2020/12-04-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-07-2020/12-07-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-08-2020/12-08-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-09-2020/12-09-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-10-2020/12-10-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/12-11-2020/12-11-2020.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/adam-data/09-22-2020/09-22-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/10-28-2020/10-28-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-09-2020/11-09-2020.shm\n",
      "Loading File:  ../data/IndividualData/adam-data/11-11-2020/11-11-2020-1.shm\n",
      "[0 1]\n",
      "[502184  13680]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 5198 samples of length 3600\n",
      "\n",
      "Train on 21888 samples, validate on 5472 samples\n",
      "Epoch 1/10\n",
      "21888/21888 [==============================] - 31s 1ms/sample - loss: 1.5312 - accuracy: 0.7431 - val_loss: 0.7450 - val_accuracy: 0.9338\n",
      "Epoch 2/10\n",
      "21888/21888 [==============================] - 31s 1ms/sample - loss: 0.5393 - accuracy: 0.9419 - val_loss: 0.4450 - val_accuracy: 0.9390\n",
      "Epoch 3/10\n",
      "21888/21888 [==============================] - 31s 1ms/sample - loss: 0.3760 - accuracy: 0.9508 - val_loss: 0.3220 - val_accuracy: 0.9591\n",
      "Epoch 4/10\n",
      "21888/21888 [==============================] - 31s 1ms/sample - loss: 0.3112 - accuracy: 0.9569 - val_loss: 0.2884 - val_accuracy: 0.9607\n",
      "Epoch 5/10\n",
      "21888/21888 [==============================] - 31s 1ms/sample - loss: 0.2845 - accuracy: 0.9592 - val_loss: 0.2588 - val_accuracy: 0.9680\n",
      "Epoch 6/10\n",
      "21888/21888 [==============================] - 31s 1ms/sample - loss: 0.2606 - accuracy: 0.9624 - val_loss: 0.2562 - val_accuracy: 0.9572\n",
      "Epoch 7/10\n",
      "21888/21888 [==============================] - 31s 1ms/sample - loss: 0.2438 - accuracy: 0.9629 - val_loss: 0.2345 - val_accuracy: 0.9655\n",
      "Epoch 8/10\n",
      "21888/21888 [==============================] - 31s 1ms/sample - loss: 0.2298 - accuracy: 0.9650 - val_loss: 0.3101 - val_accuracy: 0.9264\n",
      "Epoch 9/10\n",
      "21888/21888 [==============================] - 31s 1ms/sample - loss: 0.2198 - accuracy: 0.9653 - val_loss: 0.2015 - val_accuracy: 0.9691\n",
      "Epoch 10/10\n",
      "21888/21888 [==============================] - 31s 1ms/sample - loss: 0.1999 - accuracy: 0.9716 - val_loss: 0.2126 - val_accuracy: 0.9647\n",
      "Max value:  0.9715826  at epoch 10\n",
      "Test Accuracy: 0.5292420161600616\n",
      "Recall Accuracy: 0.5536723163841808\n",
      "AUC Score: 0.53688375205777\n"
     ]
    }
   ],
   "source": [
    "perf_df1, model_ls1, hist_ls1 = test_windowsize(win_ls = [3/6, 1, 2, 3,4],name = \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>win(sec)</th>\n",
       "      <th>acc</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.533859</td>\n",
       "      <td>0.580439</td>\n",
       "      <td>0.550941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.525779</td>\n",
       "      <td>0.543170</td>\n",
       "      <td>0.530773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.544825</td>\n",
       "      <td>0.605050</td>\n",
       "      <td>0.566770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.554829</td>\n",
       "      <td>0.653391</td>\n",
       "      <td>0.593385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.529242</td>\n",
       "      <td>0.553672</td>\n",
       "      <td>0.536884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  win(sec)       acc    recall       auc\n",
       "0  ActiModel      30.0  0.533859  0.580439  0.550941\n",
       "1  ActiModel      60.0  0.525779  0.543170  0.530773\n",
       "2  ActiModel     120.0  0.544825  0.605050  0.566770\n",
       "3  ActiModel     180.0  0.554829  0.653391  0.593385\n",
       "4  ActiModel     240.0  0.529242  0.553672  0.536884"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 19:14:08\n",
      "WindowLength: 0.50 min (450 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.19/10.19.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/afternoon_2hr33min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/evening_2hr_20min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.21/10.21.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.26/10.26.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.28/10.28.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/evening-2hr_goodDinnerTemplate_CFAmeal/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.4/10.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.6/10.6.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.8/10.8.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.11/11.11.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.14/11.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.15/11.15.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.2/11.2.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.3/11.3.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.4/11.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.23/9.23_13hr.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.25/9.25_1-46.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.10/10.10.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.12/10.12.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.14/10.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.16/10.16.shm\n",
      "[0 1]\n",
      "[444691  22853]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 9020 samples of length 450\n",
      "\n",
      "Train on 36564 samples, validate on 9142 samples\n",
      "Epoch 1/10\n",
      "36564/36564 [==============================] - 7s 189us/sample - loss: 1.3782 - accuracy: 0.6442 - val_loss: 0.8056 - val_accuracy: 0.6765\n",
      "Epoch 2/10\n",
      "36564/36564 [==============================] - 6s 177us/sample - loss: 0.7055 - accuracy: 0.6785 - val_loss: 0.6336 - val_accuracy: 0.6876\n",
      "Epoch 3/10\n",
      "36564/36564 [==============================] - 6s 177us/sample - loss: 0.6313 - accuracy: 0.6890 - val_loss: 0.6098 - val_accuracy: 0.6947\n",
      "Epoch 4/10\n",
      "36564/36564 [==============================] - 6s 177us/sample - loss: 0.6173 - accuracy: 0.6937 - val_loss: 0.5972 - val_accuracy: 0.7097\n",
      "Epoch 5/10\n",
      "36564/36564 [==============================] - 7s 178us/sample - loss: 0.6070 - accuracy: 0.7017 - val_loss: 0.6071 - val_accuracy: 0.7011\n",
      "Epoch 6/10\n",
      "36564/36564 [==============================] - 7s 180us/sample - loss: 0.5995 - accuracy: 0.7050 - val_loss: 0.5884 - val_accuracy: 0.7165\n",
      "Epoch 7/10\n",
      "36564/36564 [==============================] - 6s 176us/sample - loss: 0.5926 - accuracy: 0.7109 - val_loss: 0.5785 - val_accuracy: 0.7247\n",
      "Epoch 8/10\n",
      "36564/36564 [==============================] - 7s 179us/sample - loss: 0.5902 - accuracy: 0.7140 - val_loss: 0.5880 - val_accuracy: 0.7108\n",
      "Epoch 9/10\n",
      "36564/36564 [==============================] - 7s 178us/sample - loss: 0.5772 - accuracy: 0.7249 - val_loss: 0.5657 - val_accuracy: 0.7274\n",
      "Epoch 10/10\n",
      "36564/36564 [==============================] - 6s 178us/sample - loss: 0.5728 - accuracy: 0.7266 - val_loss: 0.5608 - val_accuracy: 0.7297\n",
      "Max value:  0.7266437  at epoch 10\n",
      "Test Accuracy: 0.6077605321507761\n",
      "Recall Accuracy: 0.614460668864814\n",
      "AUC Score: 0.6081310466234418\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 19:16:31\n",
      "WindowLength: 1.00 min (900 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.19/10.19.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/afternoon_2hr33min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/evening_2hr_20min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.21/10.21.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.26/10.26.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.28/10.28.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/evening-2hr_goodDinnerTemplate_CFAmeal/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.4/10.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.6/10.6.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.8/10.8.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.11/11.11.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.14/11.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.15/11.15.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.2/11.2.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.3/11.3.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.4/11.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.23/9.23_13hr.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.25/9.25_1-46.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.10/10.10.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.12/10.12.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.14/10.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.16/10.16.shm\n",
      "[0 1]\n",
      "[444143  22831]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 8940 samples of length 900\n",
      "\n",
      "Train on 36529 samples, validate on 9133 samples\n",
      "Epoch 1/10\n",
      "36529/36529 [==============================] - 13s 353us/sample - loss: 1.3422 - accuracy: 0.6639 - val_loss: 0.7893 - val_accuracy: 0.7086\n",
      "Epoch 2/10\n",
      "36529/36529 [==============================] - 13s 347us/sample - loss: 0.7018 - accuracy: 0.7020 - val_loss: 0.6500 - val_accuracy: 0.7063\n",
      "Epoch 3/10\n",
      "36529/36529 [==============================] - 13s 347us/sample - loss: 0.6268 - accuracy: 0.7135 - val_loss: 0.6193 - val_accuracy: 0.7067\n",
      "Epoch 4/10\n",
      "36529/36529 [==============================] - 13s 348us/sample - loss: 0.6090 - accuracy: 0.7178 - val_loss: 0.5919 - val_accuracy: 0.7270\n",
      "Epoch 5/10\n",
      "36529/36529 [==============================] - 13s 344us/sample - loss: 0.5881 - accuracy: 0.7305 - val_loss: 0.5825 - val_accuracy: 0.7282\n",
      "Epoch 6/10\n",
      "36529/36529 [==============================] - 13s 346us/sample - loss: 0.5726 - accuracy: 0.7415 - val_loss: 0.5512 - val_accuracy: 0.7536\n",
      "Epoch 7/10\n",
      "36529/36529 [==============================] - 13s 346us/sample - loss: 0.5502 - accuracy: 0.7565 - val_loss: 0.5283 - val_accuracy: 0.7665\n",
      "Epoch 8/10\n",
      "36529/36529 [==============================] - 13s 345us/sample - loss: 0.5319 - accuracy: 0.7682 - val_loss: 0.5071 - val_accuracy: 0.7830\n",
      "Epoch 9/10\n",
      "36529/36529 [==============================] - 13s 347us/sample - loss: 0.5074 - accuracy: 0.7837 - val_loss: 0.4913 - val_accuracy: 0.7905\n",
      "Epoch 10/10\n",
      "36529/36529 [==============================] - 13s 353us/sample - loss: 0.4932 - accuracy: 0.7935 - val_loss: 0.4811 - val_accuracy: 0.7981\n",
      "Max value:  0.7935065  at epoch 10\n",
      "Test Accuracy: 0.6682326621923937\n",
      "Recall Accuracy: 0.7732558139534884\n",
      "AUC Score: 0.6973906736218638\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 19:19:58\n",
      "WindowLength: 2.00 min (1800 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.19/10.19.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/afternoon_2hr33min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/evening_2hr_20min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.21/10.21.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.26/10.26.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.28/10.28.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/evening-2hr_goodDinnerTemplate_CFAmeal/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.4/10.4.shm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading File:  ../data/IndividualData/lawler-data/10.6/10.6.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.8/10.8.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.11/11.11.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.14/11.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.15/11.15.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.2/11.2.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.3/11.3.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.4/11.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.23/9.23_13hr.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.25/9.25_1-46.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.10/10.10.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.12/10.12.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.14/10.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.16/10.16.shm\n",
      "[0 1]\n",
      "[443100  22734]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 8728 samples of length 1800\n",
      "\n",
      "Train on 36374 samples, validate on 9094 samples\n",
      "Epoch 1/10\n",
      "36374/36374 [==============================] - 25s 693us/sample - loss: 1.3352 - accuracy: 0.6891 - val_loss: 0.7889 - val_accuracy: 0.7535\n",
      "Epoch 2/10\n",
      "36374/36374 [==============================] - 25s 681us/sample - loss: 0.7037 - accuracy: 0.7398 - val_loss: 0.6288 - val_accuracy: 0.7612\n",
      "Epoch 3/10\n",
      "36374/36374 [==============================] - 25s 682us/sample - loss: 0.6119 - accuracy: 0.7589 - val_loss: 0.6129 - val_accuracy: 0.7421\n",
      "Epoch 4/10\n",
      "36374/36374 [==============================] - 25s 678us/sample - loss: 0.5760 - accuracy: 0.7686 - val_loss: 0.6181 - val_accuracy: 0.7288\n",
      "Epoch 5/10\n",
      "36374/36374 [==============================] - 25s 688us/sample - loss: 0.5525 - accuracy: 0.7850 - val_loss: 0.5552 - val_accuracy: 0.7673\n",
      "Epoch 6/10\n",
      "36374/36374 [==============================] - 25s 682us/sample - loss: 0.5174 - accuracy: 0.8049 - val_loss: 0.4892 - val_accuracy: 0.8191\n",
      "Epoch 7/10\n",
      "36374/36374 [==============================] - 25s 687us/sample - loss: 0.4969 - accuracy: 0.8181 - val_loss: 0.4876 - val_accuracy: 0.8127\n",
      "Epoch 8/10\n",
      "36374/36374 [==============================] - 25s 690us/sample - loss: 0.4735 - accuracy: 0.8321 - val_loss: 0.4683 - val_accuracy: 0.8407\n",
      "Epoch 9/10\n",
      "36374/36374 [==============================] - 25s 682us/sample - loss: 0.4607 - accuracy: 0.8379 - val_loss: 0.5327 - val_accuracy: 0.7830\n",
      "Epoch 10/10\n",
      "36374/36374 [==============================] - 25s 681us/sample - loss: 0.4580 - accuracy: 0.8362 - val_loss: 0.4964 - val_accuracy: 0.8260\n",
      "Max value:  0.83793366  at epoch 9\n",
      "Test Accuracy: 0.634967919340055\n",
      "Recall Accuracy: 0.6326576576576577\n",
      "AUC Score: 0.6350088661422617\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 19:25:29\n",
      "WindowLength: 3.00 min (2700 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.19/10.19.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/afternoon_2hr33min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/evening_2hr_20min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.21/10.21.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.26/10.26.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.28/10.28.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/evening-2hr_goodDinnerTemplate_CFAmeal/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.4/10.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.6/10.6.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.8/10.8.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.11/11.11.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.14/11.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.15/11.15.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.2/11.2.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.3/11.3.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.4/11.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.23/9.23_13hr.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.25/9.25_1-46.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.10/10.10.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.12/10.12.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.14/10.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.16/10.16.shm\n",
      "[0 1]\n",
      "[442020  22674]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 8488 samples of length 2700\n",
      "\n",
      "Train on 36278 samples, validate on 9070 samples\n",
      "Epoch 1/10\n",
      "36278/36278 [==============================] - 37s 1ms/sample - loss: 1.3448 - accuracy: 0.7090 - val_loss: 0.7913 - val_accuracy: 0.7504\n",
      "Epoch 2/10\n",
      "36278/36278 [==============================] - 37s 1ms/sample - loss: 0.6535 - accuracy: 0.7758 - val_loss: 0.5829 - val_accuracy: 0.7824\n",
      "Epoch 3/10\n",
      "36278/36278 [==============================] - 37s 1ms/sample - loss: 0.5369 - accuracy: 0.7986 - val_loss: 0.5051 - val_accuracy: 0.8078\n",
      "Epoch 4/10\n",
      "36278/36278 [==============================] - 37s 1ms/sample - loss: 0.4848 - accuracy: 0.8255 - val_loss: 0.4624 - val_accuracy: 0.8405\n",
      "Epoch 5/10\n",
      "36278/36278 [==============================] - 37s 1ms/sample - loss: 0.4408 - accuracy: 0.8488 - val_loss: 0.4207 - val_accuracy: 0.8571\n",
      "Epoch 6/10\n",
      "36278/36278 [==============================] - 37s 1ms/sample - loss: 0.4062 - accuracy: 0.8668 - val_loss: 0.3964 - val_accuracy: 0.8702\n",
      "Epoch 7/10\n",
      "36278/36278 [==============================] - 37s 1ms/sample - loss: 0.3955 - accuracy: 0.8729 - val_loss: 0.3720 - val_accuracy: 0.8904\n",
      "Epoch 8/10\n",
      "36278/36278 [==============================] - 37s 1ms/sample - loss: 0.3760 - accuracy: 0.8841 - val_loss: 0.3542 - val_accuracy: 0.8992\n",
      "Epoch 9/10\n",
      "36278/36278 [==============================] - 40s 1ms/sample - loss: 0.3660 - accuracy: 0.8887 - val_loss: 0.3433 - val_accuracy: 0.9035\n",
      "Epoch 10/10\n",
      "36278/36278 [==============================] - 50s 1ms/sample - loss: 0.3477 - accuracy: 0.8995 - val_loss: 0.3329 - val_accuracy: 0.9079\n",
      "Max value:  0.8995259  at epoch 10\n",
      "Test Accuracy: 0.6649387370405277\n",
      "Recall Accuracy: 0.8289473684210527\n",
      "AUC Score: 0.7195051307514068\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 19:33:17\n",
      "WindowLength: 4.00 min (3600 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.19/10.19.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/afternoon_2hr33min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.20/evening_2hr_20min/10.20.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.21/10.21.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.26/10.26.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.28/10.28.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.31/evening-2hr_goodDinnerTemplate_CFAmeal/10.31.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.4/10.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.6/10.6.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.8/10.8.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.11/11.11.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.14/11.14.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.15/11.15.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.2/11.2.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.3/11.3.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/11.4/11.4.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.23/9.23_13hr.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/9.25/9.25_1-46.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.10/10.10.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.12/10.12.shm\n",
      "Loading File:  ../data/IndividualData/lawler-data/10.14/10.14.shm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading File:  ../data/IndividualData/lawler-data/10.16/10.16.shm\n",
      "[0 1]\n",
      "[440940  22614]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 8284 samples of length 3600\n",
      "\n",
      "Train on 36182 samples, validate on 9046 samples\n",
      "Epoch 1/10\n",
      "36182/36182 [==============================] - 52s 1ms/sample - loss: 1.3664 - accuracy: 0.6912 - val_loss: 0.7977 - val_accuracy: 0.7599\n",
      "Epoch 2/10\n",
      "36182/36182 [==============================] - 52s 1ms/sample - loss: 0.6881 - accuracy: 0.7692 - val_loss: 0.6500 - val_accuracy: 0.7295\n",
      "Epoch 3/10\n",
      "36182/36182 [==============================] - 52s 1ms/sample - loss: 0.5686 - accuracy: 0.8032 - val_loss: 0.5103 - val_accuracy: 0.8405\n",
      "Epoch 4/10\n",
      "36182/36182 [==============================] - 52s 1ms/sample - loss: 0.4967 - accuracy: 0.8373 - val_loss: 0.4505 - val_accuracy: 0.8646\n",
      "Epoch 5/10\n",
      "36182/36182 [==============================] - 52s 1ms/sample - loss: 0.4551 - accuracy: 0.8551 - val_loss: 0.4371 - val_accuracy: 0.8585\n",
      "Epoch 6/10\n",
      "36182/36182 [==============================] - 52s 1ms/sample - loss: 0.4284 - accuracy: 0.8657 - val_loss: 0.3895 - val_accuracy: 0.8906\n",
      "Epoch 7/10\n",
      "36182/36182 [==============================] - 52s 1ms/sample - loss: 0.3982 - accuracy: 0.8809 - val_loss: 0.3822 - val_accuracy: 0.8859\n",
      "Epoch 8/10\n",
      "36182/36182 [==============================] - 52s 1ms/sample - loss: 0.3869 - accuracy: 0.8850 - val_loss: 0.3829 - val_accuracy: 0.8811\n",
      "Epoch 9/10\n",
      "36182/36182 [==============================] - 51s 1ms/sample - loss: 0.3732 - accuracy: 0.8909 - val_loss: 0.3613 - val_accuracy: 0.8876\n",
      "Epoch 10/10\n",
      "36182/36182 [==============================] - 52s 1ms/sample - loss: 0.3500 - accuracy: 0.9025 - val_loss: 0.3361 - val_accuracy: 0.9108\n",
      "Max value:  0.90254825  at epoch 10\n",
      "Test Accuracy: 0.6877112506035732\n",
      "Recall Accuracy: 0.7760028399006035\n",
      "AUC Score: 0.7091098889460947\n"
     ]
    }
   ],
   "source": [
    "perf_df2, model_ls2, hist_ls2 = test_windowsize(win_ls = [3/6, 1, 2, 3,4],name = \"lawler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>win(sec)</th>\n",
       "      <th>acc</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.607761</td>\n",
       "      <td>0.614461</td>\n",
       "      <td>0.608131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.668233</td>\n",
       "      <td>0.773256</td>\n",
       "      <td>0.697391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.634968</td>\n",
       "      <td>0.632658</td>\n",
       "      <td>0.635009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.664939</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.719505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.687711</td>\n",
       "      <td>0.776003</td>\n",
       "      <td>0.709110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  win(sec)       acc    recall       auc\n",
       "0  ActiModel      30.0  0.607761  0.614461  0.608131\n",
       "1  ActiModel      60.0  0.668233  0.773256  0.697391\n",
       "2  ActiModel     120.0  0.634968  0.632658  0.635009\n",
       "3  ActiModel     180.0  0.664939  0.828947  0.719505\n",
       "4  ActiModel     240.0  0.687711  0.776003  0.709110"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 19:43:22\n",
      "WindowLength: 0.50 min (450 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/shaurya-data/10.25.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.1.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.15.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.17.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.18.2020/Data.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/shaurya-data/10.22.2020/Data.shm\n",
      "[0 1]\n",
      "[186222  11811]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 4740 samples of length 450\n",
      "\n",
      "Train on 18897 samples, validate on 4725 samples\n",
      "Epoch 1/10\n",
      "18897/18897 [==============================] - 4s 199us/sample - loss: 1.7276 - accuracy: 0.6456 - val_loss: 1.1460 - val_accuracy: 0.6849\n",
      "Epoch 2/10\n",
      "18897/18897 [==============================] - 3s 180us/sample - loss: 0.8984 - accuracy: 0.7063 - val_loss: 0.7546 - val_accuracy: 0.7096\n",
      "Epoch 3/10\n",
      "18897/18897 [==============================] - 3s 179us/sample - loss: 0.6919 - accuracy: 0.7233 - val_loss: 0.6402 - val_accuracy: 0.7223\n",
      "Epoch 4/10\n",
      "18897/18897 [==============================] - 3s 180us/sample - loss: 0.6152 - accuracy: 0.7327 - val_loss: 0.6048 - val_accuracy: 0.7331\n",
      "Epoch 5/10\n",
      "18897/18897 [==============================] - 3s 178us/sample - loss: 0.5899 - accuracy: 0.7404 - val_loss: 0.5874 - val_accuracy: 0.7403\n",
      "Epoch 6/10\n",
      "18897/18897 [==============================] - 3s 180us/sample - loss: 0.5851 - accuracy: 0.7427 - val_loss: 0.5842 - val_accuracy: 0.7441\n",
      "Epoch 7/10\n",
      "18897/18897 [==============================] - 3s 179us/sample - loss: 0.5710 - accuracy: 0.7483 - val_loss: 0.5782 - val_accuracy: 0.7471\n",
      "Epoch 8/10\n",
      "18897/18897 [==============================] - 3s 174us/sample - loss: 0.5663 - accuracy: 0.7511 - val_loss: 0.5667 - val_accuracy: 0.7452\n",
      "Epoch 9/10\n",
      "18897/18897 [==============================] - 3s 179us/sample - loss: 0.5605 - accuracy: 0.7536 - val_loss: 0.5633 - val_accuracy: 0.7479\n",
      "Epoch 10/10\n",
      "18897/18897 [==============================] - 3s 179us/sample - loss: 0.5552 - accuracy: 0.7581 - val_loss: 0.5487 - val_accuracy: 0.7659\n",
      "Max value:  0.75810975  at epoch 10\n",
      "Test Accuracy: 0.6603375527426161\n",
      "Recall Accuracy: 0.7224824355971897\n",
      "AUC Score: 0.6739061254503099\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 19:44:38\n",
      "WindowLength: 1.00 min (900 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/shaurya-data/10.25.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.1.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.15.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.17.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.18.2020/Data.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/shaurya-data/10.22.2020/Data.shm\n",
      "[0 1]\n",
      "[186087  11796]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 4740 samples of length 900\n",
      "\n",
      "Train on 18873 samples, validate on 4719 samples\n",
      "Epoch 1/10\n",
      "18873/18873 [==============================] - 7s 378us/sample - loss: 1.7007 - accuracy: 0.6494 - val_loss: 1.1029 - val_accuracy: 0.7271\n",
      "Epoch 2/10\n",
      "18873/18873 [==============================] - 7s 345us/sample - loss: 0.8718 - accuracy: 0.7342 - val_loss: 0.7146 - val_accuracy: 0.7521\n",
      "Epoch 3/10\n",
      "18873/18873 [==============================] - 7s 349us/sample - loss: 0.6644 - accuracy: 0.7536 - val_loss: 0.6242 - val_accuracy: 0.7565\n",
      "Epoch 4/10\n",
      "18873/18873 [==============================] - 6s 343us/sample - loss: 0.5921 - accuracy: 0.7661 - val_loss: 0.5646 - val_accuracy: 0.7758\n",
      "Epoch 5/10\n",
      "18873/18873 [==============================] - 7s 349us/sample - loss: 0.5646 - accuracy: 0.7749 - val_loss: 0.5462 - val_accuracy: 0.7834\n",
      "Epoch 6/10\n",
      "18873/18873 [==============================] - 7s 348us/sample - loss: 0.5496 - accuracy: 0.7795 - val_loss: 0.5279 - val_accuracy: 0.7936\n",
      "Epoch 7/10\n",
      "18873/18873 [==============================] - 6s 342us/sample - loss: 0.5322 - accuracy: 0.7854 - val_loss: 0.5163 - val_accuracy: 0.7936\n",
      "Epoch 8/10\n",
      "18873/18873 [==============================] - 7s 347us/sample - loss: 0.5260 - accuracy: 0.7884 - val_loss: 0.5217 - val_accuracy: 0.7889\n",
      "Epoch 9/10\n",
      "18873/18873 [==============================] - 7s 347us/sample - loss: 0.5243 - accuracy: 0.7894 - val_loss: 0.5344 - val_accuracy: 0.7773\n",
      "Epoch 10/10\n",
      "18873/18873 [==============================] - 7s 347us/sample - loss: 0.5126 - accuracy: 0.7963 - val_loss: 0.4963 - val_accuracy: 0.8042\n",
      "Max value:  0.7962698  at epoch 10\n",
      "Test Accuracy: 0.7548523206751054\n",
      "Recall Accuracy: 0.789827255278311\n",
      "AUC Score: 0.7586184469162639\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 19:46:28\n",
      "WindowLength: 2.00 min (1800 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/shaurya-data/10.25.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.1.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.15.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.17.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.18.2020/Data.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/shaurya-data/10.22.2020/Data.shm\n",
      "[0 1]\n",
      "[185817  11766]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 4740 samples of length 1800\n",
      "\n",
      "Train on 18825 samples, validate on 4707 samples\n",
      "Epoch 1/10\n",
      "18825/18825 [==============================] - 13s 700us/sample - loss: 1.7182 - accuracy: 0.6898 - val_loss: 1.0761 - val_accuracy: 0.7559\n",
      "Epoch 2/10\n",
      "18825/18825 [==============================] - 13s 674us/sample - loss: 0.8250 - accuracy: 0.7759 - val_loss: 0.6734 - val_accuracy: 0.7774\n",
      "Epoch 3/10\n",
      "18825/18825 [==============================] - 13s 681us/sample - loss: 0.6039 - accuracy: 0.7973 - val_loss: 0.5521 - val_accuracy: 0.7997\n",
      "Epoch 4/10\n",
      "18825/18825 [==============================] - 13s 678us/sample - loss: 0.5183 - accuracy: 0.8124 - val_loss: 0.4914 - val_accuracy: 0.8232\n",
      "Epoch 5/10\n",
      "18825/18825 [==============================] - 13s 680us/sample - loss: 0.4805 - accuracy: 0.8243 - val_loss: 0.4670 - val_accuracy: 0.8254\n",
      "Epoch 6/10\n",
      "18825/18825 [==============================] - 13s 683us/sample - loss: 0.4644 - accuracy: 0.8321 - val_loss: 0.4460 - val_accuracy: 0.8419\n",
      "Epoch 7/10\n",
      "18825/18825 [==============================] - 13s 681us/sample - loss: 0.4422 - accuracy: 0.8429 - val_loss: 0.4435 - val_accuracy: 0.8381\n",
      "Epoch 8/10\n",
      "18825/18825 [==============================] - 13s 684us/sample - loss: 0.4264 - accuracy: 0.8519 - val_loss: 0.4345 - val_accuracy: 0.8438\n",
      "Epoch 9/10\n",
      "18825/18825 [==============================] - 13s 682us/sample - loss: 0.4100 - accuracy: 0.8637 - val_loss: 0.4051 - val_accuracy: 0.8645\n",
      "Epoch 10/10\n",
      "18825/18825 [==============================] - 13s 676us/sample - loss: 0.3997 - accuracy: 0.8705 - val_loss: 0.3904 - val_accuracy: 0.8736\n",
      "Max value:  0.8704914  at epoch 10\n",
      "Test Accuracy: 0.8078059071729958\n",
      "Recall Accuracy: 0.8502160345655305\n",
      "AUC Score: 0.8123869032443761\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 19:49:20\n",
      "WindowLength: 3.00 min (2700 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/shaurya-data/10.25.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.1.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.15.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.17.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.18.2020/Data.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/shaurya-data/10.22.2020/Data.shm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[185547  11736]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 4740 samples of length 2700\n",
      "\n",
      "Train on 18777 samples, validate on 4695 samples\n",
      "Epoch 1/10\n",
      "18777/18777 [==============================] - 19s 1ms/sample - loss: 1.6780 - accuracy: 0.7200 - val_loss: 1.0611 - val_accuracy: 0.7527\n",
      "Epoch 2/10\n",
      "18777/18777 [==============================] - 19s 1000us/sample - loss: 0.8071 - accuracy: 0.7911 - val_loss: 0.6889 - val_accuracy: 0.7904\n",
      "Epoch 3/10\n",
      "18777/18777 [==============================] - 19s 991us/sample - loss: 0.6049 - accuracy: 0.8092 - val_loss: 0.6037 - val_accuracy: 0.7689\n",
      "Epoch 4/10\n",
      "18777/18777 [==============================] - 19s 1ms/sample - loss: 0.5125 - accuracy: 0.8265 - val_loss: 0.4701 - val_accuracy: 0.8413\n",
      "Epoch 5/10\n",
      "18777/18777 [==============================] - 19s 1ms/sample - loss: 0.4641 - accuracy: 0.8432 - val_loss: 0.4777 - val_accuracy: 0.8089\n",
      "Epoch 6/10\n",
      "18777/18777 [==============================] - 19s 1000us/sample - loss: 0.4371 - accuracy: 0.8582 - val_loss: 0.4231 - val_accuracy: 0.8722\n",
      "Epoch 7/10\n",
      "18777/18777 [==============================] - 19s 1ms/sample - loss: 0.4281 - accuracy: 0.8611 - val_loss: 0.3972 - val_accuracy: 0.8854\n",
      "Epoch 8/10\n",
      "18777/18777 [==============================] - 19s 1ms/sample - loss: 0.3958 - accuracy: 0.8787 - val_loss: 0.3845 - val_accuracy: 0.8965\n",
      "Epoch 9/10\n",
      "18777/18777 [==============================] - 19s 1ms/sample - loss: 0.3857 - accuracy: 0.8851 - val_loss: 0.3839 - val_accuracy: 0.8848\n",
      "Epoch 10/10\n",
      "18777/18777 [==============================] - 19s 1ms/sample - loss: 0.3643 - accuracy: 0.8947 - val_loss: 0.3510 - val_accuracy: 0.9108\n",
      "Max value:  0.8947116  at epoch 10\n",
      "Test Accuracy: 0.8219409282700422\n",
      "Recall Accuracy: 0.840929401251117\n",
      "AUC Score: 0.8229427182114898\n",
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 19:53:16\n",
      "WindowLength: 4.00 min (3600 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/shaurya-data/10.25.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.1.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.15.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.17.2020/Data.shm\n",
      "Loading File:  ../data/IndividualData/shaurya-data/11.18.2020/Data.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/shaurya-data/10.22.2020/Data.shm\n",
      "[0 1]\n",
      "[185277  11706]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 4740 samples of length 3600\n",
      "\n",
      "Train on 18729 samples, validate on 4683 samples\n",
      "Epoch 1/10\n",
      "18729/18729 [==============================] - 27s 1ms/sample - loss: 1.6872 - accuracy: 0.7078 - val_loss: 1.0281 - val_accuracy: 0.8112\n",
      "Epoch 2/10\n",
      "18729/18729 [==============================] - 26s 1ms/sample - loss: 0.8154 - accuracy: 0.8224 - val_loss: 0.6844 - val_accuracy: 0.8219\n",
      "Epoch 3/10\n",
      "18729/18729 [==============================] - 26s 1ms/sample - loss: 0.6094 - accuracy: 0.8400 - val_loss: 0.5481 - val_accuracy: 0.8463\n",
      "Epoch 4/10\n",
      "18729/18729 [==============================] - 26s 1ms/sample - loss: 0.5138 - accuracy: 0.8486 - val_loss: 0.5116 - val_accuracy: 0.8405\n",
      "Epoch 5/10\n",
      "18729/18729 [==============================] - 26s 1ms/sample - loss: 0.4695 - accuracy: 0.8575 - val_loss: 0.4731 - val_accuracy: 0.8428\n",
      "Epoch 6/10\n",
      "18729/18729 [==============================] - 26s 1ms/sample - loss: 0.4451 - accuracy: 0.8615 - val_loss: 0.4157 - val_accuracy: 0.8808\n",
      "Epoch 7/10\n",
      "18729/18729 [==============================] - 26s 1ms/sample - loss: 0.4114 - accuracy: 0.8730 - val_loss: 0.3880 - val_accuracy: 0.8873\n",
      "Epoch 8/10\n",
      "18729/18729 [==============================] - 26s 1ms/sample - loss: 0.3795 - accuracy: 0.8900 - val_loss: 0.3630 - val_accuracy: 0.8986\n",
      "Epoch 9/10\n",
      "18729/18729 [==============================] - 27s 1ms/sample - loss: 0.3605 - accuracy: 0.8957 - val_loss: 0.4020 - val_accuracy: 0.8627\n",
      "Epoch 10/10\n",
      "18729/18729 [==============================] - 26s 1ms/sample - loss: 0.3447 - accuracy: 0.9033 - val_loss: 0.3313 - val_accuracy: 0.9088\n",
      "Max value:  0.90330505  at epoch 10\n",
      "Test Accuracy: 0.8542194092827005\n",
      "Recall Accuracy: 0.866433871671759\n",
      "AUC Score: 0.8546134241984765\n"
     ]
    }
   ],
   "source": [
    "perf_df3, model_ls3, hist_ls3 = test_windowsize(win_ls = [3/6, 1, 2, 3,4],name = \"shaurya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>win(sec)</th>\n",
       "      <th>acc</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.660338</td>\n",
       "      <td>0.722482</td>\n",
       "      <td>0.673906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.754852</td>\n",
       "      <td>0.789827</td>\n",
       "      <td>0.758618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.807806</td>\n",
       "      <td>0.850216</td>\n",
       "      <td>0.812387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.821941</td>\n",
       "      <td>0.840929</td>\n",
       "      <td>0.822943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ActiModel</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.854219</td>\n",
       "      <td>0.866434</td>\n",
       "      <td>0.854613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  win(sec)       acc    recall       auc\n",
       "0  ActiModel      30.0  0.660338  0.722482  0.673906\n",
       "1  ActiModel      60.0  0.754852  0.789827  0.758618\n",
       "2  ActiModel     120.0  0.807806  0.850216  0.812387\n",
       "3  ActiModel     180.0  0.821941  0.840929  0.822943\n",
       "4  ActiModel     240.0  0.854219  0.866434  0.854613"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNmodel(winlength):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(10, 44, strides=2,activation='relu', input_shape=(winlength, 6)))\n",
    "    model.add(Conv1D(10, 20, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "    model.add(Conv1D(10, 4, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(300, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_windowsize_v2(win_ls = [],EPOCHS = 10,stridesec = 1,name = \"wenkanw\"):\n",
    "    \n",
    "    \n",
    "    perf = {\"model\":[],\"win(sec)\":[], \"acc\":[],\"recall\":[], \"auc\":[]}\n",
    "    model_ls = []\n",
    "    hist_ls = []\n",
    "    for winsize in win_ls:\n",
    "        winmin = winsize\n",
    "        winlength = int(winmin * 60 * 15)\n",
    "        step = int(stridesec * 15)\n",
    "        start_time = datetime.now()\n",
    "        arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "              \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "              \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "        [os.system(cmd) for cmd in arr]\n",
    "        print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "        print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "        print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "\n",
    "        # Load the dataset\n",
    "        \n",
    "        meal_data_train = Person_MealsDataset(person_name= name, file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "        meal_data_test = Person_MealsDataset(person_name= name, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "        # meal_data_train = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "        #print(\"Data loaded\", file=outfile, flush=True)\n",
    "        #print(\"Dataset ready\", file=outfile, flush=True)\n",
    "\n",
    "        unique_elements, counts_elements = np.unique(meal_data_train.labels, return_counts=True)\n",
    "        print(unique_elements)\n",
    "        print(counts_elements)\n",
    "\n",
    "        ### Set the seed for randomizing\n",
    "        random.seed(random_seed)\n",
    "\n",
    "        # load\n",
    "        print(\"Training using every file\\n\",file=outfile, flush=True)\n",
    "        trainingsamples,traininglabels =  meal_data_train.data_indices, meal_data_train.labels\n",
    "        testsamples,testlabels =  meal_data_test.data_indices, meal_data_test.labels\n",
    "\n",
    "        print(\"Creating balanced training data array\", file=outfile, flush=True)\n",
    "\n",
    "        shuffledUnderSampledBalancedIndices = balance_data_indices(traininglabels,mode=\"under\", shuffle=True)\n",
    "        balancedData,balancedLabels = meal_data_train.get_subset( shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "        shuffledUnderSampledBalancedIndices = balance_data_indices(testlabels,mode=\"under\", shuffle=True)\n",
    "        balancedData_test,balancedLabels_test = meal_data_test.get_subset( shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "        print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "        print(\"Training on {:d} samples of length {:d}\\n\".format(len(shuffledUnderSampledBalancedIndices), len(balancedData[0])), file=outfile, flush=True)\n",
    "\n",
    "        K.get_session().close()\n",
    "        K.set_session(tf.Session())\n",
    "        K.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "        mcp_save = keras.callbacks.ModelCheckpoint(modelpath, save_best_only=True, monitor='accuracy')\n",
    "        model = NNmodel(winlength)\n",
    "#         model = Sequential()\n",
    "#         model.add(Conv1D(10, , strides=2,activation='relu', input_shape=(winlength, 6)))\n",
    "#         model.add(Conv1D(10, 20, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "#         model.add(Conv1D(10, 4, strides=2, activation='relu',kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "#         model.add(GlobalAveragePooling1D())\n",
    "#         model.add(Dense(200, activation='relu'))\n",
    "#         model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        H = model.fit(x=balancedData, y = balancedLabels,\n",
    "        #                validation_data=[undersampledTestData, undersampledTestLabels],\n",
    "                    epochs = EPOCHS, batch_size=256, verbose=1, validation_split=0.2,\n",
    "                    callbacks=[mcp_save]) # removed addons.LossHistory(jsonpath) for compatibility with TensorFlow 2.2.0, needs to be re-added at some point\n",
    "\n",
    "        print(\"Max value: \", max(H.history['accuracy']), \" at epoch\", np.argmax(H.history['accuracy']) + 1)\n",
    "\n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "        predictions = model.predict(x=balancedData_test)\n",
    "        threshold = 0.5\n",
    "        acc =  accuracy_score(predictions>threshold,balancedLabels_test)\n",
    "        recall = recall_score(predictions>threshold,balancedLabels_test)\n",
    "        auc = roc_auc_score(predictions>threshold,balancedLabels_test)\n",
    "        print(\"Test Accuracy:\", acc)\n",
    "        print(\"Recall Accuracy:\", recall)\n",
    "        print(\"AUC Score:\", auc)\n",
    "\n",
    "        perf[\"model\"].append(\"ActiModel\")\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        perf[\"acc\"].append(acc)\n",
    "        perf[\"recall\"].append(recall)\n",
    "        perf[\"auc\"].append(auc)\n",
    "        model_ls.append(model)\n",
    "        hist_ls.append(H)\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    return perf_df, model_ls, hist_ls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/16/2020, 22:32:59\n",
      "WindowLength: 0.08 min (75 datum)\tSlide: 1 (15 datum)\tEpochs:10\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-14-20/10-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-3-20/10-3-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-9-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-14-20/11-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-16-20/11-16-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-18-20/11-18-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/lunch/lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Lunch/Lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-17-20/9-17-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-27-20/9-27-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-29-20/9-29-20.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-10-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-12-20/10-12-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-13-20/10-13-20.shm\n",
      "[0 1]\n",
      "[333993  20937]\n",
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n",
      "*****************************************************************\n",
      "\n",
      "Training on 13506 samples of length 75\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 20 from 16 for '{{node conv1d_90/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 2, 1], use_cudnn_on_gpu=true](conv1d_90/conv1d/ExpandDims, conv1d_90/conv1d/ExpandDims_1)' with input shapes: [?,1,16,10], [1,20,10,10].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1811\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1813\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 20 from 16 for '{{node conv1d_90/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 2, 1], use_cudnn_on_gpu=true](conv1d_90/conv1d/ExpandDims, conv1d_90/conv1d/ExpandDims_1)' with input shapes: [?,1,16,10], [1,20,10,10].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-cdda32e6e6c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperf_df4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ls4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_ls4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_windowsize_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwin_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstridesec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"wenkanw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-81-9f6818cc6887>\u001b[0m in \u001b[0;36mtest_windowsize_v2\u001b[0;34m(win_ls, EPOCHS, stridesec, name)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mmcp_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwinlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;31m#         model = Sequential()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#         model.add(Conv1D(10, , strides=2,activation='relu', input_shape=(winlength, 6)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-d86060d7364c>\u001b[0m in \u001b[0;36mNNmodel\u001b[0;34m(winlength)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m44\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwinlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m               \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    245\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m     name=None):\n\u001b[0;32m-> 1011\u001b[0;31m   return convolution_internal(\n\u001b[0m\u001b[1;32m   1012\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m       return op(\n\u001b[0m\u001b[1;32m   1142\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 574\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 574\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(value, filters, stride, padding, use_cudnn_on_gpu, data_format, name, input, dilations)\u001b[0m\n\u001b[1;32m   1879\u001b[0m     \u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m       result = gen_nn_ops.conv2d(\n\u001b[0m\u001b[1;32m   1882\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m           \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    973\u001b[0m         \"'conv2d' Op, not %r.\" % dilations)\n\u001b[1;32m    974\u001b[0m   \u001b[0mdilations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dilations\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdilations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m    976\u001b[0m         \u001b[0;34m\"Conv2D\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    740\u001b[0m       \u001b[0;31m# Add Op to graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0m\u001b[1;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m                                  attrs=attr_protos, op_def=op_def)\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3475\u001b[0m     \u001b[0;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3476\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3477\u001b[0;31m       ret = Operation(\n\u001b[0m\u001b[1;32m   3478\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3479\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1972\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mop_def\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1974\u001b[0;31m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0m\u001b[1;32m   1975\u001b[0m                                 control_input_ops, op_def)\n\u001b[1;32m   1976\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1813\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1815\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 20 from 16 for '{{node conv1d_90/conv1d}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 2, 1], use_cudnn_on_gpu=true](conv1d_90/conv1d/ExpandDims, conv1d_90/conv1d/ExpandDims_1)' with input shapes: [?,1,16,10], [1,20,10,10]."
     ]
    }
   ],
   "source": [
    "perf_df4, model_ls4, hist_ls4 = test_windowsize_v2(win_ls = [0.5/6, 1, 2],EPOCHS = 10,stridesec = 1,name = \"wenkanw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using every file\n",
      "\n",
      "Creating balanced training data array\n"
     ]
    }
   ],
   "source": [
    "# unique_elements, counts_elements = np.unique(meal_data_test.labels, return_counts=True)\n",
    "# ### Set the seed for randomizing\n",
    "# random.seed(1000)\n",
    "\n",
    "# trainIndex = np.asarray(range(354))\n",
    "\n",
    "\n",
    "# print(\"Training using every file\\n\",file=outfile, flush=True)\n",
    "# samples,labels =  meal_data_test.data_indices, meal_data_test.labels\n",
    "# # trainingsamples = samples_array\n",
    "# # traininglabels = labels_array\n",
    "\n",
    "# #undersample the training dataset\n",
    "# eatingindices = [i for i, e in enumerate(labels) if e >= 0.5]\n",
    "# noneatingindices = [i for i, e in enumerate(labels) if e < 0.5]\n",
    "# underSampledNoneatingIndices = random.sample(noneatingindices,len(eatingindices))\n",
    "# underSampledBalancedIndices = eatingindices + underSampledNoneatingIndices\n",
    "# shuffledUnderSampledBalancedIndices = underSampledBalancedIndices.copy()\n",
    "# random.shuffle(shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "# print(\"Creating balanced training data array\", file=outfile, flush=True)\n",
    "# axdata = []\n",
    "# aydata = []\n",
    "\n",
    "# for i in shuffledUnderSampledBalancedIndices:\n",
    "#     f = samples[i,0]\n",
    "#     t1 = samples[i,1]\n",
    "#     t2 = samples[i,2]\n",
    "#     sample = meal_data_test.data[f][t1:t2]\n",
    "    \n",
    "#     label = labels[i]\n",
    "#     axdata.append(sample)\n",
    "#     aydata.append(label)\n",
    "# #     print(\"Sample:\",axdata)\n",
    "# #     break\n",
    "\n",
    "# balancedData_test = np.array(axdata, copy=True) # Undersampled Balanced Training Set\n",
    "# balancedLabels_test = np.array(aydata, copy=True)\n",
    "# del axdata\n",
    "# del aydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ResourceVariable in module tensorflow.python.ops.resource_variable_ops object:\n",
      "\n",
      "class ResourceVariable(BaseResourceVariable)\n",
      " |  ResourceVariable(*args, **kwargs)\n",
      " |  \n",
      " |  Variable based on resource handles.\n",
      " |  \n",
      " |  See the [Variables How To](https://tensorflow.org/guide/variables)\n",
      " |  for a high level overview.\n",
      " |  \n",
      " |  A `ResourceVariable` allows you to maintain state across subsequent calls to\n",
      " |  session.run.\n",
      " |  \n",
      " |  The `ResourceVariable` constructor requires an initial value for the variable,\n",
      " |  which can be a `Tensor` of any type and shape. The initial value defines the\n",
      " |  type and shape of the variable. After construction, the type and shape of\n",
      " |  the variable are fixed. The value can be changed using one of the assign\n",
      " |  methods.\n",
      " |  \n",
      " |  Just like any `Tensor`, variables created with\n",
      " |  `tf.Variable(use_resource=True)` can be used as inputs for other Ops in the\n",
      " |  graph. Additionally, all the operators overloaded for the `Tensor` class are\n",
      " |  carried over to variables, so you can also add nodes to the graph by just\n",
      " |  doing arithmetic on variables.\n",
      " |  \n",
      " |  Unlike ref-based variable, a ResourceVariable has well-defined semantics. Each\n",
      " |  usage of a ResourceVariable in a TensorFlow graph adds a read_value operation\n",
      " |  to the graph. The Tensors returned by a read_value operation are guaranteed to\n",
      " |  see all modifications to the value of the variable which happen in any\n",
      " |  operation on which the read_value depends on (either directly, indirectly, or\n",
      " |  via a control dependency) and guaranteed to not see any modification to the\n",
      " |  value of the variable from operations that depend on the read_value operation.\n",
      " |  Updates from operations that have no dependency relationship to the read_value\n",
      " |  operation might or might not be visible to read_value.\n",
      " |  \n",
      " |  For example, if there is more than one assignment to a ResourceVariable in\n",
      " |  a single session.run call there is a well-defined value for each operation\n",
      " |  which uses the variable's value if the assignments and the read are connected\n",
      " |  by edges in the graph. Consider the following example, in which two writes\n",
      " |  can cause tf.Variable and tf.ResourceVariable to behave differently:\n",
      " |  \n",
      " |  ```python\n",
      " |  a = tf.Variable(1.0, use_resource=True)\n",
      " |  a.initializer.run()\n",
      " |  \n",
      " |  assign = a.assign(2.0)\n",
      " |  with tf.control_dependencies([assign]):\n",
      " |    b = a.read_value()\n",
      " |  with tf.control_dependencies([b]):\n",
      " |    other_assign = a.assign(3.0)\n",
      " |  with tf.control_dependencies([other_assign]):\n",
      " |    # Will print 2.0 because the value was read before other_assign ran. If\n",
      " |    # `a` was a tf.Variable instead, 2.0 or 3.0 could be printed.\n",
      " |    tf.compat.v1.Print(b, [b]).eval()\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ResourceVariable\n",
      " |      BaseResourceVariable\n",
      " |      tensorflow.python.ops.variables.VariableV1\n",
      " |      tensorflow.python.ops.variables.Variable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.types.core.Tensor\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, initial_value=None, trainable=None, collections=None, validate_shape=True, caching_device=None, name=None, dtype=None, variable_def=None, import_scope=None, constraint=None, distribute_strategy=None, synchronization=None, aggregation=None, shape=None)\n",
      " |      Creates a variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        initial_value: A `Tensor`, or Python object convertible to a `Tensor`,\n",
      " |          which is the initial value for the Variable. Can also be a\n",
      " |          callable with no argument that returns the initial value when called.\n",
      " |          (Note that initializer functions from init_ops.py must first be bound\n",
      " |          to a shape before being used here.)\n",
      " |        trainable: If `True`, the default, also adds the variable to the graph\n",
      " |          collection `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as\n",
      " |          the default list of variables to use by the `Optimizer` classes.\n",
      " |          Defaults to `True`, unless `synchronization` is set to `ON_READ`, in\n",
      " |          which case it defaults to `False`.\n",
      " |        collections: List of graph collections keys. The new variable is added to\n",
      " |          these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.\n",
      " |        validate_shape: Ignored. Provided for compatibility with tf.Variable.\n",
      " |        caching_device: Optional device string or function describing where the\n",
      " |          Variable should be cached for reading.  Defaults to the Variable's\n",
      " |          device.  If not `None`, caches on another device.  Typical use is to\n",
      " |          cache on the device where the Ops using the Variable reside, to\n",
      " |          deduplicate copying through `Switch` and other conditional statements.\n",
      " |        name: Optional name for the variable. Defaults to `'Variable'` and gets\n",
      " |          uniquified automatically.\n",
      " |        dtype: If set, initial_value will be converted to the given type.\n",
      " |          If None, either the datatype will be kept (if initial_value is\n",
      " |          a Tensor) or float32 will be used (if it is a Python object convertible\n",
      " |          to a Tensor).\n",
      " |        variable_def: `VariableDef` protocol buffer. If not None, recreates the\n",
      " |          `ResourceVariable` object with its contents. `variable_def` and other\n",
      " |          arguments (except for import_scope) are mutually exclusive.\n",
      " |        import_scope: Optional `string`. Name scope to add to the\n",
      " |          ResourceVariable. Only used when `variable_def` is provided.\n",
      " |        constraint: An optional projection function to be applied to the variable\n",
      " |          after being updated by an `Optimizer` (e.g. used to implement norm\n",
      " |          constraints or value constraints for layer weights). The function must\n",
      " |          take as input the unprojected Tensor representing the value of the\n",
      " |          variable and return the Tensor for the projected value\n",
      " |          (which must have the same shape). Constraints are not safe to\n",
      " |          use when doing asynchronous distributed training.\n",
      " |        distribute_strategy: The tf.distribute.Strategy this variable is being\n",
      " |          created inside of.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        shape: (optional) The shape of this variable. If None, the shape of\n",
      " |          `initial_value` will be used. When setting this argument to\n",
      " |          `tf.TensorShape(None)` (representing an unspecified shape), the variable\n",
      " |          can be assigned with values of different shapes.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the initial value is not specified, or does not have a\n",
      " |          shape and `validate_shape` is `True`.\n",
      " |      \n",
      " |      @compatibility(eager)\n",
      " |      When Eager Execution is enabled, the default for the `collections` argument\n",
      " |      is `None`, which signifies that this `Variable` will not be added to any\n",
      " |      collections.\n",
      " |      @end_compatibility\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseResourceVariable:\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __complex__(self)\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __deepcopy__(self, memo)\n",
      " |  \n",
      " |  __float__(self)\n",
      " |  \n",
      " |  __iadd__(self, unused_other)\n",
      " |  \n",
      " |  __idiv__(self, unused_other)\n",
      " |  \n",
      " |  __imul__(self, unused_other)\n",
      " |  \n",
      " |  __int__(self)\n",
      " |  \n",
      " |  __ipow__(self, unused_other)\n",
      " |  \n",
      " |  __irealdiv__(self, unused_other)\n",
      " |  \n",
      " |  __isub__(self, unused_other)\n",
      " |  \n",
      " |  __itruediv__(self, unused_other)\n",
      " |  \n",
      " |  __long__(self)\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  assign(self, value, use_locking=None, name=None, read_value=True)\n",
      " |      Assigns a new value to this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: A `Tensor`. The new value for this variable.\n",
      " |        use_locking: If `True`, use locking during the assignment.\n",
      " |        name: The name to use for the assignment.\n",
      " |        read_value: A `bool`. Whether to read and return the new value of the\n",
      " |            variable or not.\n",
      " |      \n",
      " |      Returns:\n",
      " |        If `read_value` is `True`, this method will return the new value of the\n",
      " |        variable after the assignment has completed. Otherwise, when in graph mode\n",
      " |        it will return the `Operation` that does the assignment, and when in eager\n",
      " |        mode it will return `None`.\n",
      " |  \n",
      " |  assign_add(self, delta, use_locking=None, name=None, read_value=True)\n",
      " |      Adds a value to this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to add to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: The name to use for the operation.\n",
      " |        read_value: A `bool`. Whether to read and return the new value of the\n",
      " |            variable or not.\n",
      " |      \n",
      " |      Returns:\n",
      " |        If `read_value` is `True`, this method will return the new value of the\n",
      " |        variable after the assignment has completed. Otherwise, when in graph mode\n",
      " |        it will return the `Operation` that does the assignment, and when in eager\n",
      " |        mode it will return `None`.\n",
      " |  \n",
      " |  assign_sub(self, delta, use_locking=None, name=None, read_value=True)\n",
      " |      Subtracts a value from this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to subtract from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: The name to use for the operation.\n",
      " |        read_value: A `bool`. Whether to read and return the new value of the\n",
      " |            variable or not.\n",
      " |      \n",
      " |      Returns:\n",
      " |        If `read_value` is `True`, this method will return the new value of the\n",
      " |        variable after the assignment has completed. Otherwise, when in graph mode\n",
      " |        it will return the `Operation` that does the assignment, and when in eager\n",
      " |        mode it will return `None`.\n",
      " |  \n",
      " |  batch_scatter_update(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Assigns `tf.IndexedSlices` to this variable batch-wise.\n",
      " |      \n",
      " |      Analogous to `batch_gather`. This assumes that this variable and the\n",
      " |      sparse_delta IndexedSlices have a series of leading dimensions that are the\n",
      " |      same for all of them, and the updates are performed on the last dimension of\n",
      " |      indices. In other words, the dimensions should be the following:\n",
      " |      \n",
      " |      `num_prefix_dims = sparse_delta.indices.ndims - 1`\n",
      " |      `batch_dim = num_prefix_dims + 1`\n",
      " |      `sparse_delta.updates.shape = sparse_delta.indices.shape + var.shape[\n",
      " |           batch_dim:]`\n",
      " |      \n",
      " |      where\n",
      " |      \n",
      " |      `sparse_delta.updates.shape[:num_prefix_dims]`\n",
      " |      `== sparse_delta.indices.shape[:num_prefix_dims]`\n",
      " |      `== var.shape[:num_prefix_dims]`\n",
      " |      \n",
      " |      And the operation performed can be expressed as:\n",
      " |      \n",
      " |      `var[i_1, ..., i_n,\n",
      " |           sparse_delta.indices[i_1, ..., i_n, j]] = sparse_delta.updates[\n",
      " |              i_1, ..., i_n, j]`\n",
      " |      \n",
      " |      When sparse_delta.indices is a 1D tensor, this operation is equivalent to\n",
      " |      `scatter_update`.\n",
      " |      \n",
      " |      To avoid this operation one can looping over the first `ndims` of the\n",
      " |      variable and using `scatter_update` on the subtensors that result of slicing\n",
      " |      the first dimension. This is a valid option for `ndims = 1`, but less\n",
      " |      efficient than this implementation.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to be assigned to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  count_up_to(self, limit)\n",
      " |      Increments this variable until it reaches `limit`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Prefer Dataset.range instead.\n",
      " |      \n",
      " |      When that Op is run it tries to increment the variable by `1`. If\n",
      " |      incrementing the variable would bring it above `limit` then the Op raises\n",
      " |      the exception `OutOfRangeError`.\n",
      " |      \n",
      " |      If no error is raised, the Op outputs the value of the variable before\n",
      " |      the increment.\n",
      " |      \n",
      " |      This is essentially a shortcut for `count_up_to(self, limit)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        limit: value at which incrementing the variable raises an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the variable value before the increment. If no\n",
      " |        other Op modifies this variable, the values produced will all be\n",
      " |        distinct.\n",
      " |  \n",
      " |  eval(self, session=None)\n",
      " |      Evaluates and returns the value of this variable.\n",
      " |  \n",
      " |  gather_nd(self, indices, name=None)\n",
      " |      Reads the value of this variable sparsely, using `gather_nd`.\n",
      " |  \n",
      " |  is_initialized(self, name=None)\n",
      " |      Checks whether a resource variable has been initialized.\n",
      " |      \n",
      " |      Outputs boolean scalar indicating whether the tensor has been initialized.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  numpy(self)\n",
      " |  \n",
      " |  read_value(self)\n",
      " |      Constructs an op which reads the value of this variable.\n",
      " |      \n",
      " |      Should be used when there are multiple reads, or when it is desirable to\n",
      " |      read the value only after some condition is true.\n",
      " |      \n",
      " |      Returns:\n",
      " |       the read operation.\n",
      " |  \n",
      " |  scatter_add(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Adds `tf.IndexedSlices` to this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to be added to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_div(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Divide this variable by `tf.IndexedSlices`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to divide this variable by.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_max(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Updates this variable with the max of `tf.IndexedSlices` and itself.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to use as an argument of max\n",
      " |          with this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_min(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Updates this variable with the min of `tf.IndexedSlices` and itself.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to use as an argument of min\n",
      " |          with this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_mul(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Multiply this variable by `tf.IndexedSlices`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to multiply this variable by.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_nd_add(self, indices, updates, name=None)\n",
      " |      Applies sparse addition to individual values or slices in a Variable.\n",
      " |      \n",
      " |      `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
      " |      \n",
      " |      `indices` must be integer tensor, containing indices into `ref`.\n",
      " |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
      " |      \n",
      " |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
      " |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
      " |      dimension of `ref`.\n",
      " |      \n",
      " |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
      " |      \n",
      " |      ```\n",
      " |      [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].\n",
      " |      ```\n",
      " |      \n",
      " |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
      " |      8 elements. In Python, that update would look like this:\n",
      " |      \n",
      " |      ```python\n",
      " |          ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
      " |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
      " |          updates = tf.constant([9, 10, 11, 12])\n",
      " |          add = ref.scatter_nd_add(indices, updates)\n",
      " |          with tf.compat.v1.Session() as sess:\n",
      " |            print sess.run(add)\n",
      " |      ```\n",
      " |      \n",
      " |      The resulting update to ref would look like this:\n",
      " |      \n",
      " |          [1, 13, 3, 14, 14, 6, 7, 20]\n",
      " |      \n",
      " |      See `tf.scatter_nd` for more details about how to make updates to\n",
      " |      slices.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The indices to be used in the operation.\n",
      " |        updates: The values to be used in the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |  \n",
      " |  scatter_nd_max(self, indices, updates, name=None)\n",
      " |      Updates this variable with the max of `tf.IndexedSlices` and itself.\n",
      " |      \n",
      " |      `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
      " |      \n",
      " |      `indices` must be integer tensor, containing indices into `ref`.\n",
      " |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
      " |      \n",
      " |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
      " |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
      " |      dimension of `ref`.\n",
      " |      \n",
      " |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
      " |      \n",
      " |      ```\n",
      " |      [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].\n",
      " |      ```\n",
      " |      \n",
      " |      See `tf.scatter_nd` for more details about how to make updates to\n",
      " |      slices.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The indices to be used in the operation.\n",
      " |        updates: The values to be used in the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |  \n",
      " |  scatter_nd_min(self, indices, updates, name=None)\n",
      " |      Updates this variable with the min of `tf.IndexedSlices` and itself.\n",
      " |      \n",
      " |      `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
      " |      \n",
      " |      `indices` must be integer tensor, containing indices into `ref`.\n",
      " |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
      " |      \n",
      " |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
      " |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
      " |      dimension of `ref`.\n",
      " |      \n",
      " |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
      " |      \n",
      " |      ```\n",
      " |      [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].\n",
      " |      ```\n",
      " |      \n",
      " |      See `tf.scatter_nd` for more details about how to make updates to\n",
      " |      slices.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The indices to be used in the operation.\n",
      " |        updates: The values to be used in the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |  \n",
      " |  scatter_nd_sub(self, indices, updates, name=None)\n",
      " |      Applies sparse subtraction to individual values or slices in a Variable.\n",
      " |      \n",
      " |      `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
      " |      \n",
      " |      `indices` must be integer tensor, containing indices into `ref`.\n",
      " |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
      " |      \n",
      " |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
      " |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
      " |      dimension of `ref`.\n",
      " |      \n",
      " |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
      " |      \n",
      " |      ```\n",
      " |      [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].\n",
      " |      ```\n",
      " |      \n",
      " |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
      " |      8 elements. In Python, that update would look like this:\n",
      " |      \n",
      " |      ```python\n",
      " |          ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
      " |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
      " |          updates = tf.constant([9, 10, 11, 12])\n",
      " |          op = ref.scatter_nd_sub(indices, updates)\n",
      " |          with tf.compat.v1.Session() as sess:\n",
      " |            print sess.run(op)\n",
      " |      ```\n",
      " |      \n",
      " |      The resulting update to ref would look like this:\n",
      " |      \n",
      " |          [1, -9, 3, -6, -6, 6, 7, -4]\n",
      " |      \n",
      " |      See `tf.scatter_nd` for more details about how to make updates to\n",
      " |      slices.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The indices to be used in the operation.\n",
      " |        updates: The values to be used in the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |  \n",
      " |  scatter_nd_update(self, indices, updates, name=None)\n",
      " |      Applies sparse assignment to individual values or slices in a Variable.\n",
      " |      \n",
      " |      `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
      " |      \n",
      " |      `indices` must be integer tensor, containing indices into `ref`.\n",
      " |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
      " |      \n",
      " |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
      " |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
      " |      dimension of `ref`.\n",
      " |      \n",
      " |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
      " |      \n",
      " |      ```\n",
      " |      [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].\n",
      " |      ```\n",
      " |      \n",
      " |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
      " |      8 elements. In Python, that update would look like this:\n",
      " |      \n",
      " |      ```python\n",
      " |          ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
      " |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
      " |          updates = tf.constant([9, 10, 11, 12])\n",
      " |          op = ref.scatter_nd_update(indices, updates)\n",
      " |          with tf.compat.v1.Session() as sess:\n",
      " |            print sess.run(op)\n",
      " |      ```\n",
      " |      \n",
      " |      The resulting update to ref would look like this:\n",
      " |      \n",
      " |          [1, 11, 3, 10, 9, 6, 7, 12]\n",
      " |      \n",
      " |      See `tf.scatter_nd` for more details about how to make updates to\n",
      " |      slices.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The indices to be used in the operation.\n",
      " |        updates: The values to be used in the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |  \n",
      " |  scatter_sub(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Subtracts `tf.IndexedSlices` from this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to be subtracted from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_update(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Assigns `tf.IndexedSlices` to this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to be assigned to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  set_shape(self, shape)\n",
      " |      Overrides the shape for this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        shape: the `TensorShape` representing the overridden shape.\n",
      " |  \n",
      " |  sparse_read(self, indices, name=None)\n",
      " |      Reads the value of this variable sparsely, using `gather`.\n",
      " |  \n",
      " |  to_proto(self, export_scope=None)\n",
      " |      Converts a `ResourceVariable` to a `VariableDef` protocol buffer.\n",
      " |      \n",
      " |      Args:\n",
      " |        export_scope: Optional `string`. Name scope to remove.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If run in EAGER mode.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `VariableDef` protocol buffer, or `None` if the `Variable` is not\n",
      " |        in the specified name scope.\n",
      " |  \n",
      " |  value(self)\n",
      " |      A cached operation which reads the value of this variable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from BaseResourceVariable:\n",
      " |  \n",
      " |  from_proto(variable_def, import_scope=None)\n",
      " |      Returns a `Variable` object created from `variable_def`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseResourceVariable:\n",
      " |  \n",
      " |  aggregation\n",
      " |  \n",
      " |  constraint\n",
      " |      Returns the constraint function associated with this variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The constraint function that was passed to the variable constructor.\n",
      " |        Can be `None` if no constraint was passed.\n",
      " |  \n",
      " |  create\n",
      " |      The op responsible for initializing this variable.\n",
      " |  \n",
      " |  device\n",
      " |      The device this variable is on.\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of this variable.\n",
      " |  \n",
      " |  graph\n",
      " |      The `Graph` of this variable.\n",
      " |  \n",
      " |  handle\n",
      " |      The handle by which this variable can be accessed.\n",
      " |  \n",
      " |  initial_value\n",
      " |      Returns the Tensor used as the initial value for the variable.\n",
      " |  \n",
      " |  initializer\n",
      " |      The op responsible for initializing this variable.\n",
      " |  \n",
      " |  name\n",
      " |      The name of the handle for this variable.\n",
      " |  \n",
      " |  op\n",
      " |      The op for this variable.\n",
      " |  \n",
      " |  shape\n",
      " |      The shape of this variable.\n",
      " |  \n",
      " |  synchronization\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from BaseResourceVariable:\n",
      " |  \n",
      " |  __array_priority__ = 100\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from tensorflow.python.ops.variables.VariableV1:\n",
      " |  \n",
      " |  SaveSliceInfo = <class 'tensorflow.python.ops.variables.Variable.SaveS...\n",
      " |      Information on how to save this Variable as a slice.\n",
      " |      \n",
      " |      Provides internal support for saving variables as slices of a larger\n",
      " |      variable.  This API is not public and is subject to change.\n",
      " |      \n",
      " |      Available properties:\n",
      " |      \n",
      " |      * full_name\n",
      " |      * full_shape\n",
      " |      * var_offset\n",
      " |      * var_shape\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.ops.variables.Variable:\n",
      " |  \n",
      " |  __abs__ = abs(x, name=None)\n",
      " |      Computes the absolute value of a tensor.\n",
      " |      \n",
      " |      Given a tensor of integer or floating-point values, this operation returns a\n",
      " |      tensor of the same type, where each element contains the absolute value of the\n",
      " |      corresponding element in the input.\n",
      " |      \n",
      " |      Given a tensor `x` of complex numbers, this operation returns a tensor of type\n",
      " |      `float32` or `float64` that is the absolute value of each element in `x`. For\n",
      " |      a complex number \\\\(a + bj\\\\), its absolute value is computed as \\\\(\\sqrt{a^2\n",
      " |      + b^2}\\\\).  For example:\n",
      " |      \n",
      " |      >>> x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n",
      " |      >>> tf.abs(x)\n",
      " |      <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      " |      array([[5.25594901],\n",
      " |             [6.60492241]])>\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,\n",
      " |          `int32`, `int64`, `complex64` or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` or `SparseTensor` of the same size, type and sparsity as `x`,\n",
      " |          with absolute values. Note, for `complex64` or `complex128` input, the\n",
      " |          returned `Tensor` will be of type `float32` or `float64`, respectively.\n",
      " |  \n",
      " |  __add__ = binary_op_wrapper(x, y)\n",
      " |      The operation invoked by the `Tensor.__add__` operator.\n",
      " |      \n",
      " |        Purpose in the API:\n",
      " |      \n",
      " |          This method is exposed in TensorFlow's API so that library developers\n",
      " |          can register dispatching for `Tensor.__add__` to allow it to handle\n",
      " |          custom composite tensors & other custom objects.\n",
      " |      \n",
      " |          The API symbol is not intended to be called by users directly and does\n",
      " |          appear in TensorFlow's generated documentation.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: The left-hand side of the `+` operator.\n",
      " |        y: The right-hand side of the `+` operator.\n",
      " |        name: an optional name for the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The result of the elementwise `+` operation.\n",
      " |  \n",
      " |  __and__ = binary_op_wrapper(x, y)\n",
      " |      Logical AND function.\n",
      " |      \n",
      " |      The operation works for the following input types:\n",
      " |      \n",
      " |      - Two single elements of type `bool`\n",
      " |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
      " |        be calculated by applying logical AND with the single element to each\n",
      " |        element in the larger Tensor.\n",
      " |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
      " |        the result will be the element-wise logical AND of the two input tensors.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      >>> a = tf.constant([True])\n",
      " |      >>> b = tf.constant([False])\n",
      " |      >>> tf.math.logical_and(a, b)\n",
      " |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>\n",
      " |      \n",
      " |      >>> c = tf.constant([True])\n",
      " |      >>> x = tf.constant([False, True, True, False])\n",
      " |      >>> tf.math.logical_and(c, x)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
      " |      \n",
      " |      >>> y = tf.constant([False, False, True, True])\n",
      " |      >>> z = tf.constant([False, True, False, True])\n",
      " |      >>> tf.math.logical_and(y, z)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False,  True])>\n",
      " |      \n",
      " |      Args:\n",
      " |          x: A `tf.Tensor` type bool.\n",
      " |          y: A `tf.Tensor` of type bool.\n",
      " |          name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
      " |  \n",
      " |  __div__ = binary_op_wrapper(x, y)\n",
      " |      Divides x / y elementwise (using Python 2 division operator semantics). (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Deprecated in favor of operator or tf.math.divide.\n",
      " |      \n",
      " |      NOTE: Prefer using the Tensor division operator or tf.divide which obey Python\n",
      " |      3 division operator semantics.\n",
      " |      \n",
      " |      This function divides `x` and `y`, forcing Python 2 semantics. That is, if `x`\n",
      " |      and `y` are both integers then the result will be an integer. This is in\n",
      " |      contrast to Python 3, where division with `/` is always a float while division\n",
      " |      with `//` is always an integer.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Compares two variables element-wise for equality.\n",
      " |  \n",
      " |  __floordiv__ = binary_op_wrapper(x, y)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
      " |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __ge__ = greater_equal(x, y, name=None)\n",
      " |      Returns the truth value of (x >= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6, 7])\n",
      " |      y = tf.constant([5, 2, 5, 10])\n",
      " |      tf.math.greater_equal(x, y) ==> [True, True, True, False]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6, 7])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.greater_equal(x, y) ==> [True, False, True, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __getitem__ = _SliceHelperVar(var, slice_spec)\n",
      " |      Creates a slice helper object given a variable.\n",
      " |      \n",
      " |      This allows creating a sub-tensor from part of the current contents\n",
      " |      of a variable. See `tf.Tensor.__getitem__` for detailed examples\n",
      " |      of slicing.\n",
      " |      \n",
      " |      This function in addition also allows assignment to a sliced range.\n",
      " |      This is similar to `__setitem__` functionality in Python. However,\n",
      " |      the syntax is different so that the user can capture the assignment\n",
      " |      operation for grouping or passing to `sess.run()`.\n",
      " |      For example,\n",
      " |      \n",
      " |      ```python\n",
      " |      import tensorflow as tf\n",
      " |      A = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)\n",
      " |      with tf.compat.v1.Session() as sess:\n",
      " |        sess.run(tf.compat.v1.global_variables_initializer())\n",
      " |        print(sess.run(A[:2, :2]))  # => [[1,2], [4,5]]\n",
      " |      \n",
      " |        op = A[:2,:2].assign(22. * tf.ones((2, 2)))\n",
      " |        print(sess.run(op))  # => [[22, 22, 3], [22, 22, 6], [7,8,9]]\n",
      " |      ```\n",
      " |      \n",
      " |      Note that assignments currently do not support NumPy broadcasting\n",
      " |      semantics.\n",
      " |      \n",
      " |      Args:\n",
      " |        var: An `ops.Variable` object.\n",
      " |        slice_spec: The arguments to `Tensor.__getitem__`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The appropriate slice of \"tensor\", based on \"slice_spec\".\n",
      " |        As an operator. The operator also has a `assign()` method\n",
      " |        that can be used to generate an assignment operator.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If a slice range is negative size.\n",
      " |        TypeError: TypeError: If the slice indices aren't int, slice,\n",
      " |          ellipsis, tf.newaxis or int32/int64 tensors.\n",
      " |  \n",
      " |  __gt__ = greater(x, y, name=None)\n",
      " |      Returns the truth value of (x > y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5, 2, 5])\n",
      " |      tf.math.greater(x, y) ==> [False, True, True]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.greater(x, y) ==> [False, False, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __invert__ = logical_not(x, name=None)\n",
      " |      Returns the truth value of `NOT x` element-wise.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> tf.math.logical_not(tf.constant([True, False]))\n",
      " |      <tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`. A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Dummy method to prevent iteration.\n",
      " |      \n",
      " |      Do not call.\n",
      " |      \n",
      " |      NOTE(mrry): If we register __getitem__ as an overloaded operator,\n",
      " |      Python will valiantly attempt to iterate over the variable's Tensor from 0\n",
      " |      to infinity.  Declaring this method prevents this unintended behavior.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: when invoked.\n",
      " |  \n",
      " |  __le__ = less_equal(x, y, name=None)\n",
      " |      Returns the truth value of (x <= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.less_equal(x, y) ==> [True, True, False]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5, 6, 6])\n",
      " |      tf.math.less_equal(x, y) ==> [True, True, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __lt__ = less(x, y, name=None)\n",
      " |      Returns the truth value of (x < y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.less(x, y) ==> [False, True, False]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5, 6, 7])\n",
      " |      tf.math.less(x, y) ==> [False, True, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __matmul__ = binary_op_wrapper(x, y)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication dimensions,\n",
      " |      and any further outer dimensions specify matching batch size.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      A simple 2-D tensor matrix multiplication:\n",
      " |      \n",
      " |      >>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      >>> a  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      " |      array([[1, 2, 3],\n",
      " |             [4, 5, 6]], dtype=int32)>\n",
      " |      >>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      >>> b  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      " |      array([[ 7,  8],\n",
      " |             [ 9, 10],\n",
      " |             [11, 12]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
      " |      array([[ 58,  64],\n",
      " |             [139, 154]], dtype=int32)>\n",
      " |      \n",
      " |      A batch matrix multiplication with batch shape [2]:\n",
      " |      \n",
      " |      >>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
      " |      >>> a  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
      " |      array([[[ 1,  2,  3],\n",
      " |              [ 4,  5,  6]],\n",
      " |             [[ 7,  8,  9],\n",
      " |              [10, 11, 12]]], dtype=int32)>\n",
      " |      >>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
      " |      >>> b  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
      " |      array([[[13, 14],\n",
      " |              [15, 16],\n",
      " |              [17, 18]],\n",
      " |             [[19, 20],\n",
      " |              [21, 22],\n",
      " |              [23, 24]]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
      " |      array([[[ 94, 100],\n",
      " |              [229, 244]],\n",
      " |             [[508, 532],\n",
      " |              [697, 730]]], dtype=int32)>\n",
      " |      \n",
      " |      Since python >= 3.5 the @ operator is supported\n",
      " |      (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\n",
      " |      it simply calls the `tf.matmul()` function, so the following lines are\n",
      " |      equivalent:\n",
      " |      \n",
      " |      >>> d = a @ b @ [[10], [11]]\n",
      " |      >>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n",
      " |          `complex64`, `complex128` and rank > 1.\n",
      " |        b: `tf.Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix. Notice, this\n",
      " |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
      " |          that assume most values in `a` are zero.\n",
      " |          See `tf.sparse.sparse_dense_matmul`\n",
      " |          for some support for `tf.sparse.SparseTensor` multiplication.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix. Notice, this\n",
      " |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
      " |          that assume most values in `a` are zero.\n",
      " |          See `tf.sparse.sparse_dense_matmul`\n",
      " |          for some support for `tf.sparse.SparseTensor` multiplication.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n",
      " |        is the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n",
      " |        for all indices `i`, `j`.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n",
      " |          `adjoint_b` are both set to `True`.\n",
      " |  \n",
      " |  __mod__ = binary_op_wrapper(x, y)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `uint64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __mul__ = binary_op_wrapper(x, y)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Compares two variables element-wise for equality.\n",
      " |  \n",
      " |  __neg__ = neg(x, name=None)\n",
      " |      Computes numerical negative value element-wise.\n",
      " |      \n",
      " |      I.e., \\\\(y = -x\\\\).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __or__ = binary_op_wrapper(x, y)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __pow__ = binary_op_wrapper(x, y)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __radd__ = r_binary_op_wrapper(y, x)\n",
      " |      The operation invoked by the `Tensor.__add__` operator.\n",
      " |      \n",
      " |        Purpose in the API:\n",
      " |      \n",
      " |          This method is exposed in TensorFlow's API so that library developers\n",
      " |          can register dispatching for `Tensor.__add__` to allow it to handle\n",
      " |          custom composite tensors & other custom objects.\n",
      " |      \n",
      " |          The API symbol is not intended to be called by users directly and does\n",
      " |          appear in TensorFlow's generated documentation.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: The left-hand side of the `+` operator.\n",
      " |        y: The right-hand side of the `+` operator.\n",
      " |        name: an optional name for the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The result of the elementwise `+` operation.\n",
      " |  \n",
      " |  __rand__ = r_binary_op_wrapper(y, x)\n",
      " |      Logical AND function.\n",
      " |      \n",
      " |      The operation works for the following input types:\n",
      " |      \n",
      " |      - Two single elements of type `bool`\n",
      " |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
      " |        be calculated by applying logical AND with the single element to each\n",
      " |        element in the larger Tensor.\n",
      " |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
      " |        the result will be the element-wise logical AND of the two input tensors.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      >>> a = tf.constant([True])\n",
      " |      >>> b = tf.constant([False])\n",
      " |      >>> tf.math.logical_and(a, b)\n",
      " |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>\n",
      " |      \n",
      " |      >>> c = tf.constant([True])\n",
      " |      >>> x = tf.constant([False, True, True, False])\n",
      " |      >>> tf.math.logical_and(c, x)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
      " |      \n",
      " |      >>> y = tf.constant([False, False, True, True])\n",
      " |      >>> z = tf.constant([False, True, False, True])\n",
      " |      >>> tf.math.logical_and(y, z)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False,  True])>\n",
      " |      \n",
      " |      Args:\n",
      " |          x: A `tf.Tensor` type bool.\n",
      " |          y: A `tf.Tensor` of type bool.\n",
      " |          name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
      " |  \n",
      " |  __rdiv__ = r_binary_op_wrapper(y, x)\n",
      " |      Divides x / y elementwise (using Python 2 division operator semantics). (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Deprecated in favor of operator or tf.math.divide.\n",
      " |      \n",
      " |      NOTE: Prefer using the Tensor division operator or tf.divide which obey Python\n",
      " |      3 division operator semantics.\n",
      " |      \n",
      " |      This function divides `x` and `y`, forcing Python 2 semantics. That is, if `x`\n",
      " |      and `y` are both integers then the result will be an integer. This is in\n",
      " |      contrast to Python 3, where division with `/` is always a float while division\n",
      " |      with `//` is always an integer.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __rfloordiv__ = r_binary_op_wrapper(y, x)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
      " |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __rmatmul__ = r_binary_op_wrapper(y, x)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication dimensions,\n",
      " |      and any further outer dimensions specify matching batch size.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      A simple 2-D tensor matrix multiplication:\n",
      " |      \n",
      " |      >>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      >>> a  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      " |      array([[1, 2, 3],\n",
      " |             [4, 5, 6]], dtype=int32)>\n",
      " |      >>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      >>> b  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      " |      array([[ 7,  8],\n",
      " |             [ 9, 10],\n",
      " |             [11, 12]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
      " |      array([[ 58,  64],\n",
      " |             [139, 154]], dtype=int32)>\n",
      " |      \n",
      " |      A batch matrix multiplication with batch shape [2]:\n",
      " |      \n",
      " |      >>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
      " |      >>> a  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
      " |      array([[[ 1,  2,  3],\n",
      " |              [ 4,  5,  6]],\n",
      " |             [[ 7,  8,  9],\n",
      " |              [10, 11, 12]]], dtype=int32)>\n",
      " |      >>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
      " |      >>> b  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
      " |      array([[[13, 14],\n",
      " |              [15, 16],\n",
      " |              [17, 18]],\n",
      " |             [[19, 20],\n",
      " |              [21, 22],\n",
      " |              [23, 24]]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
      " |      array([[[ 94, 100],\n",
      " |              [229, 244]],\n",
      " |             [[508, 532],\n",
      " |              [697, 730]]], dtype=int32)>\n",
      " |      \n",
      " |      Since python >= 3.5 the @ operator is supported\n",
      " |      (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\n",
      " |      it simply calls the `tf.matmul()` function, so the following lines are\n",
      " |      equivalent:\n",
      " |      \n",
      " |      >>> d = a @ b @ [[10], [11]]\n",
      " |      >>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n",
      " |          `complex64`, `complex128` and rank > 1.\n",
      " |        b: `tf.Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix. Notice, this\n",
      " |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
      " |          that assume most values in `a` are zero.\n",
      " |          See `tf.sparse.sparse_dense_matmul`\n",
      " |          for some support for `tf.sparse.SparseTensor` multiplication.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix. Notice, this\n",
      " |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
      " |          that assume most values in `a` are zero.\n",
      " |          See `tf.sparse.sparse_dense_matmul`\n",
      " |          for some support for `tf.sparse.SparseTensor` multiplication.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n",
      " |        is the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n",
      " |        for all indices `i`, `j`.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n",
      " |          `adjoint_b` are both set to `True`.\n",
      " |  \n",
      " |  __rmod__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `uint64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rmul__ = r_binary_op_wrapper(y, x)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ror__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rpow__ = r_binary_op_wrapper(y, x)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __rsub__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `uint32`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rtruediv__ = r_binary_op_wrapper(y, x)\n",
      " |      Divides x / y elementwise (using Python 3 division operator semantics).\n",
      " |      \n",
      " |      NOTE: Prefer using the Tensor operator or tf.divide which obey Python\n",
      " |      division operator semantics.\n",
      " |      \n",
      " |      This function forces Python 3 division operator semantics where all integer\n",
      " |      arguments are cast to floating types first.   This op is generated by normal\n",
      " |      `x / y` division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.  If you want integer division that rounds\n",
      " |      down, use `x // y` or `tf.math.floordiv`.\n",
      " |      \n",
      " |      `x` and `y` must have the same numeric type.  If the inputs are floating\n",
      " |      point, the output will have the same type.  If the inputs are integral, the\n",
      " |      inputs are cast to `float32` for `int8` and `int16` and `float64` for `int32`\n",
      " |      and `int64` (matching the behavior of Numpy).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of numeric type.\n",
      " |        y: `Tensor` denominator of numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` evaluated in floating point.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `x` and `y` have different dtypes.\n",
      " |  \n",
      " |  __rxor__ = r_binary_op_wrapper(y, x)\n",
      " |      Logical XOR function.\n",
      " |      \n",
      " |      x ^ y = (x | y) & ~(x & y)\n",
      " |      \n",
      " |      The operation works for the following input types:\n",
      " |      \n",
      " |      - Two single elements of type `bool`\n",
      " |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
      " |        be calculated by applying logical XOR with the single element to each\n",
      " |        element in the larger Tensor.\n",
      " |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
      " |        the result will be the element-wise logical XOR of the two input tensors.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      >>> a = tf.constant([True])\n",
      " |      >>> b = tf.constant([False])\n",
      " |      >>> tf.math.logical_xor(a, b)\n",
      " |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n",
      " |      \n",
      " |      >>> c = tf.constant([True])\n",
      " |      >>> x = tf.constant([False, True, True, False])\n",
      " |      >>> tf.math.logical_xor(c, x)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])>\n",
      " |      \n",
      " |      >>> y = tf.constant([False, False, True, True])\n",
      " |      >>> z = tf.constant([False, True, False, True])\n",
      " |      >>> tf.math.logical_xor(y, z)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
      " |      \n",
      " |      Args:\n",
      " |          x: A `tf.Tensor` type bool.\n",
      " |          y: A `tf.Tensor` of type bool.\n",
      " |          name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
      " |  \n",
      " |  __sub__ = binary_op_wrapper(x, y)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `uint32`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __truediv__ = binary_op_wrapper(x, y)\n",
      " |      Divides x / y elementwise (using Python 3 division operator semantics).\n",
      " |      \n",
      " |      NOTE: Prefer using the Tensor operator or tf.divide which obey Python\n",
      " |      division operator semantics.\n",
      " |      \n",
      " |      This function forces Python 3 division operator semantics where all integer\n",
      " |      arguments are cast to floating types first.   This op is generated by normal\n",
      " |      `x / y` division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.  If you want integer division that rounds\n",
      " |      down, use `x // y` or `tf.math.floordiv`.\n",
      " |      \n",
      " |      `x` and `y` must have the same numeric type.  If the inputs are floating\n",
      " |      point, the output will have the same type.  If the inputs are integral, the\n",
      " |      inputs are cast to `float32` for `int8` and `int16` and `float64` for `int32`\n",
      " |      and `int64` (matching the behavior of Numpy).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of numeric type.\n",
      " |        y: `Tensor` denominator of numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` evaluated in floating point.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `x` and `y` have different dtypes.\n",
      " |  \n",
      " |  __xor__ = binary_op_wrapper(x, y)\n",
      " |      Logical XOR function.\n",
      " |      \n",
      " |      x ^ y = (x | y) & ~(x & y)\n",
      " |      \n",
      " |      The operation works for the following input types:\n",
      " |      \n",
      " |      - Two single elements of type `bool`\n",
      " |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
      " |        be calculated by applying logical XOR with the single element to each\n",
      " |        element in the larger Tensor.\n",
      " |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
      " |        the result will be the element-wise logical XOR of the two input tensors.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      >>> a = tf.constant([True])\n",
      " |      >>> b = tf.constant([False])\n",
      " |      >>> tf.math.logical_xor(a, b)\n",
      " |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n",
      " |      \n",
      " |      >>> c = tf.constant([True])\n",
      " |      >>> x = tf.constant([False, True, True, False])\n",
      " |      >>> tf.math.logical_xor(c, x)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])>\n",
      " |      \n",
      " |      >>> y = tf.constant([False, False, True, True])\n",
      " |      >>> z = tf.constant([False, True, False, True])\n",
      " |      >>> tf.math.logical_xor(y, z)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
      " |      \n",
      " |      Args:\n",
      " |          x: A `tf.Tensor` type bool.\n",
      " |          y: A `tf.Tensor` of type bool.\n",
      " |          name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
      " |  \n",
      " |  experimental_ref(self)\n",
      " |      DEPRECATED FUNCTION\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Use ref() instead.\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      Alias of `Variable.shape`.\n",
      " |  \n",
      " |  initialized_value(self)\n",
      " |      Returns the value of the initialized variable. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      " |      \n",
      " |      You should use this instead of the variable itself to initialize another\n",
      " |      variable with a value that depends on the value of this variable.\n",
      " |      \n",
      " |      ```python\n",
      " |      # Initialize 'v' with a random tensor.\n",
      " |      v = tf.Variable(tf.random.truncated_normal([10, 40]))\n",
      " |      # Use `initialized_value` to guarantee that `v` has been\n",
      " |      # initialized before its value is used to initialize `w`.\n",
      " |      # The random values are picked only once.\n",
      " |      w = tf.Variable(v.initialized_value() * 2.0)\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` holding the value of this variable after its initializer\n",
      " |        has run.\n",
      " |  \n",
      " |  load(self, value, session=None)\n",
      " |      Load new value into this variable. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      " |      \n",
      " |      Writes new value to variable's memory. Doesn't add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.  See `tf.compat.v1.Session` for more\n",
      " |      information on launching a graph and on sessions.\n",
      " |      \n",
      " |      ```python\n",
      " |      v = tf.Variable([1, 2])\n",
      " |      init = tf.compat.v1.global_variables_initializer()\n",
      " |      \n",
      " |      with tf.compat.v1.Session() as sess:\n",
      " |          sess.run(init)\n",
      " |          # Usage passing the session explicitly.\n",
      " |          v.load([2, 3], sess)\n",
      " |          print(v.eval(sess)) # prints [2 3]\n",
      " |          # Usage with the default session.  The 'with' block\n",
      " |          # above makes 'sess' the default session.\n",
      " |          v.load([3, 4], sess)\n",
      " |          print(v.eval()) # prints [3 4]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          value: New variable value\n",
      " |          session: The session to use to evaluate this variable. If none, the\n",
      " |            default session is used.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: Session is not passed and no default session\n",
      " |  \n",
      " |  ref(self)\n",
      " |      Returns a hashable reference object to this Variable.\n",
      " |      \n",
      " |      The primary use case for this API is to put variables in a set/dictionary.\n",
      " |      We can't put variables in a set/dictionary as `variable.__hash__()` is no\n",
      " |      longer available starting Tensorflow 2.0.\n",
      " |      \n",
      " |      The following will raise an exception starting 2.0\n",
      " |      \n",
      " |      >>> x = tf.Variable(5)\n",
      " |      >>> y = tf.Variable(10)\n",
      " |      >>> z = tf.Variable(10)\n",
      " |      >>> variable_set = {x, y, z}\n",
      " |      Traceback (most recent call last):\n",
      " |        ...\n",
      " |      TypeError: Variable is unhashable. Instead, use tensor.ref() as the key.\n",
      " |      >>> variable_dict = {x: 'five', y: 'ten'}\n",
      " |      Traceback (most recent call last):\n",
      " |        ...\n",
      " |      TypeError: Variable is unhashable. Instead, use tensor.ref() as the key.\n",
      " |      \n",
      " |      Instead, we can use `variable.ref()`.\n",
      " |      \n",
      " |      >>> variable_set = {x.ref(), y.ref(), z.ref()}\n",
      " |      >>> x.ref() in variable_set\n",
      " |      True\n",
      " |      >>> variable_dict = {x.ref(): 'five', y.ref(): 'ten', z.ref(): 'ten'}\n",
      " |      >>> variable_dict[y.ref()]\n",
      " |      'ten'\n",
      " |      \n",
      " |      Also, the reference object provides `.deref()` function that returns the\n",
      " |      original Variable.\n",
      " |      \n",
      " |      >>> x = tf.Variable(5)\n",
      " |      >>> x.ref().deref()\n",
      " |      <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=5>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.optimizer.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv_v2",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
