{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Individual Model 1-wenkanw data\n",
    "## Hardware settings: 4cpus, 120GB,  1 gpus, gpu:V100, interconnection: 25ge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda Device Count:  1 Device Name:  Tesla P100-PCIE-12GB\n",
      "Torch version: 1.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from dataset import create_train_test_file_list, Person_MealsDataset, balance_data_indices\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device: \",device,\"Device Count: \", torch.cuda.device_count(), \"Device Name: \",torch.cuda.get_device_name()  )\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "### imports\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Turn off TensorFlow logging\n",
    "import tensorflow.compat.v1 as tf # maintain compatibility with TensorFlow 2.2.0\n",
    "\n",
    "import keras\n",
    "# from tensorflow.compat.v1.keras import backend as K # changed for compatibility with TensorFlow 2.2.0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "import loadfile\n",
    "import addons\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l1\n",
    "from keras.models import load_model, save_model\n",
    "\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "shimmer_global_mean = [-0.012359981,-0.0051663737,0.011612018,\n",
    "                        0.05796114,0.1477952,-0.034395125 ]\n",
    "\n",
    "shimmer_global_stddev = [0.05756385,0.040893298,0.043825723,\n",
    "                        17.199743,15.311142,21.229317 ]\n",
    "\n",
    "shimmer_trended_mean = [-0.000002,-0.000002,-0.000000,\n",
    "                0.058144,0.147621,-0.033260 ]\n",
    "\n",
    "shimmer_trended_stddev = [0.037592,0.034135,0.032263,\n",
    "                17.209038,15.321441,21.242532 ]\n",
    "\n",
    "all_zero_means = [0,0,0,0,0,0]\n",
    "\n",
    "meanvals = all_zero_means\n",
    "stdvals = shimmer_trended_stddev\n",
    "\n",
    "\n",
    "random_seed  = 1000\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_settings(winmin,stridesec, EPOCHS):\n",
    "    outfile = sys.stdout\n",
    "\n",
    "    winlength = int(winmin * 60 * 15)\n",
    "    step = int(stridesec * 15)\n",
    "    start_time = datetime.now()\n",
    "    arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "          \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "          \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "\n",
    "    [os.system(cmd) for cmd in arr]\n",
    "    print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "    print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "    print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "\n",
      "Execution Started at 12/19/2020, 16:51:08\n",
      "WindowLength: 1.00 min (900 datum)\tSlide: 1 (15 datum)\tEpochs:30\n",
      "\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-14-20/10-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-3-20/10-3-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-9-20/10-9-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-14-20/11-14-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-16-20/11-16-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-18-20/11-18-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-21-20/lunch/lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Dinner/Dinner.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/11-22-20/Lunch/Lunch.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-17-20/9-17-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-27-20/9-27-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/9-29-20/9-29-20.shm\n",
      "Loading Dataset ...\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-10-20/10-10-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-12-20/10-12-20.shm\n",
      "Loading File:  ../data/IndividualData/wenkanw-data/10-13-20/10-13-20.shm\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "winmin = 1 \n",
    "stridesec = 1\n",
    "\n",
    "print_settings(winmin,stridesec, EPOCHS)\n",
    "# Load the dataset\n",
    "meal_data_train = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "meal_data_test = Person_MealsDataset(person_name= \"wenkanw\", file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41878, 13520)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random.seed(random_seed)\n",
    "\n",
    "# load\n",
    "trainingsamples,traininglabels =  meal_data_train.data_indices, meal_data_train.labels\n",
    "testsamples,testlabels =  meal_data_test.data_indices, meal_data_test.labels\n",
    "\n",
    "train_shuffledUnderSampledBalancedIndices = balance_data_indices(traininglabels,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "test_shuffledUnderSampledBalancedIndices = balance_data_indices(testlabels,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "len(train_shuffledUnderSampledBalancedIndices),len(test_shuffledUnderSampledBalancedIndices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print([item for item, count in collections.Counter(train_shuffledUnderSampledBalancedIndices).items() if count > 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traininglabels[train_shuffledUnderSampledBalancedIndices]\n",
    "testlabels[test_shuffledUnderSampledBalancedIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_indices(X, y, test_size, random_seed = None):\n",
    "    \"\"\"\n",
    "    This function is to split the training set indices into validation set indices and training set indices\n",
    "    \n",
    "    X: indices of subset of the whole dataset\n",
    "    y: labels of the whole dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, train_test_split\n",
    "\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    if test_size ==0:\n",
    "        train_indices = X\n",
    "    elif test_size == 1:\n",
    "        test_indices = X\n",
    "    elif test_size >0 and test_size <1:\n",
    "        labels = y[X]\n",
    "        \n",
    "        train_indices, test_indices, _,_ = train_test_split(X,labels ,\n",
    "                                                            stratify=labels, \n",
    "                                                            test_size=test_size,random_state = random_seed)\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid split ratio: %.3f\"%(test_size))\n",
    "    if len(train_indices)>0:\n",
    "        print(\"Train set size: %d, with %d positive samples and %d negative samples\"%(len(y[train_indices]),sum(y[train_indices]==1),\n",
    "                                                                          sum(y[train_indices]==0)))\n",
    "    if len(test_indices)>0:\n",
    "        print(\"Test set size: %d, with %d positive samples and %d negative samples\"%(len(y[test_indices]),\n",
    "                                                                          sum(y[test_indices]==1),\n",
    "                                                                           sum(y[test_indices]==0)))\n",
    "    \n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 33502, with 16751 positive samples and 16751 negative samples\n",
      "Test set size: 8376, with 4188 positive samples and 4188 negative samples\n"
     ]
    }
   ],
   "source": [
    "train_indices, valid_indices = split_train_test_indices(X= train_shuffledUnderSampledBalancedIndices,\n",
    "                                                        y = traininglabels, test_size = 0.2,\n",
    "                                                       random_seed = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set(train_indices)\n",
    "b = set(valid_indices)\n",
    "c = a.intersection(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loader Created\n"
     ]
    }
   ],
   "source": [
    "train_set_balanced = torch.utils.data.Subset(meal_data_train, train_indices)\n",
    "valid_set_balanced = torch.utils.data.Subset(meal_data_train, valid_indices)\n",
    "\n",
    "test_set_balanced = torch.utils.data.Subset(meal_data_test, test_shuffledUnderSampledBalancedIndices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set_balanced ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "print(\"Data Loader Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set batch amounts: 262\n",
      "Test set : 66\n",
      "Start Training..\n",
      "Epoch: 0,  Epoch_Loss: 0.4631, Train Acc: 80.4818 %, Train Recall: 0.7680 \n",
      "Validation Acc:  74.2956 %,  Validation Recall: 0.5117 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 1,  Epoch_Loss: 0.3811, Train Acc: 84.2457 %, Train Recall: 0.8001 \n",
      "Validation Acc:  83.5840 %,  Validation Recall: 0.7199 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 2,  Epoch_Loss: 0.3856, Train Acc: 83.9950 %, Train Recall: 0.7998 \n",
      "Validation Acc:  85.0167 %,  Validation Recall: 0.7729 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 3,  Epoch_Loss: 0.4006, Train Acc: 83.2637 %, Train Recall: 0.7889 \n",
      "Validation Acc:  75.8118 %,  Validation Recall: 0.5979 \n",
      "\n",
      "\n",
      "Epoch: 4,  Epoch_Loss: 0.3164, Train Acc: 87.5679 %, Train Recall: 0.8408 \n",
      "Validation Acc:  88.7416 %,  Validation Recall: 0.8926 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 5,  Epoch_Loss: 0.2871, Train Acc: 88.7828 %, Train Recall: 0.8551 \n",
      "Validation Acc:  88.5029 %,  Validation Recall: 0.8023 \n",
      "\n",
      "\n",
      "Epoch: 6,  Epoch_Loss: 0.2680, Train Acc: 89.9558 %, Train Recall: 0.8713 \n",
      "Validation Acc:  91.3324 %,  Validation Recall: 0.9071 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 7,  Epoch_Loss: 0.2088, Train Acc: 92.7706 %, Train Recall: 0.9041 \n",
      "Validation Acc:  93.7321 %,  Validation Recall: 0.9384 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 8,  Epoch_Loss: 0.1897, Train Acc: 93.6422 %, Train Recall: 0.9194 \n",
      "Validation Acc:  93.7560 %,  Validation Recall: 0.9114 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 9,  Epoch_Loss: 0.1788, Train Acc: 94.0959 %, Train Recall: 0.9266 \n",
      "Validation Acc:  93.6724 %,  Validation Recall: 0.9616 \n",
      "\n",
      "\n",
      "Epoch: 10,  Epoch_Loss: 0.1636, Train Acc: 94.7615 %, Train Recall: 0.9337 \n",
      "Validation Acc:  94.9140 %,  Validation Recall: 0.9348 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 11,  Epoch_Loss: 0.1618, Train Acc: 94.8212 %, Train Recall: 0.9363 \n",
      "Validation Acc:  95.0215 %,  Validation Recall: 0.9353 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 12,  Epoch_Loss: 0.1595, Train Acc: 94.8869 %, Train Recall: 0.9375 \n",
      "Validation Acc:  95.1409 %,  Validation Recall: 0.9401 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 13,  Epoch_Loss: 0.1571, Train Acc: 95.0421 %, Train Recall: 0.9389 \n",
      "Validation Acc:  95.0692 %,  Validation Recall: 0.9384 \n",
      "\n",
      "\n",
      "Epoch: 14,  Epoch_Loss: 0.1567, Train Acc: 95.1495 %, Train Recall: 0.9407 \n",
      "Validation Acc:  95.0692 %,  Validation Recall: 0.9413 \n",
      "\n",
      "\n",
      "Epoch: 15,  Epoch_Loss: 0.1575, Train Acc: 95.0540 %, Train Recall: 0.9381 \n",
      "Validation Acc:  95.1289 %,  Validation Recall: 0.9391 \n",
      "\n",
      "\n",
      "Epoch: 16,  Epoch_Loss: 0.1572, Train Acc: 95.0600 %, Train Recall: 0.9387 \n",
      "Validation Acc:  95.1409 %,  Validation Recall: 0.9446 \n",
      "\n",
      "\n",
      "Epoch: 17,  Epoch_Loss: 0.1570, Train Acc: 95.1078 %, Train Recall: 0.9395 \n",
      "Validation Acc:  95.0812 %,  Validation Recall: 0.9427 \n",
      "\n",
      "\n",
      "Epoch: 18,  Epoch_Loss: 0.1578, Train Acc: 95.0510 %, Train Recall: 0.9386 \n",
      "Validation Acc:  95.0573 %,  Validation Recall: 0.9370 \n",
      "\n",
      "\n",
      "Epoch: 19,  Epoch_Loss: 0.1566, Train Acc: 95.0421 %, Train Recall: 0.9389 \n",
      "Validation Acc:  95.1528 %,  Validation Recall: 0.9401 \n",
      "Checkpoint Saved\n",
      "\n",
      "\n",
      "Epoch: 20,  Epoch_Loss: 0.1575, Train Acc: 95.0928 %, Train Recall: 0.9390 \n",
      "Validation Acc:  95.1289 %,  Validation Recall: 0.9403 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from utils import *\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "input_shape = (train_set_balanced[0][0].shape[1], train_set_balanced[0][0].shape[0])\n",
    "model_4 = Discriminator_ResNet( ngpu=1, input_shape =input_shape , out_fea = 1)\n",
    "# Since I use a dynamic created layer in network, need to input a sample to initialize the model first\n",
    "_ = model_4(torch.rand((1, input_shape[0],input_shape[1])))\n",
    "model_4.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_4 = optim.Adam(model_4.parameters(),lr=0.01,  weight_decay=0.1)\n",
    "lrscheduler_4 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_4, mode='min',patience= 2, factor = 0.1,threshold=0.01)\n",
    "dataloader = (train_loader, valid_loader)\n",
    "model_4, best_model_4,val_score_4,loss_ls_4, train_acc_ls_4, valid_acc_ls_4 = train_model(model_4,dataloader, optimizer_4, \n",
    "                                                                    criterion, lrscheduler_4, device= device,\n",
    "                                                                    n_epochs=30, patience = 5, l1_enabled=False,\n",
    "                                                                    checkpoint_name =\"../models/wenkanw_models/checkpoint_model_resnet.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_4\n",
    "f'cuda:{model_4.device_ids[0]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from utils import *\n",
    "\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "input_shape = (train_set_balanced[0][0].shape[1], train_set_balanced[0][0].shape[0])\n",
    "model_4 = Discriminator_ResNet( ngpu=1, input_shape =input_shape , out_fea = 1).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_4 = optim.Adam(model_4.parameters(),lr=0.01,  weight_decay=0.1)\n",
    "lrscheduler_4 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_4, mode='min',patience= 2, factor = 0.1,threshold=0.01)\n",
    "dataloader = (train_loader, test_loader)\n",
    "model_4, best_model_4,val_score_4,loss_ls_4, train_acc_ls_4, valid_acc_ls_4 = train_model(model_4,dataloader, optimizer_4, \n",
    "                                                                    criterion, lrscheduler_4, device= device,\n",
    "                                                                    n_epochs=30, patience = 5, l1_enabled=False,\n",
    "                                                                    checkpoint_name =\"../models/wenkanw_models/checkpoint_model_resnet.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_v1(model,dataloader,device=\"cpu\"):\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    # without update\n",
    "    with torch.no_grad():\n",
    "        for samples, labels in dataloader:\n",
    "            samples = samples.cuda()#.to(device)\n",
    "            labels = labels.cuda()#.to(device)\n",
    "            outputs = model(samples).to(device).squeeze()\n",
    "            #print(\"Output: \", outputs)\n",
    "            outputs = torch.round(torch.sigmoid(outputs))\n",
    "            preds = outputs>=0.5\n",
    "            preds = preds.to(dtype = torch.float)\n",
    "            preds.requires_grad = False\n",
    "#             _,preds = torch.max(outputs,1)\n",
    "            for i in range(len(preds)):\n",
    "                if preds[i] == 1 and labels[i] == 1:\n",
    "                    TP += 1\n",
    "                if preds[i] == 0 and labels[i] == 1:\n",
    "                    FN += 1\n",
    "            correct += torch.sum((preds == labels)).item()\n",
    "            total += float(len(labels))\n",
    "        acc =100 * correct/ total\n",
    "        recall = TP/(TP+FN)\n",
    "#         print(\"Evaluation Acc: %.4f %%,  Recall: %.4f \"%(acc , recall))\n",
    "    return acc, recall\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "def train_model_v1(model,dataloader, optimizer, criterion,lrscheduler,device=\"cpu\" , n_epochs=20,\n",
    "                earlystopping=True, patience= 3, l1_enabled=True,checkpoint_name =\"checkpoint.pt\" ):\n",
    "    loss_ls = [0.0]\n",
    "    train_acc_ls = [0.0]\n",
    "    valid_acc_ls = [0.0]\n",
    "    valid_acc = 0.0\n",
    "    loss =0.0\n",
    "    train_acc = 0.0\n",
    "    patience_count = 0\n",
    "    best_val_score = 0.0\n",
    "    prev_val_score = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    train_dataloader, valid_dataloader = dataloader\n",
    "    print(\"Training set batch amounts:\", len(train_dataloader))\n",
    "    print(\"Test set :\", len(valid_dataloader))\n",
    "    print(\"Start Training..\")\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        correct_cnt = 0.0\n",
    "        total_cnt = 0.0\n",
    "        TP = 0.\n",
    "        FN = 0.\n",
    "        model.train()\n",
    "        for i, (samples, labels) in enumerate(train_dataloader):\n",
    "            samples = nn.DataParallel(samples, device_ids=[0,1])\n",
    "            labels = nn.DataParallel(labels, device_ids=[0,1])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # reshape samples\n",
    "            outputs = model(samples).squeeze()\n",
    "\n",
    "            #print(\"Output: \", outputs, \"label: \", labels)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            if l1_enabled:\n",
    "                L1_loss = model.l1_loss(0.01).to(device)\n",
    "                loss += L1_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # prediction\n",
    "            #_,preds = torch.max(outputs,1)\n",
    "            outputs = torch.round(torch.sigmoid(outputs))\n",
    "            preds = outputs>=0.5\n",
    "            preds = preds.to(dtype = torch.float)\n",
    "            preds.requires_grad = False\n",
    "            \n",
    "            # Compute count of TP, FN\n",
    "            for j in range(len(preds)):\n",
    "                if preds[j] == 1. and labels[j] == 1.:\n",
    "                    TP += 1\n",
    "                if preds[j] == 0. and labels[j] == 1.:\n",
    "                    FN += 1\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            correct_cnt += torch.sum((preds == labels)).item()\n",
    "            total_cnt += float(len(labels))\n",
    "            batch_acc = 100. * (preds == labels).sum().item()/ float(len(labels))\n",
    "            if i %50 ==0:\n",
    "                #print(\"===> Batch: %d,  Batch_Loss: %.4f, Train Acc: %.4f %%,  Recall: %.f\\n\"%(i, loss,batch_acc, recall))\n",
    "                pass\n",
    "\n",
    "            \n",
    "        \n",
    "        # Compute accuracy and loss of one epoch\n",
    "        epoch_loss = running_loss / len(train_dataloader)  \n",
    "        epoch_acc = 100* correct_cnt/ total_cnt  # in percentage\n",
    "        correct_cnt = 0.0\n",
    "        total_cnt = 0.0\n",
    "        train_recall = TP/(TP+FN)\n",
    "        \n",
    "        #Validation mode\n",
    "        model.eval()\n",
    "        valid_acc, valid_recall= eval_model_v1(model,valid_dataloader,device=device)\n",
    "        \n",
    "        # record loss and accuracy\n",
    "        valid_acc_ls.append(valid_acc)  \n",
    "        train_acc_ls.append(epoch_acc)\n",
    "        loss_ls.append(epoch_loss)\n",
    "        \n",
    "        if e %1==0:\n",
    "            print(\"Epoch: %d,  Epoch_Loss: %.4f, Train Acc: %.4f %%, Train Recall: %.4f \"%(e, epoch_loss,\n",
    "                                                                                     epoch_acc,train_recall))\n",
    "            print(\"Validation Acc:  %.4f %%,  Validation Recall: %.4f \"%(valid_acc, valid_recall))\n",
    "        \n",
    "        # Reset train mode\n",
    "        model.train()\n",
    "        lrscheduler.step(valid_acc)\n",
    "        \n",
    "        \n",
    "        # If earlystopping is enabled, then save model if performance is improved\n",
    "        if earlystopping:\n",
    "            if prev_val_score !=0. and valid_acc < prev_val_score :\n",
    "                patience_count += 1\n",
    "            else:\n",
    "                patience_count = 0\n",
    "                \n",
    "            if patience_count >= patience:\n",
    "                break \n",
    "                \n",
    "            prev_val_score = valid_acc\n",
    "            if valid_acc > best_val_score or best_val_score == 0.0:\n",
    "                best_val_score = valid_acc\n",
    "                torch.save(model,checkpoint_name)\n",
    "                print(\"Checkpoint Saved\")\n",
    "            \n",
    "                \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "            \n",
    "    # Load best model\n",
    "    best_model = torch.load(checkpoint_name)\n",
    "    print(\"Load Best Model.\")\n",
    "    print(\"Training completed\")\n",
    "        \n",
    "    return model, best_model,best_val_score,loss_ls, train_acc_ls, valid_acc_ls\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
