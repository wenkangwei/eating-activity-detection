{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))\n",
    "\n",
    "# Reserve 10,000 samples for validation.\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Get model\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "import time\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * 64))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))\n",
    "\n",
    "# Reserve 10,000 samples for validation.\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Get model\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_group_model(model, train_dataset, valid_dataset, epochs = 2):\n",
    "    import time\n",
    "    # Instantiate an optimizer to train the model.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    # Instantiate a loss function.\n",
    "    loss_fn  = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    train_acc_metric = keras.metrics.BinaryAccuracy(\n",
    "        name='binary_accuracy', dtype=None, threshold=0.5)\n",
    "    val_acc_metric = keras.metrics.BinaryAccuracy(\n",
    "        name='binary_accuracy', dtype=None, threshold=0.5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        pbar = tqdm(train_dataset, dynamic_ncols=True, total=len(train_dataset))\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(pbar):\n",
    "            x_batch_train = tf.convert_to_tensor(x_batch_train.numpy(), dtype=tf.float32)\n",
    "            y_batch_train = tf.convert_to_tensor(y_batch_train.numpy(), dtype=tf.int64)\n",
    "#             print(x_batch_train)\n",
    "#             print(y_batch_train)\n",
    "            with tf.GradientTape() as tape:\n",
    "                #print(x_batch_train.shape,y_batch_train.shape)\n",
    "                logits = model(x_batch_train, training=True)\n",
    "                loss_value = loss_fn(y_batch_train, logits)\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "            # Update training metric.\n",
    "            train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "            # Log every 200 batches.\n",
    "            if step % 200 == 0:\n",
    "                descrption = f'epoch {step} loss: {loss_value:.4f}'\n",
    "\n",
    "\n",
    "        # Display metrics at the end of each epoch.\n",
    "        train_acc = train_acc_metric.result()\n",
    "        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "        # Reset training metrics at the end of each epoch\n",
    "        train_acc_metric.reset_states()\n",
    "\n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        for x_batch_val, y_batch_val in valid_dataset:\n",
    "            x_batch_val = tf.convert_to_tensor(x_batch_val.numpy(), dtype=tf.float32)\n",
    "            y_batch_val = tf.convert_to_tensor(y_batch_val.numpy(), dtype=tf.int64)\n",
    "            #print(x_batch_val.shape, y_batch_val.shape)\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            # Update val metrics\n",
    "            val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        val_acc = val_acc_metric.result()\n",
    "        val_acc_metric.reset_states()\n",
    "        print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "        print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "winmin= 6\n",
    "stridesec = 5\n",
    "win_size = 15*winmin*60\n",
    "model_1 = acti_model(input_shape =(win_size,6) )\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "train_loader = torch.utils.data.DataLoader(meal_data_train,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "labels = meal_data_train.labels\n",
    "train_indices, test_indices = split_train_test_indices(X= [i for i in range(len(labels))],\n",
    "                                                                    y = labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "\n",
    "# train_indices = train_indices[:1000]\n",
    "# train_set_balanced = train_indices[:1000]\n",
    "# test_indices = test_indices[:1000]\n",
    "# balance train set\n",
    "trainset_labels = labels[train_indices]\n",
    "train_indices_balanced = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "\n",
    "train_set_balanced = torch.utils.data.Subset(meal_data_train, train_indices_balanced)\n",
    "test_set = torch.utils.data.Subset(meal_data_train, test_indices)\n",
    "            \n",
    "            \n",
    "train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "            \n",
    "train_group_model(model_1, train_loader, test_loader, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = \"wenkanw\"\n",
    "meal_data_train = Person_MealsDataset(person_name= person, file_name = \"train_files\", winmin = 6,stridesec = 5,get_numpy_data=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(meal_data_train,batch_size=16, shuffle=True,num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataset import *\n",
    "\n",
    "class Person_MealsDataset2(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset = None,person_name= \"wenkanw\", \n",
    "                 data_indices_file = \"../data-file-indices/\",\n",
    "                 file_name = \"all_files_list\",\n",
    "                 remove_trend = 0,\n",
    "                 remove_walk = 0,\n",
    "                 remove_rest = 0,\n",
    "                 smooth_flag = 1,\n",
    "                 normalize_flag = 1,\n",
    "                 winmin = 6,\n",
    "                 stridesec = 15,\n",
    "                 gtperc = 0.5,\n",
    "                 device = 'cpu',\n",
    "                 ratio_dataset=1,\n",
    "                load_splitted_dataset = False,\n",
    "                 enable_time_feat = False,\n",
    "                 debug_flag= False,\n",
    "                 tf_data=True,\n",
    "                 get_numpy_data= True,\n",
    "                ):\n",
    "        \n",
    "        if file_name == \"train\":\n",
    "            file_name = data_indices_file + person_name +\"/\"+\"train_files.txt\"\n",
    "        elif file_name == \"test\":\n",
    "            file_name = data_indices_file + person_name +\"/\"+\"test_files.txt\"\n",
    "        else:\n",
    "            file_name = data_indices_file + person_name +\"/\"+ file_name+\".txt\"\n",
    "            \n",
    "        # Note: file_name is the name of file that contain the list of shm files' names\n",
    "        self.tf_data = tf_data\n",
    "        self.get_numpy_data= get_numpy_data\n",
    "        self.file_name = file_name\n",
    "        self.dataset = dataset\n",
    "        self.person_name = person_name\n",
    "        self.winmin = winmin\n",
    "        self.stridesec = stridesec\n",
    "        self.load_splitted_dataset = load_splitted_dataset\n",
    "        self.remove_trend = remove_trend\n",
    "        self.remove_walk = remove_walk\n",
    "        self.remove_rest = remove_rest\n",
    "        self.smooth_flag = smooth_flag\n",
    "        self.normalize_flag = normalize_flag\n",
    "        self.gtperc = gtperc,\n",
    "        self.ratio_dataset = ratio_dataset\n",
    "        self.enable_time_feat = enable_time_feat\n",
    "        self.device = device\n",
    "        self.debug_flag= debug_flag\n",
    "        if not self.dataset:\n",
    "            self.get_data(person_name)\n",
    "\n",
    "    def get_data(self, person_name):\n",
    "            \n",
    "            \n",
    "            # files_counts, data, samples_indices, labels_array\n",
    "            # Note: the data preprocessing in this function is for global time series dataset\n",
    "            \n",
    "            self.dataset, self.data, self.data_indices, self.labels = load_train_test_data(data_file_list =self.file_name,\n",
    "                                    load_splitted_dataset = False,\n",
    "                                     ratio_dataset=self.ratio_dataset,\n",
    "                                     enabled_time_feat = self.enable_time_feat, \n",
    "                                     winmin = self.winmin, stridesec = self.stridesec,gtperc = self.gtperc,\n",
    "                                     removerest = self.remove_rest,\n",
    "                                     removewalk = self.remove_walk, smooth_flag = self.smooth_flag, normalize_flag=self.normalize_flag, \n",
    "                                     remove_trend = self.remove_trend,\n",
    "                                     debug_flag=self.debug_flag )\n",
    "            \n",
    "            if self.load_splitted_dataset:\n",
    "                self.dataset = self.get_dataset()\n",
    "                \n",
    "            \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        #这里需要注意的是，第一步：read one data，是一个data\n",
    "        data = self.get_item(index)\n",
    "        if self.tf_data:\n",
    "            return data['data']\n",
    "        return data['data'],data['label']\n",
    "        \n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return  len(self.dataset) if self.load_splitted_dataset else len(self.data_indices)\n",
    "    def get_item(self, index, tensor_type=True):\n",
    "        \"\"\"\n",
    "        This function is used to obtain one sample data point\n",
    "        \"\"\"\n",
    "        f,start_time, end_time = self.data_indices[index,0], self.data_indices[index,1], self.data_indices[index,2]\n",
    "        sample = self.data[f][start_time : end_time]\n",
    "        data = pd.DataFrame(columns=['data','label'])    \n",
    "        # Add time feature to data\n",
    "        if self.enable_time_feat:\n",
    "            time_offset = self.data_indices[index,3]\n",
    "            freq = 1.0/15.0\n",
    "            time_feat = np.array([[i for i in range(len(sample))]],dtype=float).transpose()\n",
    "            time_feat *= freq\n",
    "            time_feat += float(start_time)* freq\n",
    "            time_feat += time_offset\n",
    "            sample = np.concatenate((sample, time_feat),axis=1)\n",
    "        label = self.labels[index]\n",
    "        if not self.get_numpy_data:\n",
    "            data = {\"data\":torch.tensor(sample, dtype =torch.float, device =  self.device ), 'label': label}\n",
    "        else:\n",
    "            data = {\"data\":sample, 'label': label}\n",
    "        return data\n",
    "    \n",
    "    def get_dataset(self, start_index = None, end_index = None):\n",
    "        \"\"\"\n",
    "        This function is used to obtain the whole dataset in pandas or part of whole dataset\n",
    "        It is good to use this to sample some data to analyze\n",
    "        \"\"\"\n",
    "        start_i = 0 if not start_index else start_index\n",
    "        end_i = self.__len__() if not end_index else end_index\n",
    "        \n",
    "        dataset = pd.DataFrame(columns=['data','label'])\n",
    "        for i in tqdm(range(start_i, end_i)):\n",
    "            data = self.get_item(i)\n",
    "            dataset = dataset.append(data,ignore_index=True)\n",
    "        self.dataset = dataset\n",
    "        return self.dataset\n",
    "    \n",
    "    def sample(self, num = 1000,random_state = None):\n",
    "        \"\"\"\n",
    "        Simply sample part of data for analysis\n",
    "        \"\"\"\n",
    "        if random_state != None:\n",
    "            np.random.seed(random_state)\n",
    "            \n",
    "        sample_data = pd.DataFrame(columns=['data','label'])\n",
    "        indices = np.random.choice(len(self.labels), num)\n",
    "        for i in tqdm(indices):\n",
    "            data = self.get_item(i)\n",
    "            data[\"data\"] = data[\"data\"].numpy()\n",
    "            sample_data = sample_data.append(data,ignore_index=True)\n",
    "        return sample_data\n",
    "    \n",
    "    def get_subset(self, indices_ls):\n",
    "        axdata = []\n",
    "        aydata = []\n",
    "        for i in indices_ls:\n",
    "            data = self.get_item(i, tensor_type=False)\n",
    "            sample = data['data']\n",
    "            label = data['label']\n",
    "            axdata.append(sample)\n",
    "            aydata.append(label)\n",
    "        subsetData = np.array(axdata, copy=True) # Undersampled Balanced Training Set\n",
    "        subsetLabels = np.array(aydata, copy=True)\n",
    "        del axdata\n",
    "        del aydata\n",
    "        return subsetData, subsetLabels\n",
    "    \n",
    "    def get_mealdataset_info(self,person_name = None,file_ls = [], root_path = \"../data/\",print_file=False):\n",
    "        \"\"\"\n",
    "        if file_ls is not given, then get file_ls according to person_name\n",
    "        file path = root_path + file name in all_files_list.txt\n",
    "\n",
    "        return:\n",
    "            meal event count, total minutes of all meals, total hours of all meals,total day counts\n",
    "\n",
    "        \"\"\"\n",
    "        if person_name ==None:\n",
    "            person_name = self.person_name\n",
    "        if len(file_ls) ==0:\n",
    "            data_indices_file = \"../data-file-indices/\" +person_name+\"/all_files_list.txt\"\n",
    "            fp = open(data_indices_file,\"r\")\n",
    "            txt = fp.read()\n",
    "            fp.close()\n",
    "            file_ls = txt.split(\"\\n\")\n",
    "            while '' in file_ls:\n",
    "                file_ls.remove('')\n",
    "\n",
    "        meal_counts = 0\n",
    "        sec_counts = 0\n",
    "        min_counts = 0\n",
    "        hour_counts = 0\n",
    "        total_hours = 0\n",
    "        total_mins = 0\n",
    "        total_sec = 0\n",
    "        day_counts = len(file_ls)\n",
    "        for file_name in file_ls:\n",
    "            file_name = root_path + file_name\n",
    "            TotalEvents, EventStart, EventEnd, EventNames, TimeOffset,EndTime = loadEvents(file_name, debug_flag = False, print_file=print_file)\n",
    "            meal_counts += TotalEvents\n",
    "            total_sec +=  abs(EndTime - TimeOffset)\n",
    "#             total_hours += (EndTime//(60*60) - TimeOffset//(60*60))\n",
    "#             total_mins  += (EndTime%(60*60) - TimeOffset//(60*60))\n",
    "            for i in range(len(EventStart)):\n",
    "                sec_counts += ( EventEnd[i]- EventStart[i])//(15)\n",
    "        total_hours = total_sec//(60*60)\n",
    "        min_counts = sec_counts//60\n",
    "        hour_counts = min_counts//60\n",
    "        \n",
    "        return meal_counts, min_counts,hour_counts, day_counts, total_hours\n",
    "\n",
    "\n",
    "\n",
    "class tf_dataset:\n",
    "    def __init__(self,data,shape=[5400,6],batch=16):\n",
    "        self.data = data    \n",
    "        self.dataset = tf.data.Dataset.from_generator( self.gen,(tf.float32, tf.int32),(tf.TensorShape(shape), tf.TensorShape([])))\n",
    "        #self.dataset = self.dataset.batch(batch)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def gen( self):\n",
    "        data = self.data\n",
    "        for i in range(len(data)):\n",
    "            yield data[i][0], data[i][1]\n",
    "            \n",
    "ds = tf_dataset(meal_data_train)\n",
    "\n",
    "class TF_DataSet:\n",
    "    def __init__(self,data, indices=None,batch=16):\n",
    "        self.tf_data = data\n",
    "        self.indices = indices\n",
    "        if indices != None:\n",
    "            self.dataset = tf.data.Dataset.from_tensor_slices( (self.tf_data.data_indices[self.indices], self.tf_data.labels[self.indices]) )\n",
    "        else:\n",
    "            self.dataset = tf.data.Dataset.from_tensor_slices( (self.tf_data.data_indices, self.tf_data.labels) )\n",
    "#         self.dataset = tf.data.Dataset.from_tensor_slices( (self.tf_data,self.tf_data.labels ))\n",
    "            \n",
    "        self.dataset = self.dataset.map(lambda x,y: tf.py_function(func=self.map_fun, inp=[x,y], Tout=[tf.float32,tf.int64]))\n",
    "        self.dataset = self.dataset.batch(batch)\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def map_fun(self,x,y ):\n",
    "        \n",
    "        return self.tf_data.data[x[0]][x[1]:x[2]],y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Tensorflow Model with Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "# mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "# optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn  = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "loss_tracker = keras.metrics.BinaryCrossentropy(name=\"loss\")\n",
    "mae_metric = keras.metrics.BinaryAccuracy(\n",
    "        name='binary_accuracy', dtype=None, threshold=0.5)\n",
    "\n",
    "# val_acc_metric = keras.metrics.BinaryAccuracy(\n",
    "#         name='binary_accuracy', dtype=None, threshold=0.5)\n",
    "\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        print(data[0],data[1])\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute our own loss\n",
    "            loss = loss_fn(y, y_pred)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "#         loss_tracker.update_state(loss)\n",
    "        loss_tracker.update_state(y, y_pred)\n",
    "        mae_metric.update_state(y, y_pred)\n",
    "        return {\"loss\": loss_tracker.result(), \"mae\": mae_metric.result()}\n",
    "\n",
    "#     @property\n",
    "#     def metrics(self):\n",
    "#         # We list our `Metric` objects here so that `reset_states()` can be\n",
    "#         # called automatically at the start of each epoch\n",
    "#         # or at the start of `evaluate()`.\n",
    "#         # If you don't implement this property, you have to call\n",
    "#         # `reset_states()` yourself at the time of your choosing.\n",
    "#         return [loss_tracker, mae_metric]\n",
    "\n",
    "\n",
    "# Construct an instance of CustomModel\n",
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "\n",
    "winmin= 6\n",
    "stridesec = 5\n",
    "win_size = 15*winmin*60\n",
    "model_1 = acti_model(input_shape =(win_size,6))\n",
    "\n",
    "model = CustomModel(model_1.input, model_1.output)\n",
    "\n",
    "# We don't passs a loss or metrics here.\n",
    "model.compile(optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# Just use `fit` as usual -- you can use callbacks, etc.\n",
    "\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint(\"test.h5\", save_best_only=True, monitor='accuracy')\n",
    "model.fit(ds2.dataset, validation_data=None ,epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check dataset length of tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = TF_DataSet(meal_data_train,batch=32)\n",
    "ds2.dataset.cardinality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ds2.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winmin= 6\n",
    "stridesec = 5\n",
    "win_size = 15*winmin*60\n",
    "model_1 = acti_model(input_shape =(win_size,6))\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "H = model_1.fit(x=ds2.dataset.as_numpy_iterator(), y=None,\n",
    "                       validation_data=None,\n",
    "                    epochs = 2,  verbose=1,\n",
    "                    callbacks=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to train group model (Still Need to modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataset import create_train_test_file_list,  balance_data_indices  #Person_MealsDataset,\n",
    "from utils import *\n",
    "from model import *\n",
    "def train_group_models(model, win_ls = [],EPOCHS = 10,stridesec = 1,name = \"wenkanw\",model_name=\"v2\" ,random_seed= 1000, split_day=False,test_balanced=False):\n",
    "    from numpy.random import seed\n",
    "    seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "#     tf.set_random_seed(random_seed)\n",
    "    from datetime  import datetime\n",
    "    batch_size = 128\n",
    "    outfile = sys.stdout\n",
    "    perf = {\"model\":[],\"win(sec)\":[], \"wacc\":[],\"f1\":[],\"recall\":[],\"acc\":[]}\n",
    "    model_ls = []\n",
    "    hist_ls = []\n",
    "    for winsize in win_ls:\n",
    "        tf.random.set_seed(random_seed)\n",
    "        seed(random_seed)\n",
    "        \n",
    "        winmin = winsize\n",
    "        winlength = int(winmin * 60 * 15)\n",
    "        step = int(stridesec * 15)\n",
    "        start_time = datetime.now()\n",
    "        arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "              \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "              \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "        [os.system(cmd) for cmd in arr]\n",
    "        print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "        print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "        print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "\n",
    "\n",
    "        pathtemp = \"../models/\" + name+\"_models\" +\"/\"+model_name+\"_M_F_\"\n",
    "        #pathtemp = \"../models/\" + name +\"/\"+model_name+\"_M_F_\"\n",
    "        modelpath = pathtemp + \"{:f}Min.h5\".format(winmin)\n",
    "        jsonpath = pathtemp + \"{:f}Min.json\".format(winmin)\n",
    "        print(\"Model to Save: \",modelpath)\n",
    "        print()\n",
    "        \n",
    "        ########### Load the dataset################\n",
    "        person = name\n",
    "        if split_day:\n",
    "            pathtemp = \"../models/\" + name+\"_models\" +\"/\"+model_name+\"_split_day_M_F_\"\n",
    "            #pathtemp = \"../models/\" + name +\"/\"+model_name+\"_M_F_\"\n",
    "            modelpath = pathtemp + \"{:f}Min.h5\".format(winmin)\n",
    "            jsonpath = pathtemp + \"{:f}Min.json\".format(winmin)\n",
    "            create_train_test_file_list(file_name= \"all_files_list.txt\",person_name =name,\n",
    "                         out_path = \"../data-file-indices/\",root_path= \"../\",\n",
    "                         test_ratio = 0.2, print_flag = True, shuffle=True, random_state=random_seed)\n",
    "\n",
    "            meal_data_train = Person_MealsDataset(person_name= person, file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "            meal_data_test = Person_MealsDataset(person_name= person, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "\n",
    "            train_indices, valid_indices = split_train_test_indices(X= [i for i in range(len(meal_data_train.labels))],\n",
    "                                                                    y = meal_data_train.labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            #balanced train set\n",
    "            trainset_labels = meal_data_train.labels[train_indices]\n",
    "            train_indices = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "\n",
    "            # balance test set\n",
    "            testset_labels = meal_data_test.labels\n",
    "            if test_balanced:\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices=[i for i in range(len(meal_data_test))] ,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                # without balancing data\n",
    "                test_indices = [i for i in range(len(meal_data_test))] \n",
    "                \n",
    "            # get numpy dataset\n",
    "            balancedData, balancedLabels = meal_data_train.get_subset(train_indices)\n",
    "            valid_balancedData, valid_balancedLabels = meal_data_train.get_subset(valid_indices)\n",
    "            test_Data, test_Labels = meal_data_test.get_subset(test_indices)\n",
    "\n",
    "        else:\n",
    "        \n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "            samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "            # split train set and test set\n",
    "            train_indices, test_indices = split_train_test_indices(X= [i for i in range(len(labels))],\n",
    "                                                                    y = labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            # balance train set\n",
    "            trainset_labels = labels[train_indices]\n",
    "            train_indices_balanced = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            \n",
    "            \n",
    "            testset_labels = labels[test_indices]\n",
    "            if test_balanced:\n",
    "                #balance test set\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices= test_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                test_indices = test_indices \n",
    "            \n",
    "            \n",
    "            \n",
    "            print(\"Data Loader Created\")            \n",
    "            \n",
    "            # split validation set\n",
    "            balanced_trainset_labels = labels[train_indices_balanced]\n",
    "            train_indices_balanced, valid_indices = split_train_test_indices(X= train_indices_balanced,\n",
    "                                                                    y = balanced_trainset_labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            \n",
    "            train_set_balanced = torch.utils.data.Subset(meal_data, train_indices_balanced)\n",
    "            test_set = torch.utils.data.Subset(meal_data, test_indices)\n",
    "            valid_set_balanced = torch.utils.data.Subset(meal_data, valid_indices)\n",
    "            \n",
    "            train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "            test_loader = torch.utils.data.DataLoader(test_set ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "            valid_loader = torch.utils.data.DataLoader(valid_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "            # Get numpy dataset: balanced trainset, validation set, test set\n",
    "            #balancedData, balancedLabels = meal_data.get_subset(train_indices)\n",
    "            #valid_balancedData, valid_balancedLabels = meal_data.get_subset(valid_indices)\n",
    "            #test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "            \n",
    "        \n",
    "\n",
    "        #training settings\n",
    "        mcp_save = tf.keras.callbacks.ModelCheckpoint(modelpath, save_best_only=True, monitor='accuracy')\n",
    "        \n",
    "\n",
    "        scheduler = tf.keras.callbacks.ReduceLROnPlateau( monitor='val_loss', factor=0.1, patience=3, verbose=0,\n",
    "                                             mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.)\n",
    "        \n",
    "        ##########train model ###############\n",
    "        H = model.fit(x=balancedData, y = balancedLabels,\n",
    "                       validation_data=(valid_balancedData, valid_balancedLabels),\n",
    "                    epochs = EPOCHS, batch_size=batch_size, verbose=1,\n",
    "                    callbacks=[mcp_save,scheduler]) # removed addons.LossHistory(jsonpath) for compatibility with TensorFlow 2.2.0, needs to be re-added at some point\n",
    "\n",
    "        print(\"Max value: \", max(H.history['accuracy']), \" at epoch\", np.argmax(H.history['accuracy']) + 1)\n",
    "\n",
    "        \n",
    "        # Testing \n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, balanced_accuracy_score, f1_score\n",
    "        predictions = model.predict(x=test_Data)\n",
    "        threshold = 0.5\n",
    "        wacc =  balanced_accuracy_score(test_Labels,predictions>=threshold)\n",
    "        f1 =  f1_score(test_Labels,predictions>=threshold)\n",
    "        acc =  accuracy_score(test_Labels,predictions>=threshold)\n",
    "        recall = recall_score(test_Labels,predictions>=threshold)\n",
    "        \n",
    "        #auc = roc_auc_score(test_Labels,predictions>=threshold)\n",
    "        print(\"Weighted Accuracy:\", wacc)\n",
    "        print(\"Test Accuracy:\", acc)\n",
    "        print(\"F1-score:\", f1)\n",
    "        print(\"Recall Accuracy:\", recall)\n",
    "        #print(\"AUC Score:\", auc)\n",
    "\n",
    "        perf[\"model\"].append(\"ActiModel\")\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        perf[\"wacc\"].append(wacc)\n",
    "        perf[\"f1\"].append(f1)\n",
    "        perf[\"acc\"].append(acc)\n",
    "        perf[\"recall\"].append(recall)\n",
    "        #perf[\"auc\"].append(auc)\n",
    "        model_ls.append(model)\n",
    "        hist_ls.append(H)\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    print(perf_df)\n",
    "    return perf_df, model_ls, hist_ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv_v2",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
