{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Data Transformation and Preprocessing\n",
    "### settings: CPU 4,  memory 69G, chunk: 2, GPU model: P100, number of GPU:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.py\n",
    "\n",
    "\n",
    "\n",
    "from data_loader import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def create_train_test_file_list(file_name= \"all_files_list.txt\",person_name = 'wenkanw',\n",
    "                     out_path = \"../data-file-indices/\",root_path= \"../\",\n",
    "                     test_ratio = 0.2, print_flag = True, shuffle=False, random_state=None):\n",
    "    \"\"\"\n",
    "    This function is used to split test set and training set based on file names\n",
    "    \n",
    "    \"\"\"\n",
    "    shm_file_ls = []\n",
    "    event_file_ls = []\n",
    "    new_files = []\n",
    "    if person_name == \"CAD\":\n",
    "        fp = open(out_path+ person_name+ \"/\" +\"batch-unix.txt\", \"r\")\n",
    "        txt = fp.read()\n",
    "        fp.close()\n",
    "        # save all file list\n",
    "        fp = open(out_path+ person_name+ \"/\" +\"all_files_list.txt\", \"w\")\n",
    "        fp.write(txt)\n",
    "        fp.close()\n",
    "        \n",
    "        txt_ls = txt.split(\"\\n\")\n",
    "        txt_ls.remove(\"\")\n",
    "        txt_ls= [txt+\"\\n\" for txt in txt_ls]\n",
    "        test_size = int(len(txt_ls)*test_ratio)\n",
    "        test = \"\".join(txt_ls[len(txt_ls) - test_size: ])\n",
    "        train = \"\".join(txt_ls[:len(txt_ls) - test_size ])\n",
    "        \n",
    "        fp = open(out_path+ person_name+ \"/\" +\"test_files.txt\", \"w\")\n",
    "        fp.write(test)\n",
    "        fp.close()\n",
    "        \n",
    "        fp = open(out_path+ person_name+ \"/\" +\"train_files.txt\", \"w\")\n",
    "        fp.write(train)\n",
    "        fp.close()\n",
    "        \n",
    "        if print_flag:\n",
    "            print(\"Train:\", len(txt_ls) - test_size)\n",
    "            print(train)\n",
    "            print(\"test: \",test_size)\n",
    "            print(test)\n",
    "        return \n",
    "        \n",
    "        \n",
    "    for dirname, _, filenames in os.walk(root_path + 'data/IndividualData'):\n",
    "        for filename in filenames:\n",
    "            # check every file name in the individual data folder\n",
    "            path = os.path.join(dirname, filename)\n",
    "#             print(\"Path: \",path)\n",
    "            # check if datafile is shm file and is not a test file\n",
    "            if \".shm\" in filename and person_name in path and 'test' not in path:\n",
    "                # If the data file has label file as well, then it is valid\n",
    "                # and we add it to the filename list\n",
    "                event_file_name =  filename.replace(\".shm\",\"-events.txt\")\n",
    "                \n",
    "                if event_file_name in filenames:\n",
    "                    # if both shm and event files exist\n",
    "                    new_file = path.replace(root_path+\"data/\",\"\")\n",
    "                    new_file += \"\\n\"\n",
    "                    new_files.append(new_file)\n",
    "    if shuffle:\n",
    "        random.seed(random_state)\n",
    "        random.shuffle(new_files)\n",
    "        pass\n",
    "    else:\n",
    "        new_files.sort()\n",
    "        \n",
    "    if test_ratio > 0.:\n",
    "        # split train files and test files\n",
    "        test_size = int(len(new_files)*test_ratio)\n",
    "        test_files = new_files[:test_size]\n",
    "        train_files = new_files[test_size:]\n",
    "        # write train files\n",
    "        fp = open(out_path+ person_name+ \"/\" +\"train_files.txt\", \"w\")\n",
    "        train = \"\".join(train_files)\n",
    "        \n",
    "        fp.write(train)\n",
    "        fp.close()\n",
    "        # write test files\n",
    "        fp = open(out_path+ person_name+ \"/\" +\"test_files.txt\", \"w\")\n",
    "        test = \"\".join(test_files)\n",
    "        fp.write(test)\n",
    "        fp.close()\n",
    "        \n",
    "        if print_flag:\n",
    "            print(\"Train:\")\n",
    "            print(train)\n",
    "            print(\"test: \")\n",
    "            print(test)\n",
    "    \n",
    "    fp = open(out_path+person_name+ \"/\"+file_name, \"w\")\n",
    "    all_files = \"\".join(new_files)\n",
    "    fp.write(all_files)\n",
    "    fp.close()\n",
    "    \n",
    "    if print_flag:\n",
    "        print(\"All files: \")\n",
    "        print(all_files)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Person_MealsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset = None,person_name= \"wenkanw\", \n",
    "                 data_indices_file = \"../data-file-indices/\",\n",
    "                 file_name = \"all_files_list\",\n",
    "                 remove_trend = 0,\n",
    "                 remove_walk = 0,\n",
    "                 remove_rest = 0,\n",
    "                 smooth_flag = 1,\n",
    "                 normalize_flag = 1,\n",
    "                 winmin = 6,\n",
    "                 stridesec = 15,\n",
    "                 gtperc = 0.5,\n",
    "                 device = 'cpu',\n",
    "                 ratio_dataset=1,\n",
    "                load_splitted_dataset = False,\n",
    "                 enable_time_feat = False,\n",
    "                 debug_flag= False,\n",
    "                 get_numpy_data= False\n",
    "                ):\n",
    "        \n",
    "        if file_name == \"train\":\n",
    "            file_name = data_indices_file + person_name +\"/\"+\"train_files.txt\"\n",
    "        elif file_name == \"test\":\n",
    "            file_name = data_indices_file + person_name +\"/\"+\"test_files.txt\"\n",
    "        else:\n",
    "            file_name = data_indices_file + person_name +\"/\"+ file_name+\".txt\"\n",
    "            \n",
    "        # Note: file_name is the name of file that contain the list of shm files' names\n",
    "        self.file_name = file_name\n",
    "        self.get_numpy_data = get_numpy_data\n",
    "        self.dataset = dataset\n",
    "        self.person_name = person_name\n",
    "        self.winmin = winmin\n",
    "        self.stridesec = stridesec\n",
    "        self.load_splitted_dataset = load_splitted_dataset\n",
    "        self.remove_trend = remove_trend\n",
    "        self.remove_walk = remove_walk\n",
    "        self.remove_rest = remove_rest\n",
    "        self.smooth_flag = smooth_flag\n",
    "        self.normalize_flag = normalize_flag\n",
    "        self.gtperc = gtperc,\n",
    "        self.ratio_dataset = ratio_dataset\n",
    "        self.enable_time_feat = enable_time_feat\n",
    "        self.device = device\n",
    "        self.debug_flag= debug_flag\n",
    "        if not self.dataset:\n",
    "            self.get_data(person_name)\n",
    "\n",
    "    def get_data(self, person_name):\n",
    "            \n",
    "            \n",
    "            # files_counts, data, samples_indices, labels_array\n",
    "            # Note: the data preprocessing in this function is for global time series dataset\n",
    "            \n",
    "            self.dataset, self.data, self.data_indices, self.labels = load_train_test_data(data_file_list =self.file_name,\n",
    "                                    load_splitted_dataset = False,\n",
    "                                     ratio_dataset=self.ratio_dataset,\n",
    "                                     enabled_time_feat = self.enable_time_feat, \n",
    "                                     winmin = self.winmin, stridesec = self.stridesec,gtperc = self.gtperc,\n",
    "                                     removerest = self.remove_rest,\n",
    "                                     removewalk = self.remove_walk, smooth_flag = self.smooth_flag, normalize_flag=self.normalize_flag, \n",
    "                                     remove_trend = self.remove_trend,\n",
    "                                     debug_flag=self.debug_flag )\n",
    "            \n",
    "            if self.load_splitted_dataset:\n",
    "                self.dataset = self.get_dataset()\n",
    "                \n",
    "            \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        #这里需要注意的是，第一步：read one data，是一个data\n",
    "        data = self.get_item(index)\n",
    "        if self.get_numpy_data:\n",
    "            return data['data'].numpy() ,data['label']\n",
    "        return data['data'],data['label']\n",
    "        \n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return  len(self.dataset) if self.load_splitted_dataset else len(self.data_indices)\n",
    "    def get_item(self, index, tensor_type=True):\n",
    "        \"\"\"\n",
    "        This function is used to obtain one sample data point\n",
    "        \"\"\"\n",
    "        f,start_time, end_time = self.data_indices[index,0], self.data_indices[index,1], self.data_indices[index,2]\n",
    "        sample = self.data[f][start_time : end_time]\n",
    "        data = pd.DataFrame(columns=['data','label'])    \n",
    "        # Add time feature to data\n",
    "        if self.enable_time_feat:\n",
    "            time_offset = self.data_indices[index,3]\n",
    "            freq = 1.0/15.0\n",
    "            time_feat = np.array([[i for i in range(len(sample))]],dtype=float).transpose()\n",
    "            time_feat *= freq\n",
    "            time_feat += float(start_time)* freq\n",
    "            time_feat += time_offset\n",
    "            sample = np.concatenate((sample, time_feat),axis=1)\n",
    "        label = self.labels[index]\n",
    "        if tensor_type:\n",
    "            data = {\"data\":torch.tensor(sample, dtype =torch.float, device =  self.device ), 'label': label}\n",
    "        else:\n",
    "            data = {\"data\":sample, 'label': label}\n",
    "        return data\n",
    "    \n",
    "    def get_dataset(self, start_index = None, end_index = None):\n",
    "        \"\"\"\n",
    "        This function is used to obtain the whole dataset in pandas or part of whole dataset\n",
    "        It is good to use this to sample some data to analyze\n",
    "        \"\"\"\n",
    "        start_i = 0 if not start_index else start_index\n",
    "        end_i = self.__len__() if not end_index else end_index\n",
    "        \n",
    "        dataset = pd.DataFrame(columns=['data','label'])\n",
    "        for i in tqdm(range(start_i, end_i)):\n",
    "            data = self.get_item(i)\n",
    "            dataset = dataset.append(data,ignore_index=True)\n",
    "        self.dataset = dataset\n",
    "        return self.dataset\n",
    "    \n",
    "    def sample(self, num = 1000,random_state = None):\n",
    "        \"\"\"\n",
    "        Simply sample part of data for analysis\n",
    "        \"\"\"\n",
    "        if random_state != None:\n",
    "            np.random.seed(random_state)\n",
    "            \n",
    "        sample_data = pd.DataFrame(columns=['data','label'])\n",
    "        indices = np.random.choice(len(self.labels), num)\n",
    "        for i in tqdm(indices):\n",
    "            data = self.get_item(i)\n",
    "            data[\"data\"] = data[\"data\"].numpy()\n",
    "            sample_data = sample_data.append(data,ignore_index=True)\n",
    "        return sample_data\n",
    "    \n",
    "    def get_subset(self, indices_ls):\n",
    "        axdata = []\n",
    "        aydata = []\n",
    "        for i in indices_ls:\n",
    "            data = self.get_item(i, tensor_type=False)\n",
    "            sample = data['data']\n",
    "            label = data['label']\n",
    "            axdata.append(sample)\n",
    "            aydata.append(label)\n",
    "        subsetData = np.array(axdata, copy=True) # Undersampled Balanced Training Set\n",
    "        subsetLabels = np.array(aydata, copy=True)\n",
    "        del axdata\n",
    "        del aydata\n",
    "        return subsetData, subsetLabels\n",
    "    \n",
    "    def get_GT_segment(self,root_path = \"../data/\",print_file=False):\n",
    "        file_ls = []\n",
    "        fp = open(self.file_name,\"r\")\n",
    "        txt = fp.read()\n",
    "        fp.close()\n",
    "        file_ls = txt.split(\"\\n\")\n",
    "        while '' in file_ls:\n",
    "            file_ls.remove('')\n",
    "        \n",
    "        start_ls = []\n",
    "        end_ls = []\n",
    "        total_events =[]\n",
    "        for file_name in file_ls:\n",
    "            file_name = root_path + file_name\n",
    "            TotalEvents, EventStart, EventEnd, EventNames, TimeOffset,EndTime = loadEvents(file_name, debug_flag = False, print_file=print_file)\n",
    "            start_ls.append(EventStart[:TotalEvents])\n",
    "            end_ls.append(EventEnd[:TotalEvents])\n",
    "            \n",
    "        return  start_ls, end_ls\n",
    "        \n",
    "        \n",
    "    def get_mealdataset_info(self,person_name = None,file_ls = [], file_ls_doc=None,root_path = \"../data/\",print_file=False):\n",
    "        \"\"\"\n",
    "        if file_ls is not given, then get file_ls according to person_name\n",
    "        file path = root_path + file name in all_files_list.txt\n",
    "\n",
    "        return:\n",
    "            meal event count, total minutes of all meals, total hours of all meals,total day counts\n",
    "\n",
    "        \"\"\"\n",
    "        if person_name ==None:\n",
    "            person_name = self.person_name\n",
    "        if len(file_ls) ==0:\n",
    "            if file_ls_doc != None:\n",
    "                data_indices_file = \"../data-file-indices/\" +person_name+\"/\"+ file_ls_doc\n",
    "                fp = open(data_indices_file,\"r\")\n",
    "            else:\n",
    "                fp = open(self.file_name,\"r\")\n",
    "            txt = fp.read()\n",
    "            fp.close()\n",
    "            file_ls = txt.split(\"\\n\")\n",
    "            while '' in file_ls:\n",
    "                file_ls.remove('')\n",
    "\n",
    "        meal_counts = 0\n",
    "        sec_counts = 0\n",
    "        min_counts = 0\n",
    "        hour_counts = 0\n",
    "        total_hours = 0\n",
    "        total_mins = 0\n",
    "        total_sec = 0\n",
    "        day_counts = len(file_ls)\n",
    "        for file_name in file_ls:\n",
    "            file_name = root_path + file_name\n",
    "            TotalEvents, EventStart, EventEnd, EventNames, TimeOffset,EndTime = loadEvents(file_name, debug_flag = False, print_file=print_file)\n",
    "            meal_counts += TotalEvents\n",
    "            total_sec +=  abs(EndTime - TimeOffset)\n",
    "#             total_hours += (EndTime//(60*60) - TimeOffset//(60*60))\n",
    "#             total_mins  += (EndTime%(60*60) - TimeOffset//(60*60))\n",
    "            for i in range(len(EventStart)):\n",
    "                sec_counts += ( EventEnd[i]- EventStart[i])//(15)\n",
    "        total_hours = total_sec//(60*60)\n",
    "        min_counts = sec_counts//60\n",
    "        hour_counts = min_counts//60\n",
    "        \n",
    "        return meal_counts, min_counts,hour_counts, day_counts, total_hours\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "                \n",
    "def balance_data_indices(labels, data_indices=None,sample_num = 4000,mode= \"under\", replace = False,shuffle=True, random_state = 1000):\n",
    "    \"\"\"\n",
    "    sample_num: number of samples of each class after balancing\n",
    "    mode: \n",
    "        under - undersampling\n",
    "        over - oversampling\n",
    "        mix - undersampling negative samples + oversampling positive samples, each class has sample_num amount samples in this mode\n",
    "    return:\n",
    "        balanced indices\n",
    "    \"\"\"\n",
    "    if data_indices:\n",
    "        eat_labels_index = [data_indices[i] for i, e in enumerate(labels) if e >= 0.5]\n",
    "        not_eat_labels_index = [data_indices[i] for i, e in enumerate(labels) if e < 0.5]\n",
    "    else:\n",
    "        eat_labels_index = [i for i, e in enumerate(labels) if e >= 0.5]\n",
    "        not_eat_labels_index = [i for i, e in enumerate(labels) if e < 0.5]\n",
    "        \n",
    "    eat_index = eat_labels_index\n",
    "    not_eat_index = not_eat_labels_index\n",
    "    if random_state != None:\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    if mode == \"over\":\n",
    "        eat_index = np.random.choice(eat_labels_index,len(not_eat_labels_index)).tolist()\n",
    "        pass\n",
    "    elif mode == \"under\":\n",
    "        not_eat_index = np.random.choice(not_eat_labels_index,len(eat_labels_index),replace = replace).tolist()\n",
    "        pass\n",
    "    else:\n",
    "        #default as mix\n",
    "        eat_index = np.random.choice(eat_labels_index,sample_num, replace = replace).tolist()\n",
    "        not_eat_index = np.random.choice(not_eat_labels_index,sample_num, replace = replace).tolist()\n",
    "        pass\n",
    "    \n",
    "    indices_balanced = eat_index + not_eat_index\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices_balanced)\n",
    "    \n",
    "    return indices_balanced\n",
    "\n",
    "def create_datasets(names=[], winmin = 6,stridesec = 5,smooth_flag = 1,normalize_flag = 1):\n",
    "    \"\"\"\n",
    "    generate a dictionary of datasets\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    for person in names:\n",
    "        meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec,smooth_flag = smooth_flag,\n",
    "                     normalize_flag = normalize_flag)\n",
    "        datasets[person]  = meal_data\n",
    "    return datasets\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from data_loader import loadEvents\n",
    "def get_meal_info(person_name = None,file_ls = [], file_ls_doc=None,root_path = \"../data/\",print_file=False,round_decimal=1):\n",
    "        \"\"\"\n",
    "        if file_ls is not given, then get file_ls according to person_name\n",
    "        file path = root_path + file name in all_files_list.txt\n",
    "\n",
    "        return:\n",
    "            meal event count, total minutes of all meals, total hours of all meals,total day counts\n",
    "\n",
    "        \"\"\"\n",
    "        meal_counts = 0\n",
    "        sec_counts = 0\n",
    "        min_counts = 0\n",
    "        hour_counts = 0\n",
    "        total_hours = 0\n",
    "        total_mins = 0\n",
    "        total_sec = 0\n",
    "        day_counts = 0 \n",
    "        if person_name ==None:\n",
    "            return meal_counts, min_counts,hour_counts, day_counts, total_hours\n",
    "\n",
    "        data_indices_file = \"../data-file-indices/\" +person_name+\"/all_files_list.txt\"\n",
    "        fp = open(data_indices_file,\"r\")\n",
    "        txt = fp.read()\n",
    "        fp.close()\n",
    "        file_ls = txt.split(\"\\n\")\n",
    "        while '' in file_ls:\n",
    "            file_ls.remove('')\n",
    "       \n",
    "        day_counts = len(file_ls)\n",
    "        \n",
    "        for file_name in file_ls:\n",
    "            file_name = root_path + file_name\n",
    "            TotalEvents, EventStart, EventEnd, EventNames, TimeOffset,EndTime = loadEvents(file_name, debug_flag = False, print_file=print_file)\n",
    "            meal_counts += TotalEvents\n",
    "            total_sec +=  abs(EndTime - TimeOffset)\n",
    "            for i in range(len(EventStart)):\n",
    "                sec_counts += ( EventEnd[i]- EventStart[i])//15\n",
    "                \n",
    "        total_hours = total_sec/(60*60)\n",
    "        min_counts = sec_counts/60\n",
    "        hour_counts = min_counts/60\n",
    "        average_meal_per_day = meal_counts/len(file_ls)\n",
    "        average_hour_per_meal = hour_counts/meal_counts\n",
    "        # round numbers\n",
    "        total_hours = round(total_hours, round_decimal)\n",
    "        min_counts = round(min_counts, round_decimal)\n",
    "        hour_counts = round(hour_counts, round_decimal)\n",
    "        average_meal_per_day = round(average_meal_per_day,round_decimal)\n",
    "        average_hour_per_meal = round(average_hour_per_meal, round_decimal)\n",
    "        \n",
    "        no_eating_hours = total_hours - hour_counts\n",
    "        weight_ratio = round(no_eating_hours/hour_counts, round_decimal)\n",
    "        result = pd.DataFrame({\"dataset\": person_name,\"Days\":day_counts, \n",
    "                      \"Total Hours\":total_hours,\"Meal Counts\":meal_counts,\n",
    "                      \"Average Meal Counts Per Day\":average_meal_per_day,\"Average Hours Per Meal\": average_hour_per_meal,\n",
    "                      \"Eating Hours\":hour_counts, \"No Eating Hours\":no_eating_hours,\n",
    "                     \"Balance Ratio(no_eat/eat)\":weight_ratio},index=[0])\n",
    "    \n",
    "        return result\n",
    "\n",
    "          \n",
    "def get_dataset_info(names= [\"wenkanw\"],winmin=6,stridesec=5):\n",
    "    meal_info = defaultdict(list)\n",
    "    dataset_results = pd.DataFrame()\n",
    "    for name in names:\n",
    "        result = get_meal_info(person_name=name)\n",
    "        if dataset_results.empty:\n",
    "            dataset_results = result\n",
    "        else:\n",
    "            dataset_results = dataset_results.append(result,ignore_index=True)\n",
    "    \n",
    "    # append total summary\n",
    "#     print( dataset_results)\n",
    "    total_result=pd.DataFrame({\"dataset\":\"total\"},columns = dataset_results.columns,index=[0])\n",
    "    # append average summary\n",
    "    average_result=pd.DataFrame({\"dataset\":\"average\"},columns = dataset_results.columns,index=[0])\n",
    "    key_ls = [\"Days\",\"Total Hours\",\"Meal Counts\",\"Eating Hours\",\"No Eating Hours\"]\n",
    "    for key in dataset_results.columns:\n",
    "        if key in key_ls:\n",
    "            total_result[key].at[0] = round(dataset_results[key].sum() ,1)\n",
    "            average_result[key].at[0] = round(dataset_results[key].mean(),1)\n",
    "\n",
    "    ls = [total_result, average_result]\n",
    "    for df in ls:\n",
    "        df[\"Average Meal Counts Per Day\"].at[0] = round(df[\"Meal Counts\"].values[0]/df[\"Days\"].values[0], 1)\n",
    "        df[\"Average Hours Per Meal\"].at[0] =round( df[\"Eating Hours\"].values[0]/df[\"Meal Counts\"].values[0], 1)\n",
    "        df[\"Balance Ratio(no_eat/eat)\"].at[0] =round(df[\"No Eating Hours\"].values[0]/df[\"Eating Hours\"].values[0],1)\n",
    "        dataset_results =dataset_results.append(df,ignore_index=True)\n",
    "\n",
    "    return dataset_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report,recall_score, precision_score\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report,recall_score, precision_score\n",
    "\n",
    "\n",
    "def data_parallel(module, input, device_ids, output_device=None):\n",
    "    if not device_ids:\n",
    "        return module(input)\n",
    "\n",
    "    if output_device is None:\n",
    "        output_device = device_ids[0]\n",
    "\n",
    "    replicas = nn.parallel.replicate(module, device_ids)\n",
    "    inputs = nn.parallel.scatter(input, device_ids)\n",
    "    replicas = replicas[:len(inputs)]\n",
    "    outputs = nn.parallel.parallel_apply(replicas, inputs)\n",
    "    return nn.parallel.gather(outputs, output_device)\n",
    "\n",
    "\n",
    "def eval_model(model,dataloader,device=\"cpu\"):\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    # without update\n",
    "    with torch.no_grad():\n",
    "        for samples, labels in dataloader:\n",
    "            samples = samples.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(samples).squeeze()\n",
    "            #print(\"Output: \", outputs)\n",
    "            outputs = torch.round(torch.sigmoid(outputs))\n",
    "            preds = outputs>=0.5\n",
    "            preds = preds.to(dtype = torch.float)\n",
    "            preds.requires_grad = False\n",
    "#             _,preds = torch.max(outputs,1)\n",
    "            for i in range(len(preds)):\n",
    "                if preds[i] == 1 and labels[i] == 1:\n",
    "                    TP += 1\n",
    "                if preds[i] == 0 and labels[i] == 1:\n",
    "                    FN += 1\n",
    "            correct += torch.sum((preds == labels)).item()\n",
    "            total += float(len(labels))\n",
    "        acc =100 * correct/ total\n",
    "        recall = TP/(TP+FN)\n",
    "#         print(\"Evaluation Acc: %.4f %%,  Recall: %.4f \"%(acc , recall))\n",
    "    return acc, recall\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "def train_model(model,dataloader, optimizer, criterion,lrscheduler,device=\"cpu\" , n_epochs=20,\n",
    "                earlystopping=True, patience= 5, l1_enabled=True,checkpoint_name =\"checkpoint.pt\" ):\n",
    "    loss_ls = [0.0]\n",
    "    train_acc_ls = [0.0]\n",
    "    valid_acc_ls = [0.0]\n",
    "    valid_acc = 0.0\n",
    "    loss =0.0\n",
    "    train_acc = 0.0\n",
    "    patience_count = 0\n",
    "    best_val_score = 0.0\n",
    "    prev_val_score = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    train_dataloader, valid_dataloader = dataloader\n",
    "    print(\"Training set batch amounts:\", len(train_dataloader))\n",
    "    print(\"Test set :\", len(valid_dataloader))\n",
    "    print(\"Start Training..\")\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        correct_cnt = 0.0\n",
    "        total_cnt = 0.0\n",
    "        TP = 0.\n",
    "        FN = 0.\n",
    "        model.train()\n",
    "        for i, (samples, labels) in enumerate(train_dataloader):\n",
    "            samples = samples.to(device)\n",
    "            labels = labels.to(device, dtype=torch.float32)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # reshape samples\n",
    "            outputs = model(samples).squeeze()\n",
    "\n",
    "            #print(\"Output: \", outputs, \"label: \", labels)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            if l1_enabled:\n",
    "                L1_loss = model.l1_loss(0.01).to(device)\n",
    "                loss += L1_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # prediction\n",
    "            #_,preds = torch.max(outputs,1)\n",
    "            outputs = torch.round(torch.sigmoid(outputs))\n",
    "            preds = outputs>=0.5\n",
    "            preds = preds.to(dtype = torch.float)\n",
    "            preds.requires_grad = False\n",
    "            \n",
    "            # Compute count of TP, FN\n",
    "            for j in range(len(preds)):\n",
    "                if preds[j] == 1. and labels[j] == 1.:\n",
    "                    TP += 1\n",
    "                if preds[j] == 0. and labels[j] == 1.:\n",
    "                    FN += 1\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            correct_cnt += torch.sum((preds == labels)).item()\n",
    "            total_cnt += float(len(labels))\n",
    "            batch_acc = 100. * (preds == labels).sum().item()/ float(len(labels))\n",
    "            if i %50 ==0:\n",
    "                #print(\"===> Batch: %d,  Batch_Loss: %.4f, Train Acc: %.4f %%,  Recall: %.f\\n\"%(i, loss,batch_acc, recall))\n",
    "                pass\n",
    "\n",
    "            \n",
    "        \n",
    "        # Compute accuracy and loss of one epoch\n",
    "        epoch_loss = running_loss / len(train_dataloader)  \n",
    "        epoch_acc = 100* correct_cnt/ total_cnt  # in percentage\n",
    "        correct_cnt = 0.0\n",
    "        total_cnt = 0.0\n",
    "        train_recall = TP/(TP+FN)\n",
    "        \n",
    "        #Validation mode\n",
    "        model.eval()\n",
    "        valid_acc, valid_recall= eval_model(model,valid_dataloader,device=device)\n",
    "        \n",
    "        # record loss and accuracy\n",
    "        valid_acc_ls.append(valid_acc)  \n",
    "        train_acc_ls.append(epoch_acc)\n",
    "        loss_ls.append(epoch_loss)\n",
    "        \n",
    "        if e %1==0:\n",
    "            print(\"Epoch: %d,  Epoch_Loss: %.4f, Train Acc: %.4f %%, Train Recall: %.4f, Validation Acc:  %.4f %%,  Validation Recall: %.4f  \"%(e, epoch_loss,\n",
    "                                                                                     epoch_acc,train_recall,valid_acc, valid_recall))\n",
    "        \n",
    "        # Reset train mode\n",
    "        model.train()\n",
    "        lrscheduler.step(valid_acc)\n",
    "        \n",
    "        \n",
    "        # If earlystopping is enabled, then save model if performance is improved\n",
    "        if earlystopping:\n",
    "            if prev_val_score !=0. and valid_acc < prev_val_score :\n",
    "                patience_count += 1\n",
    "            else:\n",
    "                patience_count = 0\n",
    "                \n",
    "            if patience_count >= patience:\n",
    "                break \n",
    "                \n",
    "            prev_val_score = valid_acc\n",
    "            if valid_acc > best_val_score or best_val_score == 0.0:\n",
    "                best_val_score = valid_acc\n",
    "                torch.save(model,checkpoint_name)\n",
    "                print(\"Checkpoint Saved\")\n",
    "            \n",
    "                \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "            \n",
    "    # Load best model\n",
    "    best_model = torch.load(checkpoint_name)\n",
    "    print(\"Load Best Model.\")\n",
    "    print(\"Training completed\")\n",
    "        \n",
    "    return model, best_model,best_val_score,loss_ls, train_acc_ls, valid_acc_ls\n",
    "            \n",
    "\n",
    "def plot_data(train_acc_ls,valid_acc_ls,loss_ls ):\n",
    "    \"\"\"\n",
    "    Plot validation accuracy, training accuracy and loss\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "    epochs = [i for i in range(len(train_acc_ls))]\n",
    "    _ = sns.lineplot(x=epochs, y= train_acc_ls,ax=ax[0])\n",
    "    _ = sns.lineplot(x=epochs, y= valid_acc_ls,ax=ax[0])\n",
    "    ax[0].set_xlabel(\"Epoches\")\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "    ax[0].legend([\"Training Accuracy\", \"Validation Accuracy\"])\n",
    "    \n",
    "    _ = sns.lineplot(x=epochs[1:], y= loss_ls[1:],ax=ax[1])\n",
    "    ax[1].set_xlabel(\"Epoches\")\n",
    "    ax[1].set_ylabel(\"Training Loss\")\n",
    "    ax[1].set(yscale=\"log\")\n",
    "    plt.show()\n",
    "    \n",
    "def split_train_test_indices(X, y, test_size, random_seed = None):\n",
    "    \"\"\"\n",
    "    This function is to split the training set indices into validation set indices and training set indices\n",
    "    \n",
    "    X: indices of dataset/ subset of dataset\n",
    "    y: labels of dataset / subset of dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, train_test_split\n",
    "\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    \n",
    "    if test_size ==0:\n",
    "        train_indices = X\n",
    "        y_train = y\n",
    "    elif test_size == 1:\n",
    "        test_indices = X\n",
    "        y_test = y\n",
    "    elif test_size >0 and test_size <1:\n",
    "        train_indices, test_indices, y_train, y_test = train_test_split(X,y,\n",
    "                                                            stratify=y, \n",
    "                                                            test_size=test_size,random_state = random_seed)\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid split ratio: %.3f\"%(test_size))\n",
    "    if len(train_indices)>0:\n",
    "        print(\"Train set size: %d, with %d positive samples and %d negative samples\"%(len(y_train),sum(y_train==1),\n",
    "                                                                          sum(y_train==0)))\n",
    "    if len(test_indices)>0:\n",
    "        print(\"Test set size: %d, with %d positive samples and %d negative samples\"%(len(y_test),\n",
    "                                                                          sum(y_test==1),\n",
    "                                                                           sum(y_test==0)))\n",
    "    \n",
    "    return train_indices, test_indices\n",
    "            \n",
    "\n",
    "    \n",
    "def print_settings(winmin,stridesec, EPOCHS):\n",
    "    \"\"\"\n",
    "    This is just a function to print information of training settings\n",
    "    \"\"\"\n",
    "    outfile = sys.stdout\n",
    "\n",
    "    winlength = int(winmin * 60 * 15)\n",
    "    step = int(stridesec * 15)\n",
    "    start_time = datetime.datetime.now()\n",
    "    arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "          \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "          \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "\n",
    "    [os.system(cmd) for cmd in arr]\n",
    "    print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "    print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "    print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "    \n",
    "    \n",
    "\n",
    "def cross_validation(dataset, data_indices, model,n_epochs=30,k=5, device=\"cpu\", random_state = 1000, checkpoint_path = \"./\"  ):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "    best_val_score = 0\n",
    "    overall_best_model = None\n",
    "    best_fold = None\n",
    "    all_loss_ls = []\n",
    "    all_train_acc_ls = []\n",
    "    all_valid_acc_ls = []\n",
    "    data_indices = np.array(data_indices)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k)\n",
    "    \n",
    "    labels = dataset.labels[data_indices]\n",
    "    np.random.seed(random_state)\n",
    "    seeds = np.random.randint(low=0, high=1000,size=k)\n",
    "    \n",
    "    \n",
    "    for fold_ind, (train_fold, valid_fold) in enumerate(skf.split(data_indices, labels)):\n",
    "        torch.manual_seed(seeds[fold_ind])\n",
    "        \n",
    "        print(\"===========================> Running Fold: %d\"%(fold_ind))\n",
    "        print()\n",
    "        train_indices = data_indices[train_fold]\n",
    "        valid_indices = data_indices[valid_fold]\n",
    "        # Train set    \n",
    "        train_set_fold = torch.utils.data.Subset(dataset, train_indices)\n",
    "        train_loader_fold = torch.utils.data.DataLoader(train_set_fold,batch_size=32, shuffle=True)\n",
    "\n",
    "        # validation set\n",
    "        valid_set_fold = torch.utils.data.Subset(dataset, valid_indices)\n",
    "        valid_loader_fold = torch.utils.data.DataLoader(valid_set_fold,batch_size=32, shuffle=True)\n",
    "          \n",
    "        # Re-initialize models\n",
    "        cv_model = model\n",
    "        # Since I use a dynamic created layer in network, need to input a sample to initialize the model first\n",
    "        cv_model.apply(weights_init)\n",
    "        cv_model.to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        optimizer = optim.Adam(cv_model.parameters(),lr=0.01,  weight_decay=0.1)\n",
    "        lrscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',patience= 2, factor = 0.1,threshold=0.01)\n",
    "\n",
    "\n",
    "        dataloader = (train_loader_fold,valid_loader_fold )\n",
    "        cv_model, best_model,val_score,loss_ls, train_acc_ls, valid_acc_ls = train_model(cv_model,dataloader, optimizer, criterion, \n",
    "                                                                                      lrscheduler, device= device,\n",
    "                                                                            n_epochs=n_epochs, patience = 5, l1_enabled=False,\n",
    "                                                                            checkpoint_name =checkpoint_path+\"cross_valid_checkpoint_\"+str(fold_ind)+\".pt\")\n",
    "        best_model.eval()\n",
    "        valid_acc, recall = eval_model(best_model, valid_loader_fold,device)\n",
    "        \n",
    "        all_valid_acc_ls.append(valid_acc)\n",
    "        \n",
    "        print(\"Fold %d Completed\"%(fold_ind))\n",
    "    print(\"Cross Validation Completed，score is %.4f %%\"%( np.mean(all_valid_acc_ls)))\n",
    "    \n",
    "    return all_valid_acc_ls\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 1.)\n",
    "#         nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "#         nn.init.normal_(m.weight.data, 0.0, 0.01)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 1.)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "        \n",
    "        \n",
    "def test_model(model, winmin=3, stridesec = 15,names= [\"wenkanw\"],random_seed=1000, split_day=False):\n",
    "    \"\"\"\n",
    "    A function to test tensorflow model\n",
    "    \"\"\"\n",
    "    perf = {\"name\":[],\"model\":[],\"win(sec)\":[], \"acc\":[],\"recall\":[], \"auc\":[]}\n",
    "    for name in names:\n",
    "        person = name\n",
    "        if split_day:\n",
    "            meal_data_test = Person_MealsDataset(person_name= person, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "\n",
    "            # balance test set\n",
    "            testset_labels = meal_data_test.labels\n",
    "            test_indices = balance_data_indices(testset_labels,data_indices=[i for i in range(len(meal_data_test))] ,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            # get numpy dataset\n",
    "            test_Data, test_Labels = meal_data_test.get_subset(test_indices)\n",
    "        else:            \n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "            samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "            # split train set and test set\n",
    "            train_indices, test_indices = split_train_test_indices(X= [i for i in range(len(labels))],\n",
    "                                                                            y = labels, test_size = 0.2,\n",
    "                                                                           random_seed = random_seed)\n",
    "            testset_labels = labels[test_indices]\n",
    "            test_indices = balance_data_indices(testset_labels,data_indices= test_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "            \n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "        predictions = model.predict(x=test_Data)\n",
    "        threshold = 0.5\n",
    "        acc =  accuracy_score(predictions>=threshold,test_Labels)\n",
    "        recall = recall_score(predictions>=threshold,test_Labels)\n",
    "        auc = roc_auc_score(predictions>=threshold,test_Labels)\n",
    "        print(\"Test Accuracy:\", acc)\n",
    "        print(\"Recall Accuracy:\", recall)\n",
    "        print(\"AUC Score:\", auc)\n",
    "        perf[\"name\"].append(name)\n",
    "        perf[\"model\"].append(\"ActiModel\")\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        perf[\"acc\"].append(acc)\n",
    "        perf[\"recall\"].append(recall)\n",
    "        perf[\"auc\"].append(auc)\n",
    "\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    return perf_df\n",
    "\n",
    "def train_models_v2(model, win_ls = [],EPOCHS = 10,stridesec = 1,name = \"wenkanw\",model_name=\"v2\" ,random_seed= 1000, split_day=False):\n",
    "    \"\"\"\n",
    "    A function to train tensorflow models\n",
    "    \"\"\"\n",
    "    from numpy.random import seed\n",
    "    seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "#     tf.set_random_seed(random_seed)\n",
    "    from datetime  import datetime\n",
    "    batch_size = 128\n",
    "    outfile = sys.stdout\n",
    "    perf = {\"model\":[],\"win(sec)\":[], \"acc\":[],\"recall\":[], \"auc\":[]}\n",
    "    model_ls = []\n",
    "    hist_ls = []\n",
    "    for winsize in win_ls:\n",
    "        tf.random.set_seed(random_seed)\n",
    "        seed(random_seed)\n",
    "        \n",
    "        winmin = winsize\n",
    "        winlength = int(winmin * 60 * 15)\n",
    "        step = int(stridesec * 15)\n",
    "        start_time = datetime.now()\n",
    "        arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "              \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "              \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "        [os.system(cmd) for cmd in arr]\n",
    "        print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "        print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "        print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "\n",
    "\n",
    "        pathtemp = \"../models/\" + name+\"_models\" +\"/\"+model_name+\"_M_F_\"\n",
    "        modelpath = pathtemp + \"{:f}Min.h5\".format(winmin)\n",
    "        jsonpath = pathtemp + \"{:f}Min.json\".format(winmin)\n",
    "        print(\"Model to Save: \",modelpath)\n",
    "        print()\n",
    "        # Load the dataset\n",
    "        \n",
    "        person = name\n",
    "        if split_day:\n",
    "            create_train_test_file_list(file_name= \"all_files_list.txt\",person_name =name,\n",
    "                         out_path = \"../data-file-indices/\",root_path= \"../\",\n",
    "                         test_ratio = 0.2, print_flag = True, shuffle=True, random_state=random_seed)\n",
    "\n",
    "            meal_data_train = Person_MealsDataset(person_name= person, file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "            meal_data_test = Person_MealsDataset(person_name= person, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "\n",
    "            train_indices, valid_indices = split_train_test_indices(X= [i for i in range(len(meal_data_train.labels))],\n",
    "                                                                    y = meal_data_train.labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            #balanced train set\n",
    "            trainset_labels = meal_data_train.labels[train_indices]\n",
    "            train_indices = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "\n",
    "            # balance test set\n",
    "            testset_labels = meal_data_test.labels\n",
    "            test_indices = balance_data_indices(testset_labels,data_indices=[i for i in range(len(meal_data_test))] ,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            # get numpy dataset\n",
    "            balancedData, balancedLabels = meal_data_train.get_subset(train_indices)\n",
    "            valid_balancedData, valid_balancedLabels = meal_data_train.get_subset(valid_indices)\n",
    "            test_Data, test_Labels = meal_data_test.get_subset(test_indices)\n",
    "\n",
    "        else:\n",
    "        \n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "            samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "            # split train set and test set\n",
    "            train_indices, test_indices = split_train_test_indices(X= [i for i in range(len(labels))],\n",
    "                                                                    y = labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            # balance train set\n",
    "            trainset_labels = labels[train_indices]\n",
    "            train_indices_balanced = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            #balance test set\n",
    "            testset_labels = labels[test_indices]\n",
    "            test_indices = balance_data_indices(testset_labels,data_indices= test_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "\n",
    "            train_set_balanced = torch.utils.data.Subset(meal_data, train_indices_balanced)\n",
    "            test_set = torch.utils.data.Subset(meal_data, test_indices)\n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "            test_loader = torch.utils.data.DataLoader(test_set ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "            print(\"Data Loader Created\")            \n",
    "            \n",
    "            # split validation set\n",
    "            balanced_trainset_labels = labels[train_indices_balanced]\n",
    "            train_indices, valid_indices = split_train_test_indices(X= train_indices_balanced,\n",
    "                                                                    y = balanced_trainset_labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            valid_set_balanced = torch.utils.data.Subset(meal_data, valid_indices)\n",
    "            valid_loader = torch.utils.data.DataLoader(valid_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "            balancedData, balancedLabels = meal_data.get_subset(train_indices)\n",
    "            valid_balancedData, valid_balancedLabels = meal_data.get_subset(valid_indices)\n",
    "            test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "        \n",
    "\n",
    "        #training settings\n",
    "        mcp_save = tf.keras.callbacks.ModelCheckpoint(modelpath, save_best_only=True, monitor='accuracy')\n",
    "        \n",
    "\n",
    "        scheduler = tf.keras.callbacks.ReduceLROnPlateau( monitor='val_loss', factor=0.1, patience=3, verbose=0,\n",
    "                                             mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.)\n",
    "        #train model\n",
    "        H = model.fit(x=balancedData, y = balancedLabels,\n",
    "                       validation_data=(valid_balancedData, valid_balancedLabels),\n",
    "                    epochs = EPOCHS, batch_size=batch_size, verbose=1,\n",
    "                    callbacks=[mcp_save,scheduler]) # removed addons.LossHistory(jsonpath) for compatibility with TensorFlow 2.2.0, needs to be re-added at some point\n",
    "\n",
    "        print(\"Max value: \", max(H.history['accuracy']), \" at epoch\", np.argmax(H.history['accuracy']) + 1)\n",
    "\n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "        predictions = model.predict(x=test_Data)\n",
    "        threshold = 0.5\n",
    "        acc =  accuracy_score(predictions>=threshold,test_Labels)\n",
    "        recall = recall_score(predictions>=threshold,test_Labels)\n",
    "        auc = roc_auc_score(predictions>=threshold,test_Labels)\n",
    "        print(\"Test Accuracy:\", acc)\n",
    "        print(\"Recall Accuracy:\", recall)\n",
    "        print(\"AUC Score:\", auc)\n",
    "\n",
    "        perf[\"model\"].append(\"ActiModel\")\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        perf[\"acc\"].append(acc)\n",
    "        perf[\"recall\"].append(recall)\n",
    "        perf[\"auc\"].append(auc)\n",
    "        model_ls.append(model)\n",
    "        hist_ls.append(H)\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    print(perf_df)\n",
    "    return perf_df, model_ls, hist_ls\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Model Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Discriminator_BN_Bias(nn.Module):\n",
    "    def __init__(self, ngpu, input_shape , out_fea = 1, bias=True):\n",
    "        super(Discriminator_BN_Bias, self).__init__()\n",
    "        \n",
    "        # (input_shape[0], input_shape[1] )=  (number of data point, 6 axis channel )\n",
    "        win_size,in_channels = input_shape[0], input_shape[1]\n",
    "        self.in_channels = in_channels\n",
    "        self.ngpu = ngpu  \n",
    "        self.out_fea = out_fea\n",
    "        filter_num = 10\n",
    "        \n",
    "        self.conv0 = nn.Conv1d(in_channels = self.in_channels, \n",
    "                               out_channels = filter_num, \n",
    "                               kernel_size= 44,  stride= 2, padding=0, bias=bias)\n",
    "        \n",
    "        self.relu0= nn.ReLU()\n",
    "        num_fea = (win_size-44)//2 +1\n",
    "        self.conv1 = nn.Conv1d(filter_num,filter_num, kernel_size= 20,stride= 2, padding=0, bias=bias)\n",
    "        self.relu1= nn.ReLU()\n",
    "        num_fea = (num_fea-20)//2 +1\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(filter_num)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(filter_num,filter_num, kernel_size= 4, stride= 2, padding=0, bias=bias)\n",
    "        self.relu2= nn.ReLU()\n",
    "        \n",
    "        num_fea = (num_fea-4)//2 +1\n",
    "        self.bn2 = nn.BatchNorm1d(filter_num)\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=10)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = None \n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=200, out_features=out_fea, bias=bias)\n",
    "        self.softmax = nn.Softmax(dim=out_fea)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        nn.init.normal_(self.conv0.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.conv1.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.conv2.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.linear2.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv0.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv1.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv2.bias.data, 0.0, 1.)\n",
    "#         nn.init.normal_(self.avgpool.weight.data, 0.0, 1.)\n",
    "        \n",
    "    def l1_loss(self,factor=0.01):\n",
    "        l1_crit = nn.L1Loss(size_average=False)\n",
    "        reg_loss = 0.\n",
    "        loss = 0.\n",
    "        layers = [self.conv0, self.conv1, self.conv2]\n",
    "        for layer in layers:\n",
    "            for p in layer.parameters():\n",
    "                #print(p)\n",
    "                reg_loss += l1_crit(p, torch.zeros(p.shape))\n",
    "\n",
    "        loss = factor * reg_loss\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = input.permute(0,2,1)\n",
    "        x = self.conv0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = self.conv1(x)\n",
    "        x=  self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x=  self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.avgpool(x)\n",
    "        #print(\"Pooling shape:\",x.shape)\n",
    "        x = self.flatten(x)\n",
    "        if self.linear1 == None:\n",
    "            self.linear1 = nn.Linear(in_features=x.shape[1], out_features=200, bias=True)\n",
    "            nn.init.normal_(self.linear1.weight.data, 0.0, 1.)\n",
    "            nn.init.normal_(self.linear1.bias.data, 0.0, 1.)\n",
    "            \n",
    "        x = self.relu4(self.linear1(x))\n",
    "        out = self.linear2(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels ,bias= True, filter_num = 10):\n",
    "        super(BasicBlock, self).__init__()       \n",
    "        self.conv0 = nn.Conv1d(in_channels = in_channels, \n",
    "                               out_channels = filter_num, \n",
    "                               kernel_size= 3,  stride= 1, padding=1, bias=bias)\n",
    "        self.bn0 = nn.BatchNorm1d(filter_num)\n",
    "        self.relu0 = nn.ReLU()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels = in_channels, \n",
    "                               out_channels = filter_num, \n",
    "                               kernel_size= 3,  stride= 1, padding=1, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm1d(filter_num)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv0(x)\n",
    "        out = self.bn0(out)\n",
    "        out = self.relu0(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        \n",
    "        out += residual\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Discriminator_ResNet(nn.Module):\n",
    "    def __init__(self, ngpu, input_shape , out_fea = 1):\n",
    "        super(Discriminator_ResNet, self).__init__()\n",
    "        \n",
    "        # (input_shape[0], input_shape[1] )=  (number of data point, 6 axis channel )\n",
    "        win_size, in_channels= input_shape[0], input_shape[1]\n",
    "        self.in_channels = in_channels\n",
    "        self.ngpu = ngpu  \n",
    "        self.out_fea = out_fea\n",
    "        filter_num = 10\n",
    "        \n",
    "        self.conv0 = nn.Conv1d(in_channels = self.in_channels, \n",
    "                               out_channels = filter_num, \n",
    "                               kernel_size= 44,  stride= 2, padding=0, bias=True)\n",
    "        \n",
    "        self.relu0= nn.ReLU()\n",
    "        num_fea = (win_size-44)//2 +1\n",
    "        self.conv1 = nn.Conv1d(filter_num,filter_num, kernel_size= 20,stride= 2, padding=0, bias=True)\n",
    "        self.relu1= nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(filter_num)\n",
    "        \n",
    "        self.block1 =  BasicBlock( in_channels=filter_num  ,bias= True, filter_num = filter_num)\n",
    "        self.block2 =  BasicBlock( in_channels=filter_num  ,bias= True, filter_num = filter_num)\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=10)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = None \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=200, out_features=out_fea, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=out_fea)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        nn.init.normal_(self.conv0.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.conv1.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.linear2.bias.data, 0.0, 1.)\n",
    "        \n",
    "    def l1_loss(self,factor=0.01):\n",
    "        l1_crit = nn.L1Loss(size_average=False)\n",
    "        reg_loss = 0.\n",
    "        loss = 0.\n",
    "        layers = [self.conv0, self.conv1]\n",
    "        for layer in layers:\n",
    "            for p in layer.parameters():\n",
    "                #print(p)\n",
    "                reg_loss += l1_crit(p, torch.zeros(p.shape))\n",
    "\n",
    "        loss = factor * reg_loss\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "\n",
    "        x = input.permute(0,2,1)\n",
    "        x = self.conv0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = self.conv1(x)\n",
    "        x=  self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        #print(\"Pooling shape:\",x.shape)\n",
    "        x = self.flatten(x)\n",
    "        if self.linear1 == None:\n",
    "            self.linear1 = nn.Linear(in_features=x.shape[1], out_features=200, bias=True)\n",
    "            nn.init.normal_(self.linear1.weight.data, 0.0, 1.)\n",
    "            nn.init.normal_(self.linear1.bias.data, 0.0, 1.)\n",
    "            \n",
    "        x = self.relu2(self.linear1(x))\n",
    "        out = self.linear2(x)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "from torch import nn\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu, input_shape , out_fea = 1, device=\"cpu\"):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.device =device\n",
    "        # (input_shape[0], input_shape[1] )=  (number of data point, 6 axis channel )\n",
    "        win_size, in_channels = input_shape[0], input_shape[1]\n",
    "        self.in_channels = in_channels\n",
    "        self.ngpu = ngpu  \n",
    "        self.out_fea = out_fea\n",
    "        filter_size = 10\n",
    "        self.conv0 = nn.Conv1d(in_channels = self.in_channels, \n",
    "                               out_channels = filter_size, \n",
    "                               kernel_size= 44,  stride= 2, padding=0, bias=False)\n",
    "        \n",
    "        self.relu0= nn.ReLU()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(filter_size,filter_size, kernel_size= 20,stride= 2, padding=0, bias=False)\n",
    "        self.relu1= nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(filter_size,filter_size, kernel_size= 4, stride= 2, padding=0, bias=False)\n",
    "        self.relu2= nn.ReLU()\n",
    "        \n",
    "#         self.conv3 = nn.Conv1d(filter_size,filter_size, kernel_size= 4, stride= 2, padding=0, bias=False)\n",
    "#         self.relu3= nn.LeakyReLU(0, inplace=True)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=10)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = None \n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=200, out_features=out_fea, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=out_fea)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        nn.init.normal_(self.conv0.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.conv1.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.conv2.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.linear2.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv0.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv1.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv2.bias.data, 0.0, 1.)\n",
    "#         nn.init.normal_(self.avgpool.weight.data, 0.0, 1.)\n",
    "        \n",
    "    def l1_loss(self,factor=0.01):\n",
    "        l1_crit = nn.L1Loss(size_average=False)\n",
    "        reg_loss = 0.\n",
    "        loss = 0.\n",
    "        layers = [self.conv1, self.conv2]\n",
    "        for layer in layers:\n",
    "            for p in layer.parameters():\n",
    "                #print(p)\n",
    "                reg_loss += l1_crit(p, torch.zeros(p.shape, device= self.device))\n",
    "\n",
    "        loss = factor * reg_loss\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input):\n",
    "#         print(\"input shape:\",input.shape)\n",
    "        x = input.permute(0,2,1)\n",
    "        x = self.conv0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.avgpool(x)\n",
    "        #print(\"Pooling shape:\",x.shape)\n",
    "        x = self.flatten(x)\n",
    "        if self.linear1 == None:\n",
    "            self.linear1 = nn.Linear(in_features=x.shape[1], out_features=200, bias=True)\n",
    "            nn.init.normal_(self.linear1.weight.data, 0.0, 1.)\n",
    "            nn.init.normal_(self.linear1.bias.data, 0.0, 1.)\n",
    "            \n",
    "        x = self.relu4(self.linear1(x))\n",
    "        out = self.linear2(x)\n",
    "#         out = self.sigmoid(x)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tf_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tf_utils.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from data_loader import loadEvents\n",
    "def get_meal_info(person_name = None,file_ls = [], file_ls_doc=None,root_path = \"../data/\",print_file=False,round_decimal=1):\n",
    "        \"\"\"\n",
    "        if file_ls is not given, then get file_ls according to person_name\n",
    "        file path = root_path + file name in all_files_list.txt\n",
    "\n",
    "        return:\n",
    "            meal event count, total minutes of all meals, total hours of all meals,total day counts\n",
    "\n",
    "        \"\"\"\n",
    "        meal_counts = 0\n",
    "        sec_counts = 0\n",
    "        min_counts = 0\n",
    "        hour_counts = 0\n",
    "        total_hours = 0\n",
    "        total_mins = 0\n",
    "        total_sec = 0\n",
    "        day_counts = 0 \n",
    "        if person_name ==None:\n",
    "            return meal_counts, min_counts,hour_counts, day_counts, total_hours\n",
    "\n",
    "        data_indices_file = \"../data-file-indices/\" +person_name+\"/all_files_list.txt\"\n",
    "        fp = open(data_indices_file,\"r\")\n",
    "        txt = fp.read()\n",
    "        fp.close()\n",
    "        file_ls = txt.split(\"\\n\")\n",
    "        while '' in file_ls:\n",
    "            file_ls.remove('')\n",
    "       \n",
    "        day_counts = len(file_ls)\n",
    "        \n",
    "        for file_name in file_ls:\n",
    "            file_name = root_path + file_name\n",
    "            TotalEvents, EventStart, EventEnd, EventNames, TimeOffset,EndTime = loadEvents(file_name, debug_flag = False, print_file=print_file)\n",
    "            meal_counts += TotalEvents\n",
    "            total_sec +=  abs(EndTime - TimeOffset)\n",
    "            for i in range(len(EventStart)):\n",
    "                sec_counts += ( EventEnd[i]- EventStart[i])//15\n",
    "                \n",
    "        total_hours = total_sec/(60*60)\n",
    "        min_counts = sec_counts/60\n",
    "        hour_counts = min_counts/60\n",
    "        average_meal_per_day = meal_counts/len(file_ls)\n",
    "        average_hour_per_meal = hour_counts/meal_counts\n",
    "        # round numbers\n",
    "        total_hours = round(total_hours, round_decimal)\n",
    "        min_counts = round(min_counts, round_decimal)\n",
    "        hour_counts = round(hour_counts, round_decimal)\n",
    "        average_meal_per_day = round(average_meal_per_day,round_decimal)\n",
    "        average_hour_per_meal = round(average_hour_per_meal, round_decimal)\n",
    "        \n",
    "        no_eating_hours = total_hours - hour_counts\n",
    "        weight_ratio = round(no_eating_hours/hour_counts, round_decimal)\n",
    "        result = pd.DataFrame({\"dataset\": person_name,\"Days\":day_counts, \n",
    "                      \"Total Hours\":total_hours,\"Meal Counts\":meal_counts,\n",
    "                      \"Average Meal Counts Per Day\":average_meal_per_day,\"Average Hours Per Meal\": average_hour_per_meal,\n",
    "                      \"Eating Hours\":hour_counts, \"No Eating Hours\":no_eating_hours,\n",
    "                     \"Balance Ratio(no_eat/eat)\":weight_ratio},index=[0])\n",
    "    \n",
    "        return result\n",
    "\n",
    "          \n",
    "def get_dataset_info(names= [\"wenkanw\"],winmin=6,stridesec=5):\n",
    "    \"\"\"\n",
    "    Function to get information of meal dataset\n",
    "    \"\"\"\n",
    "    meal_info = defaultdict(list)\n",
    "    dataset_results = pd.DataFrame()\n",
    "    for name in names:\n",
    "        result = get_meal_info(person_name=name)\n",
    "        if dataset_results.empty:\n",
    "            dataset_results = result\n",
    "        else:\n",
    "            dataset_results = dataset_results.append(result,ignore_index=True)\n",
    "    \n",
    "    # append total summary\n",
    "#     print( dataset_results)\n",
    "    total_result=pd.DataFrame({\"dataset\":\"total\"},columns = dataset_results.columns,index=[0])\n",
    "    # append average summary\n",
    "    average_result=pd.DataFrame({\"dataset\":\"average\"},columns = dataset_results.columns,index=[0])\n",
    "    key_ls = [\"Days\",\"Total Hours\",\"Meal Counts\",\"Eating Hours\",\"No Eating Hours\"]\n",
    "    for key in dataset_info.columns:\n",
    "        if key in key_ls:\n",
    "            total_result[key].at[0] = round(dataset_results[key].sum() ,1)\n",
    "            average_result[key].at[0] = round(dataset_results[key].mean(),1)\n",
    "\n",
    "    ls = [total_result, average_result]\n",
    "    for df in ls:\n",
    "        df[\"Average Meal Counts Per Day\"].at[0] = round(df[\"Meal Counts\"].values[0]/df[\"Days\"].values[0], 1)\n",
    "        df[\"Average Hours Per Meal\"].at[0] =round( df[\"Eating Hours\"].values[0]/df[\"Meal Counts\"].values[0], 1)\n",
    "        df[\"Balance Ratio(no_eat/eat)\"].at[0] =round(df[\"No Eating Hours\"].values[0]/df[\"Eating Hours\"].values[0],1)\n",
    "        dataset_results =dataset_results.append(df,ignore_index=True)\n",
    "\n",
    "    return dataset_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from dataset import create_train_test_file_list,  balance_data_indices  #Person_MealsDataset,\n",
    "from utils import *\n",
    "from model import *\n",
    "def train_models(model, win_ls = [],EPOCHS = 10,stridesec = 5,name = \"wenkanw\",model_name=\"acti_6min\" ,\n",
    "                 random_seed= 1000, split_day=False,test_balanced=False,\n",
    "                create_file_ls = False):\n",
    "    \"\"\"\n",
    "    Train model using train/test spit\n",
    "    \"\"\"\n",
    "    from numpy.random import seed\n",
    "    seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "#     tf.set_random_seed(random_seed)\n",
    "    from datetime  import datetime\n",
    "    batch_size = 128\n",
    "    outfile = sys.stdout\n",
    "    perf = {\"model\":[],\"data\":[],\"win(sec)\":[], \"wacc\":[],\"f1\":[],\"recall\":[],\"acc\":[]}\n",
    "    model_ls = []\n",
    "    hist_ls = []\n",
    "    for winsize in win_ls:\n",
    "        tf.random.set_seed(random_seed)\n",
    "        seed(random_seed)\n",
    "        \n",
    "        winmin = winsize\n",
    "        winlength = int(winmin * 60 * 15)\n",
    "        step = int(stridesec * 15)\n",
    "        start_time = datetime.now()\n",
    "        arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "              \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "              \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "        [os.system(cmd) for cmd in arr]\n",
    "        print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "        print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "        print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "\n",
    "\n",
    "        if split_day:\n",
    "            pathtemp = \"../models/\" + name+\"_models\" +\"/\"+model_name+\"_split_day_M_F_\"\n",
    "        else:\n",
    "            pathtemp = \"../models/\" + name+\"_models\" +\"/\"+model_name+\"_M_F_\"\n",
    "            \n",
    "        #pathtemp = \"../models/\" + name +\"/\"+model_name+\"_M_F_\"\n",
    "        modelpath = pathtemp + \"{:f}Min.h5\".format(winmin)\n",
    "        jsonpath = pathtemp + \"{:f}Min.json\".format(winmin)\n",
    "        \n",
    "        print(\"Model to Save: \",modelpath)\n",
    "        print()\n",
    "        ########### Load the dataset################\n",
    "        person = name\n",
    "        if create_file_ls:\n",
    "            create_train_test_file_list(file_name= \"all_files_list.txt\",person_name =name,\n",
    "                         out_path = \"../data-file-indices/\",root_path= \"../\",\n",
    "                         test_ratio = 0.2, print_flag = True, shuffle=True, random_state=random_seed)\n",
    "        \n",
    "        if split_day:\n",
    "            \n",
    "\n",
    "            meal_data_train = Person_MealsDataset(person_name= person, file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "            meal_data_test = Person_MealsDataset(person_name= person, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "\n",
    "            train_indices, valid_indices = split_train_test_indices(X= [i for i in range(len(meal_data_train.labels))],\n",
    "                                                                    y = meal_data_train.labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            #balanced train set\n",
    "            trainset_labels = meal_data_train.labels[train_indices]\n",
    "            train_indices = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "\n",
    "            # balance test set\n",
    "            testset_labels = meal_data_test.labels\n",
    "            if test_balanced:\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices=[i for i in range(len(meal_data_test))] ,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                # without balancing data\n",
    "                test_indices = [i for i in range(len(meal_data_test))] \n",
    "                \n",
    "            # get numpy dataset\n",
    "            balancedData, balancedLabels = meal_data_train.get_subset(train_indices)\n",
    "            valid_balancedData, valid_balancedLabels = meal_data_train.get_subset(valid_indices)\n",
    "            test_Data, test_Labels = meal_data_test.get_subset(test_indices)\n",
    "\n",
    "        else:\n",
    "        \n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "            samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "            # split train set and test set\n",
    "            train_indices, test_indices = split_train_test_indices(X= [i for i in range(len(labels))],\n",
    "                                                                    y = labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            # balance train set\n",
    "            trainset_labels = labels[train_indices]\n",
    "            train_indices_balanced = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            \n",
    "            \n",
    "            testset_labels = labels[test_indices]\n",
    "            if test_balanced:\n",
    "                #balance test set\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices= test_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                test_indices = test_indices \n",
    "            \n",
    "            \n",
    "            train_set_balanced = torch.utils.data.Subset(meal_data, train_indices_balanced)\n",
    "            test_set = torch.utils.data.Subset(meal_data, test_indices)\n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "            test_loader = torch.utils.data.DataLoader(test_set ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "            print(\"Data Loader Created\")            \n",
    "            \n",
    "            # split validation set\n",
    "            balanced_trainset_labels = labels[train_indices_balanced]\n",
    "            train_indices, valid_indices = split_train_test_indices(X= train_indices_balanced,\n",
    "                                                                    y = balanced_trainset_labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            valid_set_balanced = torch.utils.data.Subset(meal_data, valid_indices)\n",
    "            valid_loader = torch.utils.data.DataLoader(valid_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "            # Get numpy dataset: balanced trainset, validation set, test set\n",
    "            balancedData, balancedLabels = meal_data.get_subset(train_indices)\n",
    "            valid_balancedData, valid_balancedLabels = meal_data.get_subset(valid_indices)\n",
    "            test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "        \n",
    "\n",
    "        #training settings\n",
    "        mcp_save = tf.keras.callbacks.ModelCheckpoint(modelpath, save_best_only=True, monitor='accuracy')\n",
    "        \n",
    "\n",
    "        scheduler = tf.keras.callbacks.ReduceLROnPlateau( monitor='val_loss', factor=0.1, patience=3, verbose=0,\n",
    "                                             mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.)\n",
    "        \n",
    "        ##########train model ###############\n",
    "        H = model.fit(x=balancedData, y = balancedLabels,\n",
    "                       validation_data=(valid_balancedData, valid_balancedLabels),\n",
    "                    epochs = EPOCHS, batch_size=batch_size, verbose=1,\n",
    "                    callbacks=[mcp_save,scheduler]) # removed addons.LossHistory(jsonpath) for compatibility with TensorFlow 2.2.0, needs to be re-added at some point\n",
    "\n",
    "        print(\"Max value: \", max(H.history['accuracy']), \" at epoch\", np.argmax(H.history['accuracy']) + 1)\n",
    "\n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, balanced_accuracy_score, f1_score\n",
    "        predictions = model.predict(x=test_Data)\n",
    "        threshold = 0.5\n",
    "        wacc =  balanced_accuracy_score(test_Labels,predictions>=threshold)\n",
    "        f1 =  f1_score(test_Labels,predictions>=threshold)\n",
    "        acc =  accuracy_score(test_Labels,predictions>=threshold)\n",
    "        recall = recall_score(test_Labels,predictions>=threshold)\n",
    "        \n",
    "        #auc = roc_auc_score(test_Labels,predictions>=threshold)\n",
    "        print(\"Weighted Accuracy:\", wacc)\n",
    "        print(\"Test Accuracy:\", acc)\n",
    "        print(\"F1-score:\", f1)\n",
    "        print(\"Recall Accuracy:\", recall)\n",
    "        #print(\"AUC Score:\", auc)\n",
    "\n",
    "        perf[\"model\"].append(\"ActiModel\")\n",
    "        perf[\"data\"].append(name)\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        perf[\"wacc\"].append(wacc)\n",
    "        perf[\"f1\"].append(f1)\n",
    "        perf[\"acc\"].append(acc)\n",
    "        perf[\"recall\"].append(recall)\n",
    "        #perf[\"auc\"].append(auc)\n",
    "        model_ls.append(model)\n",
    "        hist_ls.append(H)\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    print(perf_df)\n",
    "    return perf_df, model_ls, hist_ls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "name_ls = [\"wenkanw\",'adam',\"lawler\",\"shaurya\"]\n",
    "dataset_info = get_dataset_info(names= name_ls)\n",
    "balance_ratio = dataset_info[dataset_info[\"dataset\"]==\"total\"]['Balance Ratio(no_eat/eat)'].values[0]\n",
    "\n",
    "def weight_accuracy(y_true, y_pred,weight= balance_ratio):\n",
    "    TP = sum( (y_true==1) &(y_pred==1) )\n",
    "    FN = sum( (y_true==1) &(y_pred==0) )\n",
    "    TN = sum( (y_true==0) &(y_pred==0) )\n",
    "    FP = sum( (y_true==0) &(y_pred==1) )\n",
    "    print(\"TP: \",TP, \"FP: \",FP, \"TN: \",TN, \"FN: \",FN)\n",
    "    return (weight*TP + TN)/(weight*(TP+FN) + (TN+FP))\n",
    "\n",
    "def test_models_time_metric(winmin=1, stridesec = 5,names= [\"wenkanw\"],random_seed=1000, split_day=False, test_balance=False, test_CAD=False):\n",
    "    \"\"\"\n",
    "    Test time metrics\n",
    "    \"\"\"\n",
    "    perf = defaultdict(list)\n",
    "    meal_info = defaultdict(list)\n",
    "    # Don't test on CAD test set if it is not enabled\n",
    "    if not test_CAD and \"CAD\" in names:\n",
    "        names.remove(\"CAD\")\n",
    "        \n",
    "    for name in names:\n",
    "        person = name\n",
    "        if split_day:\n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "\n",
    "            # balance test set\n",
    "            testset_labels = meal_data.labels\n",
    "            if test_balance:\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices=[i for i in range(len(meal_data))] ,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                test_indices = [i for i in range(len(meal_data))]\n",
    "            # get numpy dataset\n",
    "            test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "        else:            \n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "            samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "            # split train set and test set\n",
    "            train_indices, test_indices = split_train_test_indices(X= [i for i in range(len(labels))],\n",
    "                                                                            y = labels, test_size = 0.2,\n",
    "                                                                           random_seed = random_seed)\n",
    "            \n",
    "            if test_balance:\n",
    "                testset_labels = labels[test_indices]\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices= test_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                test_indices = test_indices\n",
    "            testset_labels = labels[test_indices]\n",
    "            print(\"Testing on : \", sum(testset_labels==1),\"positive samples, \",sum(testset_labels==0),\" negative samples\" )\n",
    "            test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "        if name !=\"CAD\":\n",
    "            meal_counts, min_counts,hour_counts, day_counts,total_hours = meal_data.get_mealdataset_info(person_name=name, file_ls_doc=\"all_files_list.txt\")\n",
    "        else:\n",
    "            # data from paper\n",
    "            meal_counts, min_counts,hour_counts, day_counts,total_hours = 1063, 250*60,250, 354, 4680\n",
    "        meal_info[\"dataset\"].append(name)\n",
    "        meal_info[\"Days\"].append(day_counts)\n",
    "        meal_info[\"Meal_Hours\"].append(round(hour_counts,1)) \n",
    "        meal_info[\"Meal_Counts\"].append(meal_counts) \n",
    "        meal_info[\"Total_Hours\"].append(total_hours) \n",
    "        \n",
    "        perf[\"dataset\"].append(name)\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, f1_score, balanced_accuracy_score, precision_score\n",
    "        group_model_W  = tf.keras.models.load_model('../models/CAD_models/acti_6min_M_F_6.000000Min.h5')\n",
    "        \n",
    "        if name != \"CAD\":\n",
    "            if split_day:\n",
    "                individual_model = tf.keras.models.load_model('../models/'+ name+ '_models/acti_6min_split_day_M_F_6.000000Min.h5')\n",
    "            else:    \n",
    "                individual_model = tf.keras.models.load_model('../models/'+ name+ '_models/acti_6min_M_F_6.000000Min.h5')\n",
    "        models = {\"suffix\":['Individual-Model','GroupModel'],  \"model\":[individual_model,group_model_W]}\n",
    "        \n",
    "        for i in range(len(models[\"suffix\"])):\n",
    "            suffix = models[\"suffix\"][i]\n",
    "            model = models[\"model\"][i]\n",
    "            # if the dataset is CAD group dataset and model is individual model\n",
    "            # we don't need to make prediction on that data\n",
    "            if name == \"CAD\" and suffix =='Individual-Model':\n",
    "                acc = None\n",
    "                auc = None\n",
    "                recall = None\n",
    "            else:\n",
    "\n",
    "                predictions = model.predict(x=test_Data).squeeze(1)\n",
    "                threshold = 0.5\n",
    "                prediction = (predictions>=threshold).astype(int)\n",
    "                wacc =  balanced_accuracy_score(test_Labels,prediction)\n",
    "                acc =  accuracy_score(test_Labels,prediction)\n",
    "                recall = recall_score(test_Labels,prediction)\n",
    "                f1 = f1_score(test_Labels,prediction)\n",
    "                precision = precision_score(test_Labels,prediction)\n",
    "                print(\"Test label: \",test_Labels)\n",
    "                print(\"Predictions:\",prediction)\n",
    "                \n",
    "                # weighted accuracy 2 is computed by (weight*TP +TN)/(weight*(TP+FN) + (TN+FP))\n",
    "                wacc2 = weight_accuracy(test_Labels,prediction)\n",
    "            \n",
    "            \n",
    "            print(\"Weighted Accuracy:\", wacc)\n",
    "            print(\"Weighted Accuracy2:\", wacc2)\n",
    "            print(\"Recall:\", recall)\n",
    "            print(\"Precision:\", precision)\n",
    "            print(\"F1:\", f1)\n",
    "            print(\"Test Accuracy:\", acc)\n",
    "            \n",
    "            perf[\"WAcc: \"+suffix].append(wacc)\n",
    "            perf[\"WAcc2: \"+suffix].append(wacc2)\n",
    "            perf[\"Recall: \"+suffix].append(recall)\n",
    "            perf[\"Precision: \"+suffix].append(precision)\n",
    "            perf[\"F1: \"+suffix].append(f1)\n",
    "            perf[\"Acc: \"+suffix].append(acc)\n",
    "\n",
    "    meal_info = pd.DataFrame(meal_info)\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    return meal_info, perf_df\n",
    "\n",
    "\n",
    "def print_time_metrics(result, round_decimal = 3):\n",
    "    perf_df = pd.DataFrame()\n",
    "    mykeys = [\"dataset\",\"win(sec)\",\"WAcc\", \"F1\",\"Precision\",\"Recall\"]\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']    \n",
    "    for k in mykeys:\n",
    "        for key in result.keys():\n",
    "            if k in key:\n",
    "                perf_df[key] = result[key]\n",
    "                if result[key].dtype in numerics:\n",
    "                    perf_df[key] = np.round(perf_df[key],round_decimal)\n",
    "    for key in result.keys():\n",
    "        if \"WAcc\" not in key and \"Acc\" in key:\n",
    "            perf_df[key] = result[key].values.round(round_decimal)\n",
    "    \n",
    "    mean_perf = pd.DataFrame(columns = perf_df.keys())\n",
    "    mean_perf = mean_perf.append({\"dataset\":\"average performance\",\"win(sec)\":\"-\"},ignore_index=True)\n",
    "    for key in perf_df.keys():\n",
    "        if key.lower() != \"dataset\" and  key.lower() != \"win(sec)\":\n",
    "            mean_perf[key].at[0] = perf_df[key].mean().round(round_decimal)\n",
    "    \n",
    "    perf_df = perf_df.append(mean_perf,ignore_index=True)\n",
    "            \n",
    "    return perf_df\n",
    "\n",
    "\n",
    "\n",
    "def hysteresis_threshold(model, data,start_threshold=0.8, end_threshold=0.4, winmin = 6,\n",
    "                        stepsec=5, episode_min = 1.,t_pause = 900):\n",
    "    \"\"\"\n",
    "    model: tensorflow model\n",
    "    data:  This dataset must be the self-defined class of Person_MealsDataset  datasetset in my dataset.py/pytorch dataset without using shuffle. \n",
    "    Keep the order of dataset after extracting window samples!  You can also define your own dataset using class object to create the interface\n",
    "    \n",
    "    start_threshold: the high threshold of the beginning of segmentation\n",
    "    \n",
    "    end_threshold: the end threshold of the end of segmentation\n",
    "    \n",
    "    winmin: size of a window sample in unit of  minute\n",
    "    \n",
    "    stepsec: stride to move the window in unit of second / the number of second between two adjacent window samples\n",
    "    \n",
    "    episode_min: the minimum length of eating episode in unit of minute. If end of segmentation -start of segmentation < episode_min,\n",
    "        then the episode will not be counted\n",
    "    \n",
    "    \"\"\"\n",
    "    result_ls = []\n",
    "    \n",
    "    \n",
    "    days = set(data.data_indices[:,0])\n",
    "    for day in days:\n",
    "        # Select and Extract the data and labels of the corresponding day from the whole dataset\n",
    "        sample_indices= np.where(data.data_indices[:,0]==day)[0]\n",
    "        result = {'day':day,\"stepsec\": stepsec,'segment_start':[], 'segment_end':[],'proba':[],'predictions':np.zeros([len(sample_indices)]),'labels':[],\"segment_count\":0}\n",
    "        \n",
    "        # get the numpy array of samples and labels\n",
    "        samples, labels = data.get_subset(sample_indices)\n",
    "        probas = model(samples)\n",
    "        state = 0\n",
    "        start = 0\n",
    "        end = 0 \n",
    "        pause_counter = 0\n",
    "        # one day data\n",
    "        print(\"Day: \",day)\n",
    "        for i in range(len(sample_indices)):\n",
    "            #print(\"i:\",i)\n",
    "            #sample, label = data[i][0].numpy(),data[i][1]\n",
    "            #sample = np.expand_dims(sample,axis=0)\n",
    "            #proba = model(sample).numpy()[0][0]\n",
    "            sample = samples[i]\n",
    "            label = labels[i]\n",
    "            proba = probas[i].numpy()[0]\n",
    "            \n",
    "            result['proba'].append(proba)\n",
    "            result['labels'].append(label)\n",
    "            \n",
    "            if state ==0 and proba > start_threshold:\n",
    "                state = 1\n",
    "                start = i\n",
    "            elif state == 1 and proba <end_threshold:\n",
    "                state = 2\n",
    "                end = i+1\n",
    "                pause_counter = 0\n",
    "            elif state ==2:\n",
    "                if proba > start_threshold:\n",
    "                    state = 1\n",
    "                else:\n",
    "                    pause_counter += stepsec\n",
    "                    if pause_counter >= t_pause:\n",
    "                        # convert time to second and check threshold\n",
    "                        if (end-start)*stepsec >= episode_min*60:\n",
    "                            # save data\n",
    "                            result['segment_start'].append(start)\n",
    "                            result['segment_end'].append(end)\n",
    "                            result['segment_count'] += 1\n",
    "                            result['predictions'][start:end] = 1\n",
    "                            pass\n",
    "                        end = 0\n",
    "                        state = 0\n",
    "        if state != 0:\n",
    "            # if segment ended at the end of data\n",
    "            if end != 0:\n",
    "                result['segment_start'].append(start)\n",
    "                result['segment_end'].append(end)\n",
    "                result['predictions'][start:end] = 1\n",
    "            else:\n",
    "                result['segment_count'] -= 1  \n",
    "            result['segment_count'] += 1\n",
    "            \n",
    "        result_ls.append(result)\n",
    "        print(\"Segmentation Completed. \")\n",
    "                            \n",
    "    return pd.DataFrame(result_ls)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_episode_metrics(result, meal_data):\n",
    "    \"\"\"\n",
    "    result: result from  hysteresis threshold function\n",
    "    meal_data: meal dataset of Person_MealData\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    perf = {\"TPR\":[],\"FP/TP\":[],\"TP\":[], \"FP\":[],\"FN\":[]}\n",
    "    tpr = 0. \n",
    "    FP_TP = 0.\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    # get segmentation of ground truth labeled by user\n",
    "    start_ls, end_ls = meal_data.get_GT_segment()\n",
    "    \n",
    "    meal_counts, min_counts,hour_counts, day_counts,total_hours = meal_data.get_mealdataset_info()\n",
    "    \n",
    "    # iterate every day\n",
    "    for i in range(len(result)):\n",
    "        #preds = result.iloc[i]['predictions']\n",
    "        #labels =  result.iloc[i]['labels']\n",
    "        event_start, event_end= start_ls[i], end_ls[i]\n",
    "        detect_start, detect_end = result.iloc[i]['segment_start'],result.iloc[i]['segment_end']\n",
    "        GT = np.array([-1]*len(event_start) )  # default all meals are missing -1, FN\n",
    "        detect = np.array([-1]*len(detect_start)) # default all detected meals are wrong -1, FP\n",
    "        for index in range(len(event_start)):\n",
    "            # e_s: event start,  e_e: event end\n",
    "            # d_s: detection start,  d_e: detection end\n",
    "            e_s, e_e = event_start[index], event_end[index]\n",
    "            for index2 in range(len(detect_start)):\n",
    "                # convert segment from sec to index of data point\n",
    "                d_s = detect_start[index2] * result.iloc[i]['stepsec']*15\n",
    "                d_e = detect_end[index2]* result.iloc[i]['stepsec']*15\n",
    "                #print(\"ds: {} d_e: {}, e_s:{}, e_e: {}\".format(d_s,d_e, e_s, e_e))\n",
    "                if (e_s>=d_s and e_s <= d_e) or (d_s>= e_s and d_s<= e_e):\n",
    "                    GT[index] = index2\n",
    "                    detect[index2] = index\n",
    "        #print(\"GT:\",GT, \"Detect:\", detect)\n",
    "        TP += sum(GT!=-1)\n",
    "        FN += sum(GT==-1)\n",
    "        FP += sum(detect==-1)\n",
    "                \n",
    "    \n",
    "    print(\"total_meal:\",meal_counts, \"TP: \", TP, \"FP: \", FP, \"FN: \", FN)\n",
    "    perf['TPR'].append(TP/(TP+FN))\n",
    "    if TP ==0:\n",
    "        perf['FP/TP'].append(None)\n",
    "    else:\n",
    "        perf['FP/TP'].append(FP/TP)\n",
    "    perf[\"TP\"].append(TP)\n",
    "    perf[\"FP\"].append(FP)\n",
    "    perf[\"FN\"].append(FN)\n",
    "    result_df = pd.DataFrame(perf)\n",
    "        \n",
    "    return pd.DataFrame(result_df)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def test_models_episode_metric(winmin=6, stridesec = 5,names= [\"wenkanw\"],random_seed=1000, test_balance=False, test_CAD=False,test_alldata=False):\n",
    "    perf = defaultdict(list)\n",
    "    meal_info = defaultdict(list)\n",
    "    threshold = {'wenkanw':[0.8, 0.4], 'adam':[0.8,0.4],'lawler':[0.8,0.4], 'shaurya':[0.8,0.4]}\n",
    "    # Don't test on CAD test set if it is not enabled\n",
    "    if not test_CAD and \"CAD\" in names:\n",
    "        names.remove(\"CAD\")\n",
    "        \n",
    "    for name in names:\n",
    "        \n",
    "        high_th, low_th = threshold[name][0], threshold[name][1]\n",
    "        \n",
    "        person = name\n",
    "        # test episode metrics that split dataset by days\n",
    "        if not test_alldata:\n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "        else:\n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "        # balance test set    \n",
    "        if test_balance:\n",
    "            testset_labels = meal_data.labels\n",
    "            test_indices = balance_data_indices(testset_labels,data_indices=[i for i in range(len(meal_data))] ,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "        else:\n",
    "            test_indices = [i for i in range(len(meal_data))]\n",
    "        # get numpy dataset\n",
    "        #test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "        \n",
    "        \n",
    "        meal_counts, min_counts,hour_counts, day_counts,total_hours = meal_data.get_mealdataset_info(person_name=name)\n",
    "                \n",
    "        \n",
    "        perf[\"Days\"].append(day_counts)\n",
    "        perf[\"Meal_Hours\"].append(round(hour_counts,1)) \n",
    "        perf[\"Meal_Counts\"].append(meal_counts) \n",
    "        perf[\"dataset\"].append(name)\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        \n",
    "            \n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, f1_score, balanced_accuracy_score, precision_score\n",
    "        group_model_W  = tf.keras.models.load_model('../models/CAD_models/acti_6min_M_F_6.000000Min.h5')\n",
    "        \n",
    "        individual_model = tf.keras.models.load_model('../models/'+ name+ '_models/acti_6min_split_day_M_F_6.000000Min.h5')\n",
    "            \n",
    "        models = {\"suffix\":['Individual-Model','GroupModel'],  \"model\":[individual_model,group_model_W]}\n",
    "        \n",
    "        for i in range(len(models[\"suffix\"])):\n",
    "            suffix = models[\"suffix\"][i]\n",
    "            model = models[\"model\"][i]\n",
    "            # if the dataset is CAD group dataset and model is individual model\n",
    "            # we don't need to make prediction on that data\n",
    "            \n",
    "            result = hysteresis_threshold(model, meal_data,start_threshold=high_th, end_threshold=low_th, winmin = 6, stepsec=5, episode_min = 1.)\n",
    "            episode_perf_df = get_episode_metrics(result,meal_data)\n",
    "            perf[\"TPR: \"+suffix].append(episode_perf_df[\"TPR\"].iloc[0])\n",
    "            perf[\"FP/TP: \"+suffix].append(episode_perf_df[\"FP/TP\"].iloc[0])\n",
    "            perf[\"TP: \"+suffix].append(episode_perf_df[\"TP\"].iloc[0])\n",
    "            perf[\"FP: \"+suffix].append(episode_perf_df[\"FP\"].iloc[0])\n",
    "            perf[\"FN: \"+suffix].append(episode_perf_df[\"FN\"].iloc[0])\n",
    "            \n",
    "            print(episode_perf_df)\n",
    "\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    return perf_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting packages.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile packages.py\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "### imports\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Turn off TensorFlow logging\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "import loadfile\n",
    "import addons\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, Activation\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Input, add\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.models import load_model, save_model, Model\n",
    "\n",
    "\n",
    "from dataset import create_train_test_file_list, Person_MealsDataset, balance_data_indices\n",
    "from utils import *\n",
    "from model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_model.py\n",
    "\n",
    "from packages import *\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from model import *\n",
    "def train_models(model, win_ls = [],EPOCHS = 10,stridesec = 5,name = \"wenkanw\",model_name=\"acti_6min\" ,\n",
    "                 random_seed= 1000, split_day=False,test_balanced=False,\n",
    "                create_file_ls = False):\n",
    "    from numpy.random import seed\n",
    "    seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "#     tf.set_random_seed(random_seed)\n",
    "    from datetime  import datetime\n",
    "    batch_size = 128\n",
    "    outfile = sys.stdout\n",
    "    perf = {\"model\":[],\"data\":[],\"win(sec)\":[], \"wacc\":[],\"f1\":[],\"recall\":[],\"acc\":[]}\n",
    "    model_ls = []\n",
    "    hist_ls = []\n",
    "    for winsize in win_ls:\n",
    "        tf.random.set_seed(random_seed)\n",
    "        seed(random_seed)\n",
    "        \n",
    "        winmin = winsize\n",
    "        winlength = int(winmin * 60 * 15)\n",
    "        step = int(stridesec * 15)\n",
    "        start_time = datetime.now()\n",
    "        arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "              \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "              \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "        [os.system(cmd) for cmd in arr]\n",
    "        print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "        print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "        print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "\n",
    "\n",
    "        if split_day:\n",
    "            pathtemp = \"../models/\" + name+\"_models\" +\"/\"+model_name+\"_split_day_M_F_\"\n",
    "        else:\n",
    "            pathtemp = \"../models/\" + name+\"_models\" +\"/\"+model_name+\"_M_F_\"\n",
    "            \n",
    "        #pathtemp = \"../models/\" + name +\"/\"+model_name+\"_M_F_\"\n",
    "        modelpath = pathtemp + \"{:f}Min.h5\".format(winmin)\n",
    "        jsonpath = pathtemp + \"{:f}Min.json\".format(winmin)\n",
    "        \n",
    "        print(\"Model to Save: \",modelpath)\n",
    "        print()\n",
    "        ########### Load the dataset################\n",
    "        person = name\n",
    "        if create_file_ls:\n",
    "            create_train_test_file_list(file_name= \"all_files_list.txt\",person_name =name,\n",
    "                         out_path = \"../data-file-indices/\",root_path= \"../\",\n",
    "                         test_ratio = 0.2, print_flag = True, shuffle=True, random_state=random_seed)\n",
    "        \n",
    "        if split_day:\n",
    "            \n",
    "\n",
    "            meal_data_train = Person_MealsDataset(person_name= person, file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "            meal_data_test = Person_MealsDataset(person_name= person, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "\n",
    "            train_indices, valid_indices = split_train_test_indices(X= [i for i in range(len(meal_data_train.labels))],\n",
    "                                                                    y = meal_data_train.labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            #balanced train set\n",
    "            trainset_labels = meal_data_train.labels[train_indices]\n",
    "            train_indices = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "\n",
    "            # balance test set\n",
    "            testset_labels = meal_data_test.labels\n",
    "            if test_balanced:\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices=[i for i in range(len(meal_data_test))] ,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                # without balancing data\n",
    "                test_indices = [i for i in range(len(meal_data_test))] \n",
    "                \n",
    "            # get numpy dataset\n",
    "            balancedData, balancedLabels = meal_data_train.get_subset(train_indices)\n",
    "            valid_balancedData, valid_balancedLabels = meal_data_train.get_subset(valid_indices)\n",
    "            test_Data, test_Labels = meal_data_test.get_subset(test_indices)\n",
    "\n",
    "        else:\n",
    "        \n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "            samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "            # split train set and test set\n",
    "            train_indices, test_indices = split_train_test_indices(X= [i for i in range(len(labels))],\n",
    "                                                                    y = labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            # balance train set\n",
    "            trainset_labels = labels[train_indices]\n",
    "            train_indices_balanced = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            \n",
    "            \n",
    "            testset_labels = labels[test_indices]\n",
    "            if test_balanced:\n",
    "                #balance test set\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices= test_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                test_indices = test_indices \n",
    "            \n",
    "            \n",
    "            train_set_balanced = torch.utils.data.Subset(meal_data, train_indices_balanced)\n",
    "            test_set = torch.utils.data.Subset(meal_data, test_indices)\n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "            test_loader = torch.utils.data.DataLoader(test_set ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "            print(\"Data Loader Created\")            \n",
    "            \n",
    "            # split validation set\n",
    "            balanced_trainset_labels = labels[train_indices_balanced]\n",
    "            train_indices, valid_indices = split_train_test_indices(X= train_indices_balanced,\n",
    "                                                                    y = balanced_trainset_labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            valid_set_balanced = torch.utils.data.Subset(meal_data, valid_indices)\n",
    "            valid_loader = torch.utils.data.DataLoader(valid_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "            # Get numpy dataset: balanced trainset, validation set, test set\n",
    "            balancedData, balancedLabels = meal_data.get_subset(train_indices)\n",
    "            valid_balancedData, valid_balancedLabels = meal_data.get_subset(valid_indices)\n",
    "            test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "        \n",
    "\n",
    "        #training settings\n",
    "        mcp_save = tf.keras.callbacks.ModelCheckpoint(modelpath, save_best_only=True, monitor='accuracy')\n",
    "        \n",
    "\n",
    "        scheduler = tf.keras.callbacks.ReduceLROnPlateau( monitor='val_loss', factor=0.1, patience=3, verbose=0,\n",
    "                                             mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.)\n",
    "        \n",
    "        ##########train model ###############\n",
    "        H = model.fit(x=balancedData, y = balancedLabels,\n",
    "                       validation_data=(valid_balancedData, valid_balancedLabels),\n",
    "                    epochs = EPOCHS, batch_size=batch_size, verbose=1,\n",
    "                    callbacks=[mcp_save,scheduler]) # removed addons.LossHistory(jsonpath) for compatibility with TensorFlow 2.2.0, needs to be re-added at some point\n",
    "\n",
    "        print(\"Max value: \", max(H.history['accuracy']), \" at epoch\", np.argmax(H.history['accuracy']) + 1)\n",
    "\n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, balanced_accuracy_score, f1_score\n",
    "        predictions = model.predict(x=test_Data)\n",
    "        threshold = 0.5\n",
    "        wacc =  balanced_accuracy_score(test_Labels,predictions>=threshold)\n",
    "        f1 =  f1_score(test_Labels,predictions>=threshold)\n",
    "        acc =  accuracy_score(test_Labels,predictions>=threshold)\n",
    "        recall = recall_score(test_Labels,predictions>=threshold)\n",
    "        \n",
    "        #auc = roc_auc_score(test_Labels,predictions>=threshold)\n",
    "        print(\"Weighted Accuracy:\", wacc)\n",
    "        print(\"Test Accuracy:\", acc)\n",
    "        print(\"F1-score:\", f1)\n",
    "        print(\"Recall Accuracy:\", recall)\n",
    "        #print(\"AUC Score:\", auc)\n",
    "\n",
    "        perf[\"model\"].append(\"ActiModel\")\n",
    "        perf[\"data\"].append(name)\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        perf[\"wacc\"].append(wacc)\n",
    "        perf[\"f1\"].append(f1)\n",
    "        perf[\"acc\"].append(acc)\n",
    "        perf[\"recall\"].append(recall)\n",
    "        #perf[\"auc\"].append(auc)\n",
    "        model_ls.append(model)\n",
    "        hist_ls.append(H)\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    print(perf_df)\n",
    "    return perf_df, model_ls, hist_ls\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hysteresis_threshold.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hysteresis_threshold.py\n",
    "from packages import *\n",
    "import os\n",
    "\n",
    "def save_proba(result,path=\"proba.csv\"):\n",
    "    \"\"\"\n",
    "    Save predicted probability to csv files\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=[\"day\",\"proba\",\"labels\"])\n",
    "    for i, day in enumerate(result[\"day\"].unique().tolist()):\n",
    "        dat = {\"day\":[],\"proba\":[],\"labels\":[]}\n",
    "        dat[\"proba\"] = result[\"proba\"].iloc[i]\n",
    "        dat[\"day\"] = [day]*len(result[\"proba\"].iloc[i])\n",
    "        dat[\"labels\"] = result[\"labels\"].iloc[i]\n",
    "        dat = pd.DataFrame(dat)\n",
    "        df =df.append(dat)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(\"File \"+path+\" Saved\")\n",
    "    \n",
    "def load_proba(path):\n",
    "    \"\"\"\n",
    "    Load predicted probability on individual dataset\n",
    "    \"\"\"\n",
    "    # key is day, value is possibility sequence\n",
    "    proba_ls = {}\n",
    "    labels_ls = {}\n",
    "    df = pd.read_csv(path)\n",
    "    for i in df['day'].unique():\n",
    "        probas = df[df['day']==i][\"proba\"].values\n",
    "        labels = df[df['day']==i][\"labels\"].values\n",
    "        proba_ls[i] = probas\n",
    "        labels_ls[i] = labels\n",
    "    return proba_ls,labels_ls\n",
    "    \n",
    "def hysteresis_threshold(model, data,days_ls = [], start_threshold=0.8, end_threshold=0.4, winmin = 6,\n",
    "                        stepsec=5, episode_min = 1.,t_pause = 900,load_proba_flag = True,\n",
    "                         path =\"../results/possibility_results/\", file_name= None):\n",
    "    \"\"\"\n",
    "    model: tensorflow model\n",
    "    data:  This dataset must be the self-defined class of Person_MealsDataset  datasetset in my dataset.py/pytorch dataset without using shuffle. \n",
    "    Keep the order of dataset after extracting window samples!  You can also define your own dataset using class object to create the interface\n",
    "    \n",
    "    start_threshold: the high threshold of the beginning of segmentation\n",
    "    \n",
    "    end_threshold: the end threshold of the end of segmentation\n",
    "    \n",
    "    winmin: size of a window sample in unit of  minute\n",
    "    \n",
    "    stepsec: stride to move the window in unit of second / the number of second between two adjacent window samples\n",
    "    \n",
    "    episode_min: the minimum length of eating episode in unit of minute. If end of segmentation -start of segmentation < episode_min,\n",
    "        then the episode will not be counted\n",
    "    file_name: csv file that store predicted possibility of model\n",
    "    path: path to load / save predicted probability\n",
    "    \n",
    "    load_proba_flag: if enabled,  load saved probability to do hysteresis threshold\n",
    "    \"\"\"\n",
    "    result_ls = []\n",
    "    proba_ls,labels_ls = {}, {}\n",
    "    if file_name == None:\n",
    "        file_name =path+\"{}_{}min_{}slide_proba.csv\".format(data.person_name, winmin,stepsec)\n",
    "    \n",
    "    if load_proba_flag and  os.path.isfile(file_name):\n",
    "        # load generated probability if we already generate it\n",
    "        proba_ls,labels_ls = load_proba(file_name)\n",
    "        pass\n",
    "    else:\n",
    "        # generate possibility for hysteresis threshold if we have not done yet\n",
    "        if not days_ls:\n",
    "            days = set(data.data_indices[:,0])\n",
    "        else:\n",
    "            days = days_ls\n",
    "        #pbar = tqdm(days, total=len(days))\n",
    "        for day in days:\n",
    "            # Select and Extract the data and labels of the corresponding day from the whole dataset\n",
    "            sample_indices= np.where(data.data_indices[:,0]==day)[0]\n",
    "\n",
    "            # get the numpy array of samples and labels\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            samples, labels = data.get_subset(sample_indices)\n",
    "            #print(\"--- Get data:  %s seconds ---\" % (time.time() - start_time))\n",
    "            probas = model(samples).numpy().squeeze()\n",
    "            #print(\"--- Prediction %s seconds ---\" % (time.time() - start_time))\n",
    "            print(\"--- Day %d: %s seconds ---\" % (day, time.time() - start_time))\n",
    "            proba_ls[day] = probas\n",
    "            labels_ls[day] = labels\n",
    "        df = {}\n",
    "        df[\"day\"] = list(days)\n",
    "#         day_key = list(proba_ls.keys())\n",
    "#         day_key.sort()\n",
    "        df[\"proba\"]= [proba_ls[k] for k in days]\n",
    "        df['labels'] = [labels_ls[k] for k in days]\n",
    "        df = pd.DataFrame(df)\n",
    "        save_proba(df,path=file_name)   \n",
    "\n",
    "    if not days_ls:\n",
    "        days = set(data.data_indices[:,0])\n",
    "    else:\n",
    "        days = days_ls\n",
    "    pbar = tqdm(days, total=len(days))\n",
    "    for day in pbar:\n",
    "        \n",
    "        # Select and Extract the data and labels of the corresponding day from the whole dataset\n",
    "        sample_indices= np.where(data.data_indices[:,0]==day)[0]\n",
    "        \n",
    "        probas = proba_ls[day]\n",
    "        labels = labels_ls[day]\n",
    "        result = {'day':day,\"stepsec\": stepsec,'segment_start':[], 'segment_end':[],'proba':[],'predictions':np.zeros([len(sample_indices)]),'labels':[],\"segment_count\":0}\n",
    "        state = 0\n",
    "        start = 0\n",
    "        end = 0 \n",
    "        pause_counter = 0\n",
    "        # one day data\n",
    "        #print(\"Day: \",day)\n",
    "        for i in range(len(sample_indices)):\n",
    "            #print(\"i:\",i)\n",
    "            #sample, label = data[i][0].numpy(),data[i][1]\n",
    "            #sample = np.expand_dims(sample,axis=0)\n",
    "            #proba = model(sample).numpy()[0][0]\n",
    "            #sample = samples[i]\n",
    "            label = labels[i]\n",
    "            proba = probas[i]\n",
    "            \n",
    "            result['proba'].append(proba)\n",
    "            result['labels'].append(label)\n",
    "            \n",
    "            if state ==0 and proba > start_threshold:\n",
    "                state = 1\n",
    "                start = i\n",
    "            elif state == 1 and proba <end_threshold:\n",
    "                state = 2\n",
    "                end = i+1\n",
    "                pause_counter = 0\n",
    "            elif state ==2:\n",
    "                if proba > start_threshold:\n",
    "                    state = 1\n",
    "                else:\n",
    "                    pause_counter += stepsec\n",
    "                    if pause_counter >= t_pause:\n",
    "                        # convert time to second and check threshold\n",
    "                        if (end-start)*stepsec >= episode_min*60:\n",
    "                            # save data\n",
    "                            result['segment_start'].append(start)\n",
    "                            result['segment_end'].append(end)\n",
    "                            result['segment_count'] += 1\n",
    "                            result['predictions'][start:end] = 1\n",
    "                            pass\n",
    "                        end = 0\n",
    "                        state = 0\n",
    "        if state != 0:\n",
    "            # if segment ended at the end of data\n",
    "            if end != 0:\n",
    "                result['segment_start'].append(start)\n",
    "                result['segment_end'].append(end)\n",
    "                result['predictions'][start:end] = 1\n",
    "            else:\n",
    "                result['segment_count'] -= 1  \n",
    "            result['segment_count'] += 1\n",
    "#         print(\"--- One Day: %s seconds ---\" % (time.time() - start_time))    \n",
    "        result_ls.append(result)\n",
    "    print(\"Segmentation Completed. \")\n",
    "    result_ls = pd.DataFrame(result_ls)\n",
    "                      \n",
    "    return result_ls\n",
    "\n",
    "\n",
    "\n",
    "def get_episode_metrics(result, meal_data,days_ls= None):\n",
    "    \"\"\"\n",
    "    Obtain and format the episode metric results \n",
    "    \n",
    "    result: result from  hysteresis threshold function\n",
    "    meal_data: meal dataset of Person_MealData\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    perf = {\"TPR\":[],\"FP/TP\":[],\"TP\":[], \"FP\":[],\"FN\":[]}\n",
    "    tpr = 0. \n",
    "    FP_TP = 0.\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    # get segmentation of ground truth labeled by user\n",
    "    start_ls, end_ls = meal_data.get_GT_segment()\n",
    "    if days_ls:\n",
    "        start_ls = [start_ls[day] for day in days_ls]\n",
    "        end_ls =  [end_ls[day] for day in days_ls]\n",
    "    \n",
    "    meal_counts, min_counts,hour_counts, day_counts,total_hours = meal_data.get_mealdataset_info()\n",
    "    \n",
    "    # iterate every day\n",
    "    for i in range(len(start_ls)):\n",
    "        #preds = result.iloc[i]['predictions']\n",
    "        #labels =  result.iloc[i]['labels']\n",
    "        event_start, event_end= start_ls[i], end_ls[i]\n",
    "        detect_start, detect_end = result.iloc[i]['segment_start'],result.iloc[i]['segment_end']\n",
    "        GT = np.array([-1]*len(event_start) )  # default all meals are missing -1, FN\n",
    "        detect = np.array([-1]*len(detect_start)) # default all detected meals are wrong -1, FP\n",
    "        for index in range(len(event_start)):\n",
    "            # e_s: event start,  e_e: event end\n",
    "            # d_s: detection start,  d_e: detection end\n",
    "            e_s, e_e = event_start[index], event_end[index]\n",
    "            for index2 in range(len(detect_start)):\n",
    "                # convert segment from sec to index of data point\n",
    "                d_s = detect_start[index2] * result.iloc[i]['stepsec']*15\n",
    "                d_e = detect_end[index2]* result.iloc[i]['stepsec']*15\n",
    "                #print(\"ds: {} d_e: {}, e_s:{}, e_e: {}\".format(d_s,d_e, e_s, e_e))\n",
    "                if (e_s>=d_s and e_s <= d_e) or (d_s>= e_s and d_s<= e_e):\n",
    "                    GT[index] = index2\n",
    "                    detect[index2] = index\n",
    "        #print(\"GT:\",GT, \"Detect:\", detect)\n",
    "        TP += sum(GT!=-1)\n",
    "        FN += sum(GT==-1)\n",
    "        FP += sum(detect==-1)\n",
    "                \n",
    "    \n",
    "    print(\"total_meal:\",meal_counts, \"TP: \", TP, \"FP: \", FP, \"FN: \", FN)\n",
    "    perf['TPR'].append(TP/(TP+FN) if (TP+FN)>0 else 0)\n",
    "    if TP ==0:\n",
    "        perf['FP/TP'].append(None)\n",
    "    else:\n",
    "        perf['FP/TP'].append(FP/TP)\n",
    "    perf[\"TP\"].append(TP)\n",
    "    perf[\"FP\"].append(FP)\n",
    "    perf[\"FN\"].append(FN)\n",
    "    result_df = pd.DataFrame(perf)\n",
    "        \n",
    "    return pd.DataFrame(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils to measure and print  Time/Episode Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting metrics.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile metrics.py\n",
    "\n",
    "from packages import *\n",
    "from collections import defaultdict\n",
    "\n",
    "def weight_accuracy(y_true, y_pred,weight, print_flag=True):\n",
    "    TP = sum( (y_true==1) &(y_pred==1) )\n",
    "    FN = sum( (y_true==1) &(y_pred==0) )\n",
    "    TN = sum( (y_true==0) &(y_pred==0) )\n",
    "    FP = sum( (y_true==0) &(y_pred==1) )\n",
    "    if print_flag:\n",
    "        print(\"TP: \",TP, \"FP: \",FP, \"TN: \",TN, \"FN: \",FN)\n",
    "    return (weight*TP + TN)/(weight*(TP+FN) + (TN+FP))\n",
    "\n",
    "def test_models_time_metric(balance_ratio,winmin=1, stridesec = 5,names= [\"wenkanw\"],random_seed=1000, split_day=False, test_balance=False, test_CAD=False):\n",
    "    perf = defaultdict(list)\n",
    "    meal_info = defaultdict(list)\n",
    "    # Don't test on CAD test set if it is not enabled\n",
    "    if not test_CAD and \"CAD\" in names:\n",
    "        names.remove(\"CAD\")\n",
    "        \n",
    "    for name in names:\n",
    "        person = name\n",
    "        if split_day:\n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "\n",
    "            # balance test set\n",
    "            testset_labels = meal_data.labels\n",
    "            if test_balance:\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices=[i for i in range(len(meal_data))] ,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                test_indices = [i for i in range(len(meal_data))]\n",
    "            # get numpy dataset\n",
    "            test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "        else:            \n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "            samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "            # split train set and test set\n",
    "            train_indices, test_indices = split_train_test_indices(X= [i for i in range(len(labels))],\n",
    "                                                                            y = labels, test_size = 0.2,\n",
    "                                                                           random_seed = random_seed)\n",
    "            \n",
    "            if test_balance:\n",
    "                testset_labels = labels[test_indices]\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices= test_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                test_indices = test_indices\n",
    "            testset_labels = labels[test_indices]\n",
    "            print(\"Testing on : \", sum(testset_labels==1),\"positive samples, \",sum(testset_labels==0),\" negative samples\" )\n",
    "            test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "        if name !=\"CAD\":\n",
    "            meal_counts, min_counts,hour_counts, day_counts,total_hours = meal_data.get_mealdataset_info(person_name=name, file_ls_doc=\"all_files_list.txt\")\n",
    "        else:\n",
    "            # data from paper\n",
    "            meal_counts, min_counts,hour_counts, day_counts,total_hours = 1063, 250*60,250, 354, 4680\n",
    "        meal_info[\"dataset\"].append(name)\n",
    "        meal_info[\"Days\"].append(day_counts)\n",
    "        meal_info[\"Meal_Hours\"].append(round(hour_counts,1)) \n",
    "        meal_info[\"Meal_Counts\"].append(meal_counts) \n",
    "        meal_info[\"Total_Hours\"].append(total_hours) \n",
    "        \n",
    "        perf[\"dataset\"].append(name)\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, f1_score, balanced_accuracy_score, precision_score\n",
    "        group_model_W  = tf.keras.models.load_model('../models/CAD_models/acti_6min_M_F_6.000000Min.h5')\n",
    "        \n",
    "        if name != \"CAD\":\n",
    "            if split_day:\n",
    "                individual_model = tf.keras.models.load_model('../models/'+ name+ '_models/acti_6min_split_day_M_F_6.000000Min.h5')\n",
    "            else:    \n",
    "                individual_model = tf.keras.models.load_model('../models/'+ name+ '_models/acti_6min_M_F_6.000000Min.h5')\n",
    "        models = {\"suffix\":['Individual-Model','GroupModel'],  \"model\":[individual_model,group_model_W]}\n",
    "        \n",
    "        for i in range(len(models[\"suffix\"])):\n",
    "            suffix = models[\"suffix\"][i]\n",
    "            model = models[\"model\"][i]\n",
    "            # if the dataset is CAD group dataset and model is individual model\n",
    "            # we don't need to make prediction on that data\n",
    "            if name == \"CAD\" and suffix =='Individual-Model':\n",
    "                acc = None\n",
    "                auc = None\n",
    "                recall = None\n",
    "            else:\n",
    "\n",
    "                predictions = model.predict(x=test_Data).squeeze(1)\n",
    "                threshold = 0.5\n",
    "                prediction = (predictions>=threshold).astype(int)\n",
    "                wacc =  balanced_accuracy_score(test_Labels,prediction)\n",
    "                acc =  accuracy_score(test_Labels,prediction)\n",
    "                recall = recall_score(test_Labels,prediction)\n",
    "                f1 = f1_score(test_Labels,prediction)\n",
    "                precision = precision_score(test_Labels,prediction)\n",
    "                print(\"Test label: \",test_Labels)\n",
    "                print(\"Predictions:\",prediction)\n",
    "                \n",
    "                # weighted accuracy 2 is computed by (weight*TP +TN)/(weight*(TP+FN) + (TN+FP))\n",
    "                wacc2 = weight_accuracy(test_Labels,prediction, weight=balance_ratio)\n",
    "            \n",
    "            \n",
    "            print(\"Weighted Accuracy:\", wacc)\n",
    "            print(\"Weighted Accuracy2:\", wacc2)\n",
    "            print(\"Recall:\", recall)\n",
    "            print(\"Precision:\", precision)\n",
    "            print(\"F1:\", f1)\n",
    "            print(\"Test Accuracy:\", acc)\n",
    "            \n",
    "            perf[\"WAcc: \"+suffix].append(wacc)\n",
    "            perf[\"WAcc2: \"+suffix].append(wacc2)\n",
    "            perf[\"Recall: \"+suffix].append(recall)\n",
    "            perf[\"Precision: \"+suffix].append(precision)\n",
    "            perf[\"F1: \"+suffix].append(f1)\n",
    "            perf[\"Acc: \"+suffix].append(acc)\n",
    "\n",
    "    meal_info = pd.DataFrame(meal_info)\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    return meal_info, perf_df\n",
    "\n",
    "\n",
    "\n",
    "def print_time_metrics(result, old_result = None,round_decimal = 3,):\n",
    "    perf_df = pd.DataFrame()\n",
    "    mykeys = [\"dataset\",\"win(sec)\",\"WAcc\", \"F1\",\"Precision\",\"Recall\"]\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']    \n",
    "    for k in mykeys:\n",
    "        for key in result.keys():\n",
    "            if k in key:\n",
    "                perf_df[key] = result[key]\n",
    "                if result[key].dtype in numerics:\n",
    "                    perf_df[key] = np.round(perf_df[key],round_decimal)\n",
    "    for key in result.keys():\n",
    "        if \"WAcc\" not in key and \"Acc\" in key:\n",
    "            perf_df[key] = result[key].values.round(round_decimal)\n",
    "    \n",
    "    if not isinstance(old_result, type(None)):\n",
    "        # remove average performance from old results and append new data to table\n",
    "        drop_vals = result[\"dataset\"].values.tolist()\n",
    "        drop_vals.append(\"average performance\")\n",
    "        idx = []\n",
    "        for i in range(len(old_result)):\n",
    "            if old_result['dataset'].iloc[i] in drop_vals:\n",
    "                idx.append(i)\n",
    "                \n",
    "        #idx =old_result[(old_result['dataset']==\"average performance\") | (old_result['dataset']==result[\"dataset\"].values[0])].index\n",
    "        print(\"index\",idx)\n",
    "        new_result = old_result.drop(index=idx,axis=0)\n",
    "        perf_df= new_result.append(perf_df, ignore_index=True)\n",
    "        \n",
    "    mean_perf = pd.DataFrame(columns = perf_df.keys())\n",
    "    mean_perf = mean_perf.append({\"dataset\":\"average performance\",\"win(sec)\":\"-\"},ignore_index=True)\n",
    "    for key in perf_df.keys():\n",
    "        if key.lower() != \"dataset\" and  key.lower() != \"win(sec)\":\n",
    "            mean_perf[key].at[0] = perf_df[key].mean().round(round_decimal)\n",
    "    \n",
    "    perf_df = perf_df.append(mean_perf,ignore_index=True)\n",
    "            \n",
    "    return perf_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "# Episode metric\n",
    "##########################\n",
    "\n",
    "hythreshold = {}\n",
    "for n in ['wenkanw','adam','lawler','shaurya']:\n",
    "    hythreshold[n]= [0.8, 0.4]\n",
    "def test_models_episode_metric(winmin=6, stridesec = 5,names= [\"wenkanw\"],random_seed=1000,\n",
    "                               test_balance=False, test_CAD=False,\n",
    "                               test_alldata=False,threshold= hythreshold,\n",
    "                               load_proba_flag=True, use_group_threshold = 0,\n",
    "                              proba_path =\"../results/possibility_results/\"):\n",
    "    perf = defaultdict(list)\n",
    "    meal_info = defaultdict(list)\n",
    "    \n",
    "    # Don't test on CAD test set if it is not enabled\n",
    "    if not test_CAD and \"CAD\" in names:\n",
    "        names.remove(\"CAD\")\n",
    "    group_threshold = {'wenkanw':[0.8, 0.4], 'adam':[0.8,0.4],'lawler':[0.8,0.4], 'shaurya':[0.8,0.4]}\n",
    "    for name in names:\n",
    "        person = name\n",
    "        # test episode metrics that split dataset by days\n",
    "        if not test_alldata:\n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "        else:\n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "        # balance test set    \n",
    "        if test_balance:\n",
    "            testset_labels = meal_data.labels\n",
    "            test_indices = balance_data_indices(testset_labels,data_indices=[i for i in range(len(meal_data))] ,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "        else:\n",
    "            test_indices = [i for i in range(len(meal_data))]\n",
    "        # get numpy dataset\n",
    "        #test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "        \n",
    "        \n",
    "        meal_counts, min_counts,hour_counts, day_counts,total_hours = meal_data.get_mealdataset_info(person_name=name)\n",
    "                \n",
    "        \n",
    "        perf[\"Days\"].append(day_counts)\n",
    "        perf[\"Meal_Hours\"].append(round(hour_counts,1)) \n",
    "        perf[\"Meal_Counts\"].append(meal_counts) \n",
    "        perf[\"dataset\"].append(name)\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        \n",
    "            \n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, f1_score, balanced_accuracy_score, precision_score\n",
    "        group_model_W  = tf.keras.models.load_model('../models/CAD_models/acti_6min_M_F_6.000000Min.h5')\n",
    "        \n",
    "        individual_model = tf.keras.models.load_model('../models/'+ name+ '_models/acti_6min_split_day_M_F_6.000000Min.h5')\n",
    "            \n",
    "        models = {\"suffix\":['Individual-Model','GroupModel'],  \"model\":[individual_model,group_model_W]}\n",
    "        proba_path += person+\"/\"\n",
    "        for i in range(len(models[\"suffix\"])):\n",
    "            suffix = models[\"suffix\"][i]\n",
    "            model = models[\"model\"][i]\n",
    "            # if the dataset is CAD group dataset and model is individual model\n",
    "            # we don't need to make prediction on that data\n",
    "            path= proba_path\n",
    "            if use_group_threshold==0:\n",
    "                high_th, low_th = threshold[name][0], threshold[name][1]\n",
    "            elif use_group_threshold==1:\n",
    "                if suffix == \"GroupModel\":\n",
    "                    high_th, low_th = group_threshold[name][0], group_threshold[name][1]\n",
    "                else:\n",
    "                    high_th, low_th = threshold[name][0], threshold[name][1]\n",
    "            else:\n",
    "                high_th, low_th = group_threshold[name][0], group_threshold[name][1]\n",
    "                \n",
    "            \n",
    "            if suffix == \"GroupModel\":\n",
    "                path = proba_path +\"group_\"\n",
    "                \n",
    "                \n",
    "            result = hysteresis_threshold(model, meal_data,start_threshold=high_th, end_threshold=low_th,\n",
    "                                          winmin = 6, stepsec=5, episode_min = 1.,\n",
    "                                         load_proba_flag=load_proba_flag, path =path)\n",
    "            episode_perf_df = get_episode_metrics(result,meal_data)\n",
    "            perf[\"TPR: \"+suffix].append(episode_perf_df[\"TPR\"].iloc[0])\n",
    "            perf[\"FP/TP: \"+suffix].append(episode_perf_df[\"FP/TP\"].iloc[0])\n",
    "            perf[\"TP: \"+suffix].append(episode_perf_df[\"TP\"].iloc[0])\n",
    "            perf[\"FP: \"+suffix].append(episode_perf_df[\"FP\"].iloc[0])\n",
    "            perf[\"FN: \"+suffix].append(episode_perf_df[\"FN\"].iloc[0])\n",
    "            \n",
    "            print(episode_perf_df)\n",
    "\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    return perf_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def print_episode_metrics(result,old_result= None, round_decimal=3):\n",
    "    \"\"\"\n",
    "    print the episode_perf_df result from test_models_episode_metric\n",
    "    in suitable order\n",
    "    \"\"\"\n",
    "    result_df= result[['dataset','Days' ,'Meal_Hours',\"Meal_Counts\",\"win(sec)\",\n",
    "            \"TPR: Individual-Model\",\"TPR: GroupModel\",\"FP/TP: Individual-Model\",\"FP/TP: GroupModel\",\n",
    "           \"TP: Individual-Model\",\"TP: GroupModel\",\n",
    "           \"FP: Individual-Model\",\"FP: GroupModel\",\n",
    "           \"FN: Individual-Model\",\"FN: GroupModel\"]]\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']   \n",
    "    for key in result_df.columns:\n",
    "        if result_df[key].dtype in numerics:\n",
    "            #result_df[key] = np.round(result_df[key].values, round_decimal) \n",
    "            for i in range(len(result_df[key])):\n",
    "                result_df[key].at[i] = result_df[key].iloc[i].round(round_decimal)\n",
    "            \n",
    "    if not isinstance(old_result, type(None)):\n",
    "        # remove average performance from old results and append new data to table\n",
    "        drop_vals = result[\"dataset\"].values.tolist()\n",
    "        drop_vals.append(\"average performance\")\n",
    "        idx = []\n",
    "        for i in range(len(old_result)):\n",
    "            if old_result['dataset'].iloc[i] in drop_vals:\n",
    "                idx.append(i)\n",
    "        \n",
    "        #idx =old_result[(old_result['dataset']==\"average performance\") | (old_result['dataset'].values == result[\"dataset\"].values)].index\n",
    "        print(\"index\",idx)\n",
    "        new_result = old_result.drop(index=idx,axis=0)\n",
    "        result_df= new_result.append(result_df, ignore_index=True)\n",
    "    \n",
    "    mean_perf = pd.DataFrame(columns = result_df.keys())\n",
    "    mean_perf = mean_perf.append({\"dataset\":\"average performance\",\"win(sec)\":\"-\",'Days':\"-\" ,\n",
    "                                  'Meal_Hours':\"-\",\"Meal_Counts\":\"-\"},ignore_index=True)\n",
    "    for key in mean_perf.keys():\n",
    "        if key not in ['dataset','Days' ,'Meal_Hours',\"Meal_Counts\",\"win(sec)\"]:\n",
    "            mean_perf[key].at[0] = result_df[key].mean().round(round_decimal)\n",
    "    \n",
    "    result_df = result_df.append(mean_perf,ignore_index=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cross_validation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cross_validation.py\n",
    "\n",
    "from packages import *\n",
    "from hysteresis_threshold import *\n",
    "from dataset import *\n",
    "from collections import defaultdict\n",
    "from metrics import *\n",
    "\n",
    "def cross_validation_metrics(model_arch,balance_ratio,names= [\"wenkanw\"], fold_num = 5, winmin=6, stridesec = 5,model_name= \"acti_model\",epochs = 20,\n",
    "                                 random_seed=1000, split_day=False, test_balance=False, re_train = False,load_data=True,\n",
    "                                 test_CAD=False, metrics =['time','episode'], ind_threshold= None,load_proba_flag=True):\n",
    "    import os\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, f1_score, balanced_accuracy_score, precision_score\n",
    "    batch_size = 128\n",
    "    time_perf = defaultdict(list)\n",
    "    episode_perf = defaultdict(list)\n",
    "    meal_info = defaultdict(list)\n",
    "    time_individual_perf = defaultdict(list)\n",
    "    episode_individual_perf = defaultdict(list)\n",
    "    time_group_perf = defaultdict(list)\n",
    "    episode_group_perf = defaultdict(list)\n",
    "    \n",
    "    group_threshold = [0.8,0.4] \n",
    "                \n",
    "    model = None\n",
    "    kf = KFold(n_splits=5, random_state= 1000,shuffle=False)\n",
    "    # Don't test on CAD test set if it is not enabled\n",
    "    if not test_CAD and \"CAD\" in names:\n",
    "        names.remove(\"CAD\")\n",
    "    \n",
    "    \n",
    "    for name in names:\n",
    "        #load individual whole dataset\n",
    "        person = name\n",
    "        meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "        samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "            \n",
    "        if name !=\"CAD\":\n",
    "            meal_counts, min_counts,hour_counts, day_counts,total_hours = meal_data.get_mealdataset_info(person_name=name, file_ls_doc=\"all_files_list.txt\")\n",
    "        else:\n",
    "            # data from paper\n",
    "            meal_counts, min_counts,hour_counts, day_counts,total_hours = 1063, 250*60,250, 354, 4680\n",
    "        meal_info[\"dataset\"].append(name)\n",
    "        meal_info[\"Days\"].append(day_counts)\n",
    "        meal_info[\"Meal_Hours\"].append(round(hour_counts,1)) \n",
    "        meal_info[\"Meal_Counts\"].append(meal_counts) \n",
    "        meal_info[\"Total_Hours\"].append(total_hours) \n",
    "        \n",
    "        time_perf[\"dataset\"].append(name)\n",
    "        time_perf[\"win(sec)\"].append(winmin*60)\n",
    "        \n",
    "        episode_perf[\"dataset\"].append(name)\n",
    "        episode_perf[\"win(sec)\"].append(winmin*60)\n",
    "        \n",
    "        days = np.unique(meal_data.data_indices[:,0])\n",
    "        samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "#         samples,labels = meal_data.get_subset([i for i in range(len(meal_data.labels))])\n",
    "\n",
    "        # K-fold cross validation\n",
    "        for fold, (day_train_idx, day_test_idx) in enumerate(kf.split(days)):\n",
    "            print(\"Fold: %d\"%(fold),\"Train on days: \",day_train_idx, \"Test on days: \",day_test_idx)\n",
    "            day_train_idx = day_train_idx.tolist()\n",
    "            day_test_idx = day_test_idx.tolist()\n",
    "            train_indices = []\n",
    "            test_indices = []\n",
    "            # partition dataset by days\n",
    "            for i, day in enumerate(meal_data.data_indices[:,0]):\n",
    "                if day in day_train_idx:\n",
    "                    train_indices.append(i)\n",
    "                else:\n",
    "                    test_indices.append(i)\n",
    "#             print(\"Train indices: \", train_indices)\n",
    "#             print(\"Test indices: \", test_indices)\n",
    "#             assert False\n",
    "            # balance train set\n",
    "            trainset_labels = labels[train_indices]\n",
    "            train_indices_balanced = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            \n",
    "            testset_labels = labels[test_indices]\n",
    "            if test_balance:\n",
    "                #balance test set\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices= test_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                test_indices = test_indices \n",
    "                   \n",
    "            \n",
    "            # split validation set\n",
    "            balanced_trainset_labels = labels[train_indices_balanced]\n",
    "            train_indices, valid_indices = split_train_test_indices(X= train_indices_balanced,\n",
    "                                                                    y = balanced_trainset_labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "                    \n",
    "            if not load_data and \"time\" not in metrics:\n",
    "                balancedData, balancedLabels = meal_data.get_subset([])\n",
    "                valid_balancedData, valid_balancedLabels = meal_data.get_subset([])\n",
    "                test_Data, test_Labels = meal_data.get_subset([])\n",
    "            else:\n",
    "                # Get numpy dataset: balanced trainset, validation set, test set\n",
    "                balancedData, balancedLabels = meal_data.get_subset(train_indices)\n",
    "                valid_balancedData, valid_balancedLabels = meal_data.get_subset(valid_indices)\n",
    "                test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "\n",
    "                # balancedData, balancedLabels = samples[train_indices],labels[train_indices]  \n",
    "                # valid_balancedData, valid_balancedLabels = samples[valid_indices],labels[valid_indices] \n",
    "                # test_Data, test_Labels = samples[test_indices],labels[test_indices]\n",
    "\n",
    "                print(\"Train on : \", sum(balancedLabels==1),\"positive samples, \",sum(balancedLabels==0),\" negative samples\" )\n",
    "                print(\"Testing on : \", sum(valid_balancedLabels==1),\"positive samples, \",sum(valid_balancedLabels==0),\" negative samples\" )\n",
    "                print(\"Testing on : \", sum(test_Labels==1),\"positive samples, \",sum(test_Labels==0),\" negative samples\" )\n",
    "            \n",
    "            \n",
    "            #train models\n",
    "            pathtemp = \"../models/\" + name+\"_models\" +\"/\"+\"cv_fold_\"+str(fold) +\"_\"+model_name+\"_M_F_\"\n",
    "            modelpath = pathtemp + \"{:f}Min.h5\".format(winmin)\n",
    "            jsonpath = pathtemp + \"{:f}Min.json\".format(winmin)\n",
    "            \n",
    "            # if model doesn't exist or re_train is enabled, then re_trian\n",
    "            # otherwise, just load model\n",
    "            if not os.path.isfile(modelpath) or re_train:\n",
    "                #training settings\n",
    "                win_size = 15*winmin*60\n",
    "                model =model_arch(input_shape =(win_size,6) )\n",
    "                model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                \n",
    "                mcp_save = tf.keras.callbacks.ModelCheckpoint(modelpath, save_best_only=True, monitor='val_accuracy')\n",
    "                scheduler = tf.keras.callbacks.ReduceLROnPlateau( monitor='val_loss', factor=0.1, patience=3, verbose=0,\n",
    "                                                     mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.)\n",
    "                ##########train model ###############\n",
    "                H = model.fit(x=balancedData, y = balancedLabels,\n",
    "                               validation_data=(valid_balancedData, valid_balancedLabels),\n",
    "                            epochs = epochs, batch_size=batch_size, verbose=1,\n",
    "                            callbacks=[mcp_save,scheduler]) # removed addons.LossHistory(jsonpath) for compatibility with TensorFlow 2.2.0, needs to be re-added at some point\n",
    "\n",
    "                print(\"Max value: \", max(H.history['accuracy']), \" at epoch\", np.argmax(H.history['accuracy']) + 1)\n",
    "                print(\"Model saved to path: \",modelpath)\n",
    "            else:\n",
    "                model  = tf.keras.models.load_model(modelpath)\n",
    "                \n",
    "            # obtain individual model and pre-trained group model\n",
    "            individual_model = model\n",
    "            group_model_W  = tf.keras.models.load_model('../models/CAD_models/acti_6min_M_F_6.000000Min.h5')\n",
    "            models = {\"suffix\":['Individual-Model','GroupModel'],  \"model\":[individual_model,group_model_W]}\n",
    "            \n",
    "            ##### Test models####\n",
    "            ############ Time metrics ##########\n",
    "            if 'time' in metrics:\n",
    "                for i in range(len(models[\"suffix\"])):\n",
    "                    suffix = models[\"suffix\"][i]\n",
    "                    model = models[\"model\"][i]\n",
    "                    # if the dataset is CAD group dataset and model is individual model\n",
    "                    # we don't need to make prediction on that data\n",
    "                    if name == \"CAD\" and suffix =='Individual-Model':\n",
    "                        acc = None\n",
    "                        auc = None\n",
    "                        recall = None\n",
    "                    else:\n",
    "\n",
    "                        predictions = model.predict(x=test_Data).squeeze(1)\n",
    "                        threshold = 0.5\n",
    "                        prediction = (predictions>=threshold).astype(int)\n",
    "                        wacc =  balanced_accuracy_score(test_Labels,prediction)\n",
    "                        acc =  accuracy_score(test_Labels,prediction)\n",
    "                        recall = recall_score(test_Labels,prediction)\n",
    "                        f1 = f1_score(test_Labels,prediction)\n",
    "                        precision = precision_score(test_Labels,prediction)\n",
    "                        # weighted accuracy 2 is computed by (weight*TP +TN)/(weight*(TP+FN) + (TN+FP))\n",
    "                        wacc2 = weight_accuracy(test_Labels,prediction,weight = balance_ratio[name],print_flag=False)\n",
    "                        \n",
    "                        # store performance for one fold \n",
    "                        if suffix == \"GroupModel\":\n",
    "                            time_group_perf[\"WAcc: \"+suffix].append(wacc)\n",
    "                            time_group_perf[\"WAcc2: \"+suffix].append(wacc2)\n",
    "                            time_group_perf[\"Recall: \"+suffix].append(recall)\n",
    "                            time_group_perf[\"Precision: \"+suffix].append(precision)\n",
    "                            time_group_perf[\"F1: \"+suffix].append(f1)\n",
    "                            time_group_perf[\"Acc: \"+suffix].append(acc)\n",
    "                        else:\n",
    "                            time_individual_perf[\"WAcc: \"+suffix].append(wacc)\n",
    "                            time_individual_perf[\"WAcc2: \"+suffix].append(wacc2)\n",
    "                            time_individual_perf[\"Recall: \"+suffix].append(recall)\n",
    "                            time_individual_perf[\"Precision: \"+suffix].append(precision)\n",
    "                            time_individual_perf[\"F1: \"+suffix].append(f1)\n",
    "                            time_individual_perf[\"Acc: \"+suffix].append(acc)\n",
    "                            \n",
    "\n",
    "            ######## episode metric ############\n",
    "            if \"episode\" in metrics:\n",
    "                proba_path =\"../results/possibility_results/\"+person +\"/cv_fold_\"+str(fold)+\"_\"\n",
    "                for i in range(len(models[\"suffix\"])):\n",
    "                    suffix = models[\"suffix\"][i]\n",
    "                    model = models[\"model\"][i]\n",
    "                    result_path = proba_path\n",
    "                    if suffix == \"GroupModel\":\n",
    "                        result_path += \"group_\"\n",
    "                        high_th, low_th  = group_threshold[0],group_threshold[1]\n",
    "                    else:\n",
    "                        if ind_threshold:\n",
    "                            high_th, low_th  = ind_threshold[name][0],ind_threshold[name][1] \n",
    "                        else:\n",
    "                            high_th, low_th  = group_threshold[0],group_threshold[1]\n",
    "                        \n",
    "                    result = hysteresis_threshold(model, meal_data,days_ls = day_test_idx,start_threshold=high_th, end_threshold=low_th,\n",
    "                                                  winmin = winmin, stepsec=stridesec, episode_min = 1.,\n",
    "                                                 load_proba_flag=load_proba_flag, path =result_path)\n",
    "                    episode_perf_df = get_episode_metrics(result,meal_data,days_ls = day_test_idx)\n",
    "                    \n",
    "                    TP = episode_perf_df[\"TP\"].iloc[0]\n",
    "                    FP = episode_perf_df[\"FP\"].iloc[0]\n",
    "                    FN = episode_perf_df[\"FN\"].iloc[0]\n",
    "                    if suffix == \"GroupModel\":\n",
    "                        episode_group_perf[\"TP: \"+suffix].append(TP)\n",
    "                        episode_group_perf[\"FP: \"+suffix].append(FP)\n",
    "                        episode_group_perf[\"FN: \"+suffix].append(FN)\n",
    "                    else:\n",
    "                        episode_individual_perf[\"TP: \"+suffix].append(TP)\n",
    "                        episode_individual_perf[\"FP: \"+suffix].append(FP)\n",
    "                        episode_individual_perf[\"FN: \"+suffix].append(FN)\n",
    "\n",
    "        if 'time' in metrics:\n",
    "            for key in  time_group_perf.keys():\n",
    "                time_perf[key].append(  np.mean(time_group_perf[key])) \n",
    "                time_group_perf[key].clear()\n",
    "                \n",
    "            for key in  time_individual_perf.keys():\n",
    "                time_perf[key].append(  np.mean(time_individual_perf[key])) \n",
    "                time_individual_perf[key].clear()\n",
    "                \n",
    "        if 'episode' in metrics:\n",
    "            for key in episode_group_perf.keys():\n",
    "                episode_perf[key].append(  np.sum(episode_group_perf[key])) \n",
    "                episode_group_perf[key].clear()\n",
    "            for key in episode_individual_perf.keys():\n",
    "                episode_perf[key].append(  np.sum(episode_individual_perf[key])) \n",
    "                episode_individual_perf[key].clear()\n",
    "            \n",
    "    meal_info = pd.DataFrame(meal_info)\n",
    "    episode_perf = pd.DataFrame(episode_perf)\n",
    "    time_perf = pd.DataFrame(time_perf)\n",
    "    # Compute TPR, FP/TP for all models\n",
    "    for suffix in ['Individual-Model','GroupModel']:\n",
    "            episode_perf[\"TPR: \"+suffix] = episode_perf['TP: '+suffix]/(episode_perf['TP: '+suffix] + episode_perf['FN: '+suffix])\n",
    "            episode_perf['FP/TP: '+suffix] = episode_perf['FP: '+suffix]/episode_perf['TP: '+suffix]\n",
    "            \n",
    "    return meal_info, time_perf,episode_perf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_threshold_cv(datasets ,ts_ls=[],te_ls=[], fold_num=5,round_num = 3,path_name = \"../results/possibility_results/\"):\n",
    "    \"\"\"\n",
    "    Test the hysteresis threshold values  for individual models based on generated possibility in csv files from hysteresis_threshold function\n",
    "    \n",
    "    datasets:  a dictionary of datasets generated from create_dataset() in dataset.py\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import KFold\n",
    "    kf = KFold(n_splits=5, random_state= 1000,shuffle=False)\n",
    "    res = pd.DataFrame()\n",
    "    \n",
    "    for person in datasets.keys():\n",
    "        data = datasets[person]\n",
    "        for ts in ts_ls:\n",
    "            for te in te_ls:\n",
    "                TP,FN, FP = 0,0,0\n",
    "                df= {'dataset':[],'Ts':[],\"Te\":[],'TP':[],'FN':[],\"FP\":[],'TPR':[],'FP/TP':[]}\n",
    "                days = np.unique(data.data_indices[:,0])\n",
    "                samples,labels =  data.data_indices,data.labels\n",
    "                # K-fold cross validation\n",
    "                for fold, (day_train_idx, day_test_idx) in enumerate(kf.split(days)):\n",
    "                    day_train_idx, day_test_idx =day_train_idx.tolist(), day_test_idx.tolist()\n",
    "                    result_path =path_name +person+\"/cv_fold_\"+str(fold)+\"_\"\n",
    "                    model_name = '../models/{}_models/cv_fold_{}_acti_model_M_F_6.000000Min.h5'.format(person, fold)\n",
    "                    model = tf.keras.models.load_model(model_name)\n",
    "                    ht_result = hysteresis_threshold(model, data,days_ls= day_test_idx,start_threshold=ts, end_threshold=te, \n",
    "                                  winmin = 6, stepsec=5, episode_min = 1., load_proba_flag=True,path =result_path)\n",
    "                    episode_perf = get_episode_metrics(ht_result,data,days_ls= day_test_idx)\n",
    "                    TP += episode_perf[\"TP\"].iloc[0]\n",
    "                    FP += episode_perf[\"FP\"].iloc[0]\n",
    "                    FN += episode_perf[\"FN\"].iloc[0]\n",
    "                    \n",
    "                df[\"dataset\"].append(person)\n",
    "                df[\"Ts\"].append(ts)\n",
    "                df[\"Te\"].append(te)\n",
    "                df[\"TP\"].append(TP)\n",
    "                df[\"FN\"].append(FN)\n",
    "                df[\"FP\"].append(FP)\n",
    "                df[\"TPR\"].append(round(TP/(TP+FN) if (TP+FN)>0 else 0,round_num))\n",
    "                df[\"FP/TP\"].append(round(FP/TP, round_num))\n",
    "                df = pd.DataFrame(df)\n",
    "                print(df)\n",
    "                res = res.append(df, ignore_index=True)\n",
    "                \n",
    "    return res  \n",
    "\n",
    "def find_optimal_threshold(threshold_results,mode=\"min_fp\", min_tpr= 0.85, max_fp=1.):\n",
    "    \"\"\"\n",
    "    To find the optimal threshold for each individual model, based on threshold_results from  function test_threshold_cv()\n",
    "    \n",
    "    threshold_results: output dataframe from test_threshold_cv() function\n",
    "    \"\"\"\n",
    "    best_threshold = pd.DataFrame()\n",
    "    threshold_results[\"ratio\"] = threshold_results[\"TP\"].values/ (threshold_results[\"TP\"].values+threshold_results[\"FN\"].values +threshold_results[\"FP\"].values)\n",
    "    for person in threshold_results[\"dataset\"].unique():\n",
    "        \n",
    "        df= threshold_results.loc[threshold_results['dataset']==person]\n",
    "        if mode ==\"min_fp\":\n",
    "            # find min FP/TP with TPR inside range\n",
    "            df= df.loc[threshold_results[\"TPR\"]>min_tpr]\n",
    "            if len(df) == 0:\n",
    "                df= df.iloc[threshold_results[\"TPR\"].argmax()]\n",
    "            else:\n",
    "                df = df.iloc[df[\"FP/TP\"].argmin()]\n",
    "        elif mode == \"max_tpr\":\n",
    "            # find max TPR with FP/TP inside range\n",
    "            df = df.loc[df[\"FP/TP\"]<max_fp]\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                df= df.iloc[threshold_results[\"FP/TP\"].argmin()]\n",
    "            else:\n",
    "                df= df.iloc[threshold_results[\"TPR\"].argmax()]\n",
    "        else:\n",
    "            df= df.loc[threshold_results[\"TPR\"]>min_tpr]\n",
    "            if len(df) == 0:\n",
    "                df= df.iloc[threshold_results[\"TPR\"].argmax()]\n",
    "            else:\n",
    "                df = df.iloc[df[\"ratio\"].argmax()]\n",
    "                \n",
    "        best_threshold = best_threshold.append(df)\n",
    "        thresholds = {}\n",
    "        for name in best_threshold['dataset'].values:\n",
    "            thresholds[name] = [ best_threshold[best_threshold['dataset']==name]['Ts'].values[0] ,\n",
    "                                best_threshold[best_threshold['dataset']==name]['Te'].values[0] ]\n",
    "        #best_threshold[[\"dataset\",\"Ts\",\"Te\"]]\n",
    "    best_threshold = best_threshold[[\"dataset\",\"Ts\",\"Te\",\"TPR\",\"FP/TP\",\"TP\",\"FP\",\"FN\"]]\n",
    "    return best_threshold , thresholds\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting visualization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile visualization.py\n",
    "from __future__ import print_function\n",
    "from packages import *\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_threshold_results(threshold_results, te_val = 0.1, ts_val=0.6,figsize=(12,10),legend_out=True, grid =False):\n",
    "    \"\"\"\n",
    "    plot threshold results generated from test_threshold_csv()  function in cross_validation.py file\n",
    "    \"\"\"\n",
    "    print(\"Fixed Te, Change Ts\")\n",
    "    fig, ax= plt.subplots(2,2, figsize=figsize)\n",
    "    \n",
    "    for j in range(2):\n",
    "            ax[0,j].set_xlim(threshold_results['Ts'].min(),threshold_results['Ts'].max())\n",
    "    \n",
    "    for j in range(2):\n",
    "            ax[1,j].set_xlim(threshold_results['Te'].min(),threshold_results['Te'].max())\n",
    "            \n",
    "    th_df = threshold_results[(threshold_results['Te']==te_val)]\n",
    "    \n",
    "    fig_ts1 = sns.lineplot(data=th_df, x=\"Ts\", y= \"TPR\", hue=\"dataset\",ax=ax[0,0])\n",
    "    fig_ts2 = sns.lineplot(data=th_df, x=\"Ts\", y= \"FP/TP\", hue=\"dataset\",ax=ax[0,1])\n",
    "    _ = ax[0,0].set_title(\"Te = \"+str(te_val))\n",
    "    _ = ax[0,1].set_title(\"Te = \"+str(te_val))\n",
    "\n",
    "    \n",
    "    th_df = threshold_results[(threshold_results['Ts']==ts_val)]\n",
    "    fig_te1 = sns.lineplot(data=th_df, x=\"Te\", y= \"TPR\", hue=\"dataset\",ax=ax[1,0])\n",
    "    fig_te2 = sns.lineplot(data=th_df, x=\"Te\", y= \"FP/TP\", hue=\"dataset\",ax=ax[1,1])\n",
    "    _ = ax[1,0].set_title(\"Ts = \"+str(ts_val))\n",
    "    _ = ax[1,1].set_title(\"Ts = \"+str(ts_val))\n",
    "    if grid:\n",
    "        fig_ts1.grid()\n",
    "        fig_ts2.grid()\n",
    "        fig_te1.grid()\n",
    "        fig_te2.grid()\n",
    "    if not legend_out :\n",
    "        fig_ts1.legend(loc=\"upper right\")\n",
    "        fig_ts2.legend(loc=\"upper right\")\n",
    "        fig_te1.legend(loc=\"upper right\")\n",
    "        fig_te2.legend(loc=\"upper right\")\n",
    "    else:\n",
    "        fig_te2.legend_.remove()\n",
    "        fig_ts1.legend_.remove()\n",
    "        fig_te1.legend_.remove()\n",
    "        fig_ts2.legend(loc=2, bbox_to_anchor=(1.05,1.0),borderaxespad = 0.)\n",
    "    \n",
    "    return [fig_ts1,fig_ts2], [fig_te1,fig_te2]\n",
    "\n",
    "def save_figs(fig_ts, fig_te, dpi=400,fig_path = \"../results/images/\",prefix=\"\"):\n",
    "    \n",
    "    fig = fig_ts[0].get_figure()\n",
    "    fig.savefig(fig_path+prefix+\"fig_ts1\", dpi = dpi)\n",
    "    fig = fig_ts[1].get_figure()\n",
    "    fig.savefig(fig_path+prefix+\"fig_ts2\", dpi = dpi)\n",
    "    fig = fig_te[0].get_figure()\n",
    "    fig.savefig(fig_path+prefix+\"fig_te1\", dpi = dpi)\n",
    "    fig = fig_te[1].get_figure()\n",
    "    fig.savefig(fig_path+prefix+\"fig_te2\", dpi = dpi)\n",
    "\n",
    "\n",
    "def map_prediction_gt(meal_data, day,possib_result ):\n",
    "    \"\"\"\n",
    "    Convert segmentation back to binary labels\n",
    "    and Map the prediction possibility sequence  back to the same shape\n",
    "    \"\"\"\n",
    "    day = int(day)\n",
    "    res = possib_result \n",
    "    possib = np.array(res.proba.iloc[day])\n",
    "    step= res[\"stepsec\"].iloc[0] *15\n",
    "    start_ls, end_ls = meal_data.get_GT_segment()\n",
    "    start_ls = start_ls[day]\n",
    "    end_ls = end_ls[day]\n",
    "    proba = np.zeros([len(meal_data.data[day]), ] )\n",
    "    labels = np.zeros([len(meal_data.data[day]), ] )\n",
    "    preds = np.zeros([len(meal_data.data[day]), ] )\n",
    "    \n",
    "    #probability sequence\n",
    "    for i in range(len(possib)):\n",
    "        proba[i*step: (i+1)*step]= possib[i]\n",
    "    \n",
    "    # GT label\n",
    "    for i in range(len(start_ls)):\n",
    "        labels[start_ls[i]:end_ls[i]+1] = 1\n",
    "        \n",
    "    # prediction label by hysteresis threshold\n",
    "    seg_start_ls = res[\"segment_start\"].iloc[day]\n",
    "    seg_end_ls = res[\"segment_end\"].iloc[day]\n",
    "    for i in range(len(seg_start_ls)):\n",
    "        s = int(seg_start_ls[i] * step)\n",
    "        e = int(seg_end_ls[i] * step)\n",
    "        preds[s:e] =1\n",
    "    return proba, labels, preds\n",
    "\n",
    "def map_results(meal_data,possib_result):\n",
    "    \"\"\"\n",
    "    Convert segmentation back to binary labels\n",
    "    and Map the prediction possibility sequence  back to the same shape\n",
    "    for all days of data\n",
    "    \"\"\"\n",
    "    proba_ls, labels_ls, preds_ls = [],[],[]\n",
    "\n",
    "    for day in range(len(possib_result)):\n",
    "        proba, labels, preds = map_prediction_gt(meal_data, day,possib_result )\n",
    "        proba_ls.append(proba)\n",
    "        labels_ls.append(labels)\n",
    "        preds_ls.append(preds)\n",
    "    return  proba_ls, labels_ls, preds_ls\n",
    "\n",
    "def get_episode_output(names= [], threshold= None, use_group_model= False,load_proba_flag=True):\n",
    "    \"\"\"\n",
    "    Generate probability sequences for all days of data in all dataset\n",
    "    \"\"\"\n",
    "    output_df = {\"dataset\":[],\"proba_ls\":[],\"labels_ls\":[],\"preds_ls\":[]}\n",
    "    if not threshold:\n",
    "        threshold = {}\n",
    "        for name in names:\n",
    "            threshold[name] = [0.8,0.4]\n",
    "    for person in names:\n",
    "        output_df[\"dataset\"].append(person)\n",
    "        meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = 6,stridesec = 5,smooth_flag = 1,\n",
    "                         normalize_flag = 1)\n",
    "        high_th, low_th = threshold[person][0], threshold[person][1]\n",
    "        if use_group_model:\n",
    "            model = tf.keras.models.load_model('../models/CAD_models/acti_6min_M_F_6.000000Min.h5')\n",
    "            result = hysteresis_threshold(model, meal_data,start_threshold=high_th, end_threshold=low_th, winmin = 6, stepsec=5, episode_min = 1.,\n",
    "                                     load_proba_flag=load_proba_flag,path=\"../results/possibility_results/group_\")\n",
    "        else:\n",
    "            model = tf.keras.models.load_model('../models/'+ person + '_models/acti_6min_split_day_M_F_6.000000Min.h5')\n",
    "            result = hysteresis_threshold(model, meal_data,start_threshold=high_th, end_threshold=low_th, winmin = 6, stepsec=5, episode_min = 1.,\n",
    "                                     load_proba_flag=load_proba_flag)\n",
    "            \n",
    "        proba_ls, labels_ls, preds_ls =map_results(meal_data,result)\n",
    "        output_df[\"proba_ls\"].append(proba_ls)\n",
    "        output_df[\"labels_ls\"].append(labels_ls)\n",
    "        output_df[\"preds_ls\"].append(preds_ls)\n",
    "    return pd.DataFrame(output_df)\n",
    "\n",
    "\n",
    "def generate_possibility(dataset,fold_num=5):\n",
    "    \"\"\"\n",
    "    dataset: person_meal dataset\n",
    "    output: generated possibility adn ground true label and prediction in episode\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame()\n",
    "    from sklearn.model_selection import KFold\n",
    "    days = np.unique(dataset.data_indices[:,0])\n",
    "    kf = KFold(n_splits=5, random_state= 1000,shuffle=False)\n",
    "    \n",
    "    for fold, (day_train_idx, day_test_idx) in enumerate(kf.split(days)):\n",
    "        day_test_idx = day_test_idx.tolist()\n",
    "        proba_path =\"../results/possibility_results/{}/cv_fold_{}_\".format(dataset.person_name,fold)\n",
    "        partial_result = hysteresis_threshold(None, dataset,days_ls = day_test_idx,start_threshold=0.8, end_threshold=0.4,\n",
    "                                                  winmin = 6, stepsec=5, episode_min = 1.,\n",
    "                                                 load_proba_flag=True, path =proba_path)\n",
    "        #print(partial_result)\n",
    "        result = result.append(partial_result,ignore_index=True)\n",
    "    proba_ls, labels_ls, preds_ls =map_results(dataset,result)\n",
    "    return proba_ls, labels_ls, preds_ls, result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_prob(offset,winsize,day, model_result=\"I\",file_name =\"possibility_seq\" ):\n",
    "    \"\"\"\n",
    "    Note:\n",
    "        proba_ls, labels_ls, preds_ls are global variables from notebook\n",
    "    \"\"\"\n",
    "    stride  = 5 *15 # 5 seconds between two adjacent labels/window samples\n",
    "    fig_path = \"../results/images/\"\n",
    "    day = int(day)\n",
    "    global proba_ls\n",
    "    global labels_ls\n",
    "    global preds_ls\n",
    "    if model_result ==\"I\":\n",
    "        proba_list=proba_ls\n",
    "        labels_list= labels_ls\n",
    "        preds_list=preds_ls\n",
    "    else:\n",
    "        proba_list=proba_ls_g\n",
    "        labels_list= labels_ls_g\n",
    "        preds_list=preds_ls_g\n",
    "    proba, labels, preds = proba_list[day], labels_list[day], preds_list[day]    \n",
    "    \n",
    "    offset = offset *15\n",
    "    if winsize == -1:\n",
    "        winsize = len(labels)\n",
    "        offset = 0\n",
    "    else:\n",
    "        winsize = winsize*15\n",
    "    \n",
    "    if  len(labels)-winsize <0:\n",
    "        #offset = len(labels)-winsize\n",
    "        offset =0\n",
    "        winsize = len(labels)\n",
    "        \n",
    "    t = np.arange(start = offset, stop= offset+winsize, step=1)\n",
    "    print(\"Offset: \",offset, \"winszie: \",winsize,\"t shape: \",t.shape, \"label shape:\", labels.shape, preds.shape)\n",
    "    fig, ax = plt.subplots(3,1,figsize= (20,12))\n",
    "    df1 = proba[offset:offset+winsize]\n",
    "    df2 = np.array(preds[offset:offset+winsize]) #*10-5\n",
    "    df3 = np.array(labels[offset:offset+winsize])#*10-5\n",
    "    x1= sns.lineplot(x=t, y=df1 , ax =ax[0],color= 'grey',label=\"Possibility\")\n",
    "    x2 = sns.lineplot(t,df2 , ax =ax[1],color='g', linewidth=1.5,label=\"Prediction(Eat)\")\n",
    "    x3 = sns.lineplot(t,df3 , ax =ax[2],color='b', linewidth=1.5, label=\"Label(Eat)\")\n",
    "        \n",
    "    \n",
    "        \n",
    "    ax[0].fill_between( t, df1, \n",
    "                interpolate=True, color='grey')\n",
    "    \n",
    "    ax[0].fill_between(t, df3, where=(df3==1), \n",
    "                interpolate=True, color='blue')\n",
    "    ax[0].fill_between( t,df2, where=(df2==1), \n",
    "                interpolate=True, color='green')\n",
    "    ax[0].set_ylim(0,1)\n",
    "    print(offset,len(labels)-winsize )\n",
    "    if offset >= len(labels)-winsize:\n",
    "        title_txt =\"Day: \"+ str(day) + \" Whole Day samples: \"+str(len(labels)) +\" . \" + \"Sample plotted: \"+str(winsize)+\". \"\n",
    "    else:\n",
    "        title_txt = \"Day: \"+ str(day) + \"Samples from \"+str(offset) +\"~\" + str((offset+winsize)) +\". \"+ \"Number of Sample plotted: \"+str(winsize)+\". \"\n",
    "    ax[0].set_title(title_txt)\n",
    "    ax[2].set_xlabel(\"index of sample\")\n",
    "    ax[0].set_ylabel(\"Possibility\")\n",
    "    ax[1].set_ylabel(\"Predictions\")\n",
    "    ax[2].set_ylabel(\"Ground Truch\")\n",
    "    \n",
    "    ax[0].legend([\"Possibility\"],loc='upper left')\n",
    "    ax[1].legend([\"1: Eat, 0:Other\"],loc='upper left')\n",
    "    ax[2].legend([\"1: Eat, 0:Other\"],loc='upper left')\n",
    "    x3 = x3.get_figure()\n",
    "    x3.savefig(fig_path+file_name, dpi = 80)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv_v2",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
