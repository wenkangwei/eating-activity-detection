{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Data Transformation and Preprocessing\n",
    "### settings: CPU 4,  memory 69G, chunk: 2, GPU model: P100, number of GPU:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.py\n",
    "\n",
    "\n",
    "from data_loader import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "def create_train_test_file_list(file_name= \"all_files_list.txt\",person_name = 'wenkanw',\n",
    "                     out_path = \"../data-file-indices/\",root_path= \"../\",\n",
    "                     test_ratio = 0.2, print_flag = True):\n",
    "    \"\"\"\n",
    "    This function is used to split test set and training set based on file names\n",
    "    \n",
    "    \"\"\"\n",
    "    shm_file_ls = []\n",
    "    event_file_ls = []\n",
    "    new_files = []\n",
    "    if person_name == \"CAD\":\n",
    "        fp = open(out_path+ person_name+ \"/\" +\"batch-unix.txt\", \"r\")\n",
    "        txt = fp.read()\n",
    "        fp.close()\n",
    "        # save all file list\n",
    "        fp = open(out_path+ person_name+ \"/\" +\"all_files_list.txt\", \"w\")\n",
    "        fp.write(txt)\n",
    "        fp.close()\n",
    "        \n",
    "        txt_ls = txt.split(\"\\n\")\n",
    "        txt_ls.remove(\"\")\n",
    "        txt_ls= [txt+\"\\n\" for txt in txt_ls]\n",
    "        test_size = int(len(txt_ls)*test_ratio)\n",
    "        test = \"\".join(txt_ls[len(txt_ls) - test_size: ])\n",
    "        train = \"\".join(txt_ls[:len(txt_ls) - test_size ])\n",
    "        \n",
    "        fp = open(out_path+ person_name+ \"/\" +\"test_files.txt\", \"w\")\n",
    "        fp.write(test)\n",
    "        fp.close()\n",
    "        \n",
    "        fp = open(out_path+ person_name+ \"/\" +\"train_files.txt\", \"w\")\n",
    "        fp.write(train)\n",
    "        fp.close()\n",
    "        \n",
    "        if print_flag:\n",
    "            print(\"Train:\", len(txt_ls) - test_size)\n",
    "            print(train)\n",
    "            print(\"test: \",test_size)\n",
    "            print(test)\n",
    "        return \n",
    "        \n",
    "        \n",
    "    for dirname, _, filenames in os.walk(root_path + 'data/IndividualData'):\n",
    "        for filename in filenames:\n",
    "            # check every file name in the individual data folder\n",
    "            path = os.path.join(dirname, filename)\n",
    "#             print(\"Path: \",path)\n",
    "            # check if datafile is shm file and is not a test file\n",
    "            if \".shm\" in filename and person_name in path and 'test' not in path:\n",
    "                # If the data file has label file as well, then it is valid\n",
    "                # and we add it to the filename list\n",
    "                event_file_name =  filename.replace(\".shm\",\"-events.txt\")\n",
    "                \n",
    "                if event_file_name in filenames:\n",
    "                    # if both shm and event files exist\n",
    "                    new_file = path.replace(root_path+\"data/\",\"\")\n",
    "                    new_file += \"\\n\"\n",
    "                    new_files.append(new_file)\n",
    "        \n",
    "    new_files.sort()\n",
    "    if test_ratio > 0.:\n",
    "        # split train files and test files\n",
    "        test_size = int(len(new_files)*test_ratio)\n",
    "        test_files = new_files[:test_size]\n",
    "        train_files = new_files[test_size:]\n",
    "        # write train files\n",
    "        fp = open(out_path+ person_name+ \"/\" +\"train_files.txt\", \"w\")\n",
    "        train = \"\".join(train_files)\n",
    "        \n",
    "        fp.write(train)\n",
    "        fp.close()\n",
    "        # write test files\n",
    "        fp = open(out_path+ person_name+ \"/\" +\"test_files.txt\", \"w\")\n",
    "        test = \"\".join(test_files)\n",
    "        fp.write(test)\n",
    "        fp.close()\n",
    "        \n",
    "        if print_flag:\n",
    "            print(\"Train:\")\n",
    "            print(train)\n",
    "            print(\"test: \")\n",
    "            print(test)\n",
    "    \n",
    "    fp = open(out_path+person_name+ \"/\"+file_name, \"w\")\n",
    "    all_files = \"\".join(new_files)\n",
    "    fp.write(all_files)\n",
    "    fp.close()\n",
    "    \n",
    "    if print_flag:\n",
    "        print(\"All files: \")\n",
    "        print(all_files)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Person_MealsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset = None,person_name= \"wenkanw\", \n",
    "                 data_indices_file = \"../data-file-indices/\",\n",
    "                 file_name = \"all_files_list\",\n",
    "                 remove_trend = 0,\n",
    "                 remove_walk = 0,\n",
    "                 remove_rest = 0,\n",
    "                 smooth_flag = 1,\n",
    "                 normalize_flag = 1,\n",
    "                 winmin = 6,\n",
    "                 stridesec = 15,\n",
    "                 gtperc = 0.5,\n",
    "                 device = 'cpu',\n",
    "                 ratio_dataset=1,\n",
    "                load_splitted_dataset = False,\n",
    "                 enable_time_feat = False,\n",
    "                 debug_flag= False\n",
    "                ):\n",
    "        \n",
    "        if file_name == \"train\":\n",
    "            file_name = data_indices_file + person_name +\"/\"+\"train_files.txt\"\n",
    "        elif file_name == \"test\":\n",
    "            file_name = data_indices_file + person_name +\"/\"+\"test_files.txt\"\n",
    "        else:\n",
    "            file_name = data_indices_file + person_name +\"/\"+ file_name+\".txt\"\n",
    "            \n",
    "        # Note: file_name is the name of file that contain the list of shm files' names\n",
    "        self.file_name = file_name\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.winmin = winmin\n",
    "        self.stridesec = stridesec\n",
    "        self.load_splitted_dataset = load_splitted_dataset\n",
    "        self.remove_trend = remove_trend\n",
    "        self.remove_walk = remove_walk\n",
    "        self.remove_rest = remove_rest\n",
    "        self.smooth_flag = smooth_flag\n",
    "        self.normalize_flag = normalize_flag\n",
    "        self.gtperc = gtperc,\n",
    "        self.ratio_dataset = ratio_dataset\n",
    "        self.enable_time_feat = enable_time_feat\n",
    "        self.device = device\n",
    "        self.debug_flag= debug_flag\n",
    "        if not self.dataset:\n",
    "            self.get_data(person_name)\n",
    "\n",
    "    def get_data(self, person_name):\n",
    "            \n",
    "            \n",
    "            # files_counts, data, samples_indices, labels_array\n",
    "            # Note: the data preprocessing in this function is for global time series dataset\n",
    "            \n",
    "            self.dataset, self.data, self.data_indices, self.labels = load_train_test_data(data_file_list =self.file_name,\n",
    "                                    load_splitted_dataset = False,\n",
    "                                     ratio_dataset=self.ratio_dataset,\n",
    "                                     enabled_time_feat = self.enable_time_feat, \n",
    "                                     winmin = self.winmin, stridesec = self.stridesec,gtperc = self.gtperc,\n",
    "                                     removerest = self.remove_rest,\n",
    "                                     removewalk = self.remove_walk, smooth_flag = self.smooth_flag, normalize_flag=self.normalize_flag, \n",
    "                                     remove_trend = self.remove_trend,\n",
    "                                     debug_flag=self.debug_flag )\n",
    "            \n",
    "            if self.load_splitted_dataset:\n",
    "                self.dataset = self.get_dataset()\n",
    "                \n",
    "            \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        #这里需要注意的是，第一步：read one data，是一个data\n",
    "        data = self.get_item(index)\n",
    "        return data['data'],data['label']\n",
    "        \n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return  len(self.dataset) if self.load_splitted_dataset else len(self.data_indices)\n",
    "    def get_item(self, index, tensor_type=True):\n",
    "        \"\"\"\n",
    "        This function is used to obtain one sample data point\n",
    "        \"\"\"\n",
    "        f,start_time, end_time = self.data_indices[index,0], self.data_indices[index,1], self.data_indices[index,2]\n",
    "        sample = self.data[f][start_time : end_time]\n",
    "        data = pd.DataFrame(columns=['data','label'])    \n",
    "        # Add time feature to data\n",
    "        if self.enable_time_feat:\n",
    "            time_offset = self.data_indices[index,3]\n",
    "            freq = 1.0/15.0\n",
    "            time_feat = np.array([[i for i in range(len(sample))]],dtype=float).transpose()\n",
    "            time_feat *= freq\n",
    "            time_feat += float(start_time)* freq\n",
    "            time_feat += time_offset\n",
    "            sample = np.concatenate((sample, time_feat),axis=1)\n",
    "        label = self.labels[index]\n",
    "        if tensor_type:\n",
    "            data = {\"data\":torch.tensor(sample, dtype =torch.float, device =  self.device ), 'label': label}\n",
    "        else:\n",
    "            data = {\"data\":sample, 'label': label}\n",
    "        return data\n",
    "    \n",
    "    def get_dataset(self, start_index = None, end_index = None):\n",
    "        \"\"\"\n",
    "        This function is used to obtain the whole dataset in pandas or part of whole dataset\n",
    "        It is good to use this to sample some data to analyze\n",
    "        \"\"\"\n",
    "        start_i = 0 if not start_index else start_index\n",
    "        end_i = self.__len__() if not end_index else end_index\n",
    "        \n",
    "        dataset = pd.DataFrame(columns=['data','label'])\n",
    "        for i in tqdm(range(start_i, end_i)):\n",
    "            data = self.get_item(i)\n",
    "            dataset = dataset.append(data,ignore_index=True)\n",
    "        self.dataset = dataset\n",
    "        return self.dataset\n",
    "    \n",
    "    def sample(self, num = 1000,random_state = None):\n",
    "        \"\"\"\n",
    "        Simply sample part of data for analysis\n",
    "        \"\"\"\n",
    "        if random_state != None:\n",
    "            np.random.seed(random_state)\n",
    "            \n",
    "        sample_data = pd.DataFrame(columns=['data','label'])\n",
    "        indices = np.random.choice(len(self.labels), num)\n",
    "        for i in tqdm(indices):\n",
    "            data = self.get_item(i)\n",
    "            data[\"data\"] = data[\"data\"].numpy()\n",
    "            sample_data = sample_data.append(data,ignore_index=True)\n",
    "        return sample_data\n",
    "    \n",
    "    def get_subset(self, indices_ls):\n",
    "        axdata = []\n",
    "        aydata = []\n",
    "        for i in indices_ls:\n",
    "            data = self.get_item(i, tensor_type=False)\n",
    "            sample = data['data']\n",
    "            label = data['label']\n",
    "            axdata.append(sample)\n",
    "            aydata.append(label)\n",
    "        subsetData = np.array(axdata, copy=True) # Undersampled Balanced Training Set\n",
    "        subsetLabels = np.array(aydata, copy=True)\n",
    "        del axdata\n",
    "        del aydata\n",
    "        return subsetData, subsetLabels\n",
    "        \n",
    "        \n",
    "            \n",
    "                \n",
    "def balance_data_indices(labels, sample_num = 4000,mode= \"under\", replace = False,shuffle=True, random_state = 1000):\n",
    "    \"\"\"\n",
    "    sample_num: number of samples of each class after balancing\n",
    "    mode: \n",
    "        under - undersampling\n",
    "        over - oversampling\n",
    "        mix - undersampling negative samples + oversampling positive samples, each class has sample_num amount samples in this mode\n",
    "    return:\n",
    "        balanced indices\n",
    "    \"\"\"\n",
    "    eat_labels_index = [i for i, e in enumerate(labels) if e >= 0.5]\n",
    "    not_eat_labels_index = [i for i, e in enumerate(labels) if e < 0.5]\n",
    "    eat_index = eat_labels_index\n",
    "    not_eat_index = not_eat_labels_index\n",
    "    if random_state != None:\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    if mode == \"over\":\n",
    "        eat_index = np.random.choice(eat_labels_index,len(not_eat_labels_index)).tolist()\n",
    "        pass\n",
    "    elif mode == \"under\":\n",
    "        not_eat_index = np.random.choice(not_eat_labels_index,len(eat_labels_index)).tolist()\n",
    "        pass\n",
    "    else:\n",
    "        #default as mix\n",
    "        eat_index = np.random.choice(eat_labels_index,sample_num, replace = replace).tolist()\n",
    "        not_eat_index = np.random.choice(not_eat_labels_index,sample_num, replace = replace).tolist()\n",
    "        pass\n",
    "    \n",
    "    indices_balanced = eat_index + not_eat_index\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices_balanced)\n",
    "    \n",
    "    return indices_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report,recall_score, precision_score\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report,recall_score, precision_score\n",
    "\n",
    "\n",
    "def eval_model(model,dataloader,device=\"cpu\"):\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    # without update\n",
    "    with torch.no_grad():\n",
    "        for samples, labels in dataloader:\n",
    "            samples = samples.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(samples).to(device).squeeze()\n",
    "            #print(\"Output: \", outputs)\n",
    "            outputs = torch.round(torch.sigmoid(outputs))\n",
    "            preds = outputs>=0.5\n",
    "            preds = preds.to(dtype = torch.float)\n",
    "            preds.requires_grad = False\n",
    "#             _,preds = torch.max(outputs,1)\n",
    "            for i in range(len(preds)):\n",
    "                if preds[i] == 1 and labels[i] == 1:\n",
    "                    TP += 1\n",
    "                if preds[i] == 0 and labels[i] == 1:\n",
    "                    FN += 1\n",
    "            correct += torch.sum((preds == labels)).item()\n",
    "            total += float(len(labels))\n",
    "        acc =100 * correct/ total\n",
    "        recall = TP/(TP+FN)\n",
    "#         print(\"Evaluation Acc: %.4f %%,  Recall: %.4f \"%(acc , recall))\n",
    "    return acc, recall\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "def train_model(model,dataloader, optimizer, criterion,lrscheduler,device=\"cpu\" , n_epochs=20,\n",
    "                earlystopping=True, patience= 3, l1_enabled=True,checkpoint_name =\"checkpoint.pt\" ):\n",
    "    loss_ls = [0.0]\n",
    "    train_acc_ls = [0.0]\n",
    "    valid_acc_ls = [0.0]\n",
    "    valid_acc = 0.0\n",
    "    loss =0.0\n",
    "    train_acc = 0.0\n",
    "    patience_count = 0\n",
    "    best_val_score = 0.0\n",
    "    prev_val_score = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    train_dataloader, valid_dataloader = dataloader\n",
    "    print(\"Training set batch amounts:\", len(train_dataloader))\n",
    "    print(\"Test set :\", len(valid_dataloader))\n",
    "    print(\"Start Training..\")\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        correct_cnt = 0.0\n",
    "        total_cnt = 0.0\n",
    "        TP = 0.\n",
    "        FN = 0.\n",
    "        model.train()\n",
    "        for i, (samples, labels) in enumerate(train_dataloader):\n",
    "            samples = samples.to(device)\n",
    "            labels = labels.to(device, dtype=torch.float32)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # reshape samples\n",
    "            outputs = model(samples).squeeze()\n",
    "\n",
    "            #print(\"Output: \", outputs, \"label: \", labels)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            if l1_enabled:\n",
    "                L1_loss = model.l1_loss(0.01).to(device)\n",
    "                loss += L1_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # prediction\n",
    "            #_,preds = torch.max(outputs,1)\n",
    "            outputs = torch.round(torch.sigmoid(outputs))\n",
    "            preds = outputs>=0.5\n",
    "            preds = preds.to(dtype = torch.float)\n",
    "            preds.requires_grad = False\n",
    "            \n",
    "            # Compute count of TP, FN\n",
    "            for j in range(len(preds)):\n",
    "                if preds[j] == 1. and labels[j] == 1.:\n",
    "                    TP += 1\n",
    "                if preds[j] == 0. and labels[j] == 1.:\n",
    "                    FN += 1\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            correct_cnt += torch.sum((preds == labels)).item()\n",
    "            total_cnt += float(len(labels))\n",
    "            batch_acc = 100. * (preds == labels).sum().item()/ float(len(labels))\n",
    "            if i %50 ==0:\n",
    "                #print(\"===> Batch: %d,  Batch_Loss: %.4f, Train Acc: %.4f %%,  Recall: %.f\\n\"%(i, loss,batch_acc, recall))\n",
    "                pass\n",
    "\n",
    "            \n",
    "        \n",
    "        # Compute accuracy and loss of one epoch\n",
    "        epoch_loss = running_loss / len(train_dataloader)  \n",
    "        epoch_acc = 100* correct_cnt/ total_cnt  # in percentage\n",
    "        correct_cnt = 0.0\n",
    "        total_cnt = 0.0\n",
    "        train_recall = TP/(TP+FN)\n",
    "        \n",
    "        #Validation mode\n",
    "        model.eval()\n",
    "        valid_acc, valid_recall= eval_model(model,valid_dataloader,device=device)\n",
    "        \n",
    "        # record loss and accuracy\n",
    "        valid_acc_ls.append(valid_acc)  \n",
    "        train_acc_ls.append(epoch_acc)\n",
    "        loss_ls.append(epoch_loss)\n",
    "        \n",
    "        if e %1==0:\n",
    "            print(\"Epoch: %d,  Epoch_Loss: %.4f, Train Acc: %.4f %%, Train Recall: %.4f \"%(e, epoch_loss,\n",
    "                                                                                     epoch_acc,train_recall))\n",
    "            print(\"Validation Acc:  %.4f %%,  Validation Recall: %.4f \"%(valid_acc, valid_recall))\n",
    "        \n",
    "        # Reset train mode\n",
    "        model.train()\n",
    "        lrscheduler.step(valid_acc)\n",
    "        \n",
    "        \n",
    "        # If earlystopping is enabled, then save model if performance is improved\n",
    "        if earlystopping:\n",
    "            if prev_val_score !=0. and valid_acc < prev_val_score :\n",
    "                patience_count += 1\n",
    "            else:\n",
    "                patience_count = 0\n",
    "                \n",
    "            if patience_count >= patience:\n",
    "                break \n",
    "                \n",
    "            prev_val_score = valid_acc\n",
    "            if valid_acc > best_val_score or best_val_score == 0.0:\n",
    "                best_val_score = valid_acc\n",
    "                torch.save(model,checkpoint_name)\n",
    "                print(\"Checkpoint Saved\")\n",
    "            \n",
    "                \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "            \n",
    "    # Load best model\n",
    "    best_model = torch.load(checkpoint_name)\n",
    "    print(\"Load Best Model.\")\n",
    "    print(\"Training completed\")\n",
    "        \n",
    "    return model, best_model,best_val_score,loss_ls, train_acc_ls, valid_acc_ls\n",
    "            \n",
    "\n",
    "def plot_data(train_acc_ls,valid_acc_ls,loss_ls ):\n",
    "    \"\"\"\n",
    "    Plot validation accuracy, training accuracy and loss\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "    epochs = [i for i in range(len(train_acc_ls))]\n",
    "    _ = sns.lineplot(x=epochs, y= train_acc_ls,ax=ax[0])\n",
    "    _ = sns.lineplot(x=epochs, y= valid_acc_ls,ax=ax[0])\n",
    "    ax[0].set_xlabel(\"Epoches\")\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "    ax[0].legend([\"Training Accuracy\", \"Validation Accuracy\"])\n",
    "    \n",
    "    _ = sns.lineplot(x=epochs[1:], y= loss_ls[1:],ax=ax[1])\n",
    "    ax[1].set_xlabel(\"Epoches\")\n",
    "    ax[1].set_ylabel(\"Training Loss\")\n",
    "    ax[1].set(yscale=\"log\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Discriminator_BN_Bias(nn.Module):\n",
    "    def __init__(self, ngpu, input_shape , out_fea = 1, bias=True):\n",
    "        super(Discriminator_BN_Bias, self).__init__()\n",
    "        \n",
    "        # (input_shape[0], input_shape[1] )=  (number of data point, 6 axis channel )\n",
    "        in_channels, win_size = input_shape[0], input_shape[1]\n",
    "        self.in_channels = in_channels\n",
    "        self.ngpu = ngpu  \n",
    "        self.out_fea = out_fea\n",
    "        filter_num = 10\n",
    "        \n",
    "        self.conv0 = nn.Conv1d(in_channels = self.in_channels, \n",
    "                               out_channels = filter_num, \n",
    "                               kernel_size= 44,  stride= 2, padding=0, bias=bias)\n",
    "        \n",
    "        self.relu0= nn.ReLU()\n",
    "        num_fea = (win_size-44)//2 +1\n",
    "        self.conv1 = nn.Conv1d(filter_num,filter_num, kernel_size= 20,stride= 2, padding=0, bias=bias)\n",
    "        self.relu1= nn.ReLU()\n",
    "        num_fea = (num_fea-20)//2 +1\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(filter_num)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(filter_num,filter_num, kernel_size= 4, stride= 2, padding=0, bias=bias)\n",
    "        self.relu2= nn.ReLU()\n",
    "        \n",
    "        num_fea = (num_fea-4)//2 +1\n",
    "        self.bn2 = nn.BatchNorm1d(filter_num)\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=10)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = None \n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=200, out_features=out_fea, bias=bias)\n",
    "        self.softmax = nn.Softmax(dim=out_fea)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        nn.init.normal_(self.conv0.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.conv1.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.conv2.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.linear2.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv0.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv1.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv2.bias.data, 0.0, 1.)\n",
    "#         nn.init.normal_(self.avgpool.weight.data, 0.0, 1.)\n",
    "        \n",
    "    def l1_loss(self,factor=0.01):\n",
    "        l1_crit = nn.L1Loss(size_average=False)\n",
    "        reg_loss = 0.\n",
    "        loss = 0.\n",
    "        layers = [self.conv0, self.conv1, self.conv2]\n",
    "        for layer in layers:\n",
    "            for p in layer.parameters():\n",
    "                #print(p)\n",
    "                reg_loss += l1_crit(p, torch.zeros(p.shape))\n",
    "\n",
    "        loss = factor * reg_loss\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = input.permute(0,2,1)\n",
    "        x = self.conv0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = self.conv1(x)\n",
    "        x=  self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x=  self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.avgpool(x)\n",
    "        #print(\"Pooling shape:\",x.shape)\n",
    "        x = self.flatten(x)\n",
    "        if self.linear1 == None:\n",
    "            self.linear1 = nn.Linear(in_features=x.shape[1], out_features=200, bias=True)\n",
    "            nn.init.normal_(self.linear1.weight.data, 0.0, 1.)\n",
    "            nn.init.normal_(self.linear1.bias.data, 0.0, 1.)\n",
    "            \n",
    "        x = self.relu4(self.linear1(x))\n",
    "        out = self.linear2(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels ,bias= True, filter_num = 10):\n",
    "        super(BasicBlock, self).__init__()       \n",
    "        self.conv0 = nn.Conv1d(in_channels = in_channels, \n",
    "                               out_channels = filter_num, \n",
    "                               kernel_size= 3,  stride= 1, padding=1, bias=bias)\n",
    "        self.bn0 = nn.BatchNorm1d(filter_num)\n",
    "        self.relu0 = nn.ReLU()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels = in_channels, \n",
    "                               out_channels = filter_num, \n",
    "                               kernel_size= 3,  stride= 1, padding=1, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm1d(filter_num)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv0(x)\n",
    "        out = self.bn0(out)\n",
    "        out = self.relu0(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        \n",
    "        out += residual\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Discriminator_ResNet(nn.Module):\n",
    "    def __init__(self, ngpu, input_shape , out_fea = 1):\n",
    "        super(Discriminator_ResNet, self).__init__()\n",
    "        \n",
    "        # (input_shape[0], input_shape[1] )=  (number of data point, 6 axis channel )\n",
    "        in_channels, win_size = input_shape[0], input_shape[1]\n",
    "        self.in_channels = in_channels\n",
    "        self.ngpu = ngpu  \n",
    "        self.out_fea = out_fea\n",
    "        filter_num = 10\n",
    "        \n",
    "        self.conv0 = nn.Conv1d(in_channels = self.in_channels, \n",
    "                               out_channels = filter_num, \n",
    "                               kernel_size= 44,  stride= 2, padding=0, bias=True)\n",
    "        \n",
    "        self.relu0= nn.ReLU()\n",
    "        num_fea = (win_size-44)//2 +1\n",
    "        self.conv1 = nn.Conv1d(filter_num,filter_num, kernel_size= 20,stride= 2, padding=0, bias=True)\n",
    "        self.relu1= nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(filter_num)\n",
    "        \n",
    "        self.block1 =  BasicBlock( in_channels=filter_num  ,bias= True, filter_num = filter_num)\n",
    "        self.block2 =  BasicBlock( in_channels=filter_num  ,bias= True, filter_num = filter_num)\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=10)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = None \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=200, out_features=out_fea, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=out_fea)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        nn.init.normal_(self.conv0.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.conv1.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.linear2.bias.data, 0.0, 1.)\n",
    "        \n",
    "    def l1_loss(self,factor=0.01):\n",
    "        l1_crit = nn.L1Loss(size_average=False)\n",
    "        reg_loss = 0.\n",
    "        loss = 0.\n",
    "        layers = [self.conv0, self.conv1]\n",
    "        for layer in layers:\n",
    "            for p in layer.parameters():\n",
    "                #print(p)\n",
    "                reg_loss += l1_crit(p, torch.zeros(p.shape))\n",
    "\n",
    "        loss = factor * reg_loss\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "\n",
    "        x = input.permute(0,2,1)\n",
    "        x = self.conv0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = self.conv1(x)\n",
    "        x=  self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        #print(\"Pooling shape:\",x.shape)\n",
    "        x = self.flatten(x)\n",
    "        if self.linear1 == None:\n",
    "            self.linear1 = nn.Linear(in_features=x.shape[1], out_features=200, bias=True)\n",
    "            nn.init.normal_(self.linear1.weight.data, 0.0, 1.)\n",
    "            nn.init.normal_(self.linear1.bias.data, 0.0, 1.)\n",
    "            \n",
    "        x = self.relu2(self.linear1(x))\n",
    "        out = self.linear2(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
