{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Data Transformation and Preprocessing\n",
    "### settings: CPU 4,  memory 69G, chunk: 2, GPU model: P100, number of GPU:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.py\n",
    "\n",
    "\n",
    "from data_loader import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def create_train_test_file_list(file_name= \"all_files_list.txt\",person_name = 'wenkanw',\n",
    "                     out_path = \"../data-file-indices/\",root_path= \"../\",\n",
    "                     test_ratio = 0.2, print_flag = True, shuffle=False, random_state=None):\n",
    "    \"\"\"\n",
    "    This function is used to split test set and training set based on file names\n",
    "    \n",
    "    \"\"\"\n",
    "    shm_file_ls = []\n",
    "    event_file_ls = []\n",
    "    new_files = []\n",
    "    if person_name == \"CAD\":\n",
    "        fp = open(out_path+ person_name+ \"/\" +\"batch-unix.txt\", \"r\")\n",
    "        txt = fp.read()\n",
    "        fp.close()\n",
    "        # save all file list\n",
    "        fp = open(out_path+ person_name+ \"/\" +\"all_files_list.txt\", \"w\")\n",
    "        fp.write(txt)\n",
    "        fp.close()\n",
    "        \n",
    "        txt_ls = txt.split(\"\\n\")\n",
    "        txt_ls.remove(\"\")\n",
    "        txt_ls= [txt+\"\\n\" for txt in txt_ls]\n",
    "        test_size = int(len(txt_ls)*test_ratio)\n",
    "        test = \"\".join(txt_ls[len(txt_ls) - test_size: ])\n",
    "        train = \"\".join(txt_ls[:len(txt_ls) - test_size ])\n",
    "        \n",
    "        fp = open(out_path+ person_name+ \"/\" +\"test_files.txt\", \"w\")\n",
    "        fp.write(test)\n",
    "        fp.close()\n",
    "        \n",
    "        fp = open(out_path+ person_name+ \"/\" +\"train_files.txt\", \"w\")\n",
    "        fp.write(train)\n",
    "        fp.close()\n",
    "        \n",
    "        if print_flag:\n",
    "            print(\"Train:\", len(txt_ls) - test_size)\n",
    "            print(train)\n",
    "            print(\"test: \",test_size)\n",
    "            print(test)\n",
    "        return \n",
    "        \n",
    "        \n",
    "    for dirname, _, filenames in os.walk(root_path + 'data/IndividualData'):\n",
    "        for filename in filenames:\n",
    "            # check every file name in the individual data folder\n",
    "            path = os.path.join(dirname, filename)\n",
    "#             print(\"Path: \",path)\n",
    "            # check if datafile is shm file and is not a test file\n",
    "            if \".shm\" in filename and person_name in path and 'test' not in path:\n",
    "                # If the data file has label file as well, then it is valid\n",
    "                # and we add it to the filename list\n",
    "                event_file_name =  filename.replace(\".shm\",\"-events.txt\")\n",
    "                \n",
    "                if event_file_name in filenames:\n",
    "                    # if both shm and event files exist\n",
    "                    new_file = path.replace(root_path+\"data/\",\"\")\n",
    "                    new_file += \"\\n\"\n",
    "                    new_files.append(new_file)\n",
    "    if shuffle:\n",
    "        random.seed(random_state)\n",
    "        random.shuffle(new_files)\n",
    "        pass\n",
    "    else:\n",
    "        new_files.sort()\n",
    "        \n",
    "    if test_ratio > 0.:\n",
    "        # split train files and test files\n",
    "        test_size = int(len(new_files)*test_ratio)\n",
    "        test_files = new_files[:test_size]\n",
    "        train_files = new_files[test_size:]\n",
    "        # write train files\n",
    "        fp = open(out_path+ person_name+ \"/\" +\"train_files.txt\", \"w\")\n",
    "        train = \"\".join(train_files)\n",
    "        \n",
    "        fp.write(train)\n",
    "        fp.close()\n",
    "        # write test files\n",
    "        fp = open(out_path+ person_name+ \"/\" +\"test_files.txt\", \"w\")\n",
    "        test = \"\".join(test_files)\n",
    "        fp.write(test)\n",
    "        fp.close()\n",
    "        \n",
    "        if print_flag:\n",
    "            print(\"Train:\")\n",
    "            print(train)\n",
    "            print(\"test: \")\n",
    "            print(test)\n",
    "    \n",
    "    fp = open(out_path+person_name+ \"/\"+file_name, \"w\")\n",
    "    all_files = \"\".join(new_files)\n",
    "    fp.write(all_files)\n",
    "    fp.close()\n",
    "    \n",
    "    if print_flag:\n",
    "        print(\"All files: \")\n",
    "        print(all_files)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Person_MealsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset = None,person_name= \"wenkanw\", \n",
    "                 data_indices_file = \"../data-file-indices/\",\n",
    "                 file_name = \"all_files_list\",\n",
    "                 remove_trend = 0,\n",
    "                 remove_walk = 0,\n",
    "                 remove_rest = 0,\n",
    "                 smooth_flag = 1,\n",
    "                 normalize_flag = 1,\n",
    "                 winmin = 6,\n",
    "                 stridesec = 15,\n",
    "                 gtperc = 0.5,\n",
    "                 device = 'cpu',\n",
    "                 ratio_dataset=1,\n",
    "                load_splitted_dataset = False,\n",
    "                 enable_time_feat = False,\n",
    "                 debug_flag= False,\n",
    "                 get_numpy_data= False\n",
    "                ):\n",
    "        \n",
    "        if file_name == \"train\":\n",
    "            file_name = data_indices_file + person_name +\"/\"+\"train_files.txt\"\n",
    "        elif file_name == \"test\":\n",
    "            file_name = data_indices_file + person_name +\"/\"+\"test_files.txt\"\n",
    "        else:\n",
    "            file_name = data_indices_file + person_name +\"/\"+ file_name+\".txt\"\n",
    "            \n",
    "        # Note: file_name is the name of file that contain the list of shm files' names\n",
    "        self.file_name = file_name\n",
    "        self.get_numpy_data = get_numpy_data\n",
    "        self.dataset = dataset\n",
    "        self.person_name = person_name\n",
    "        self.winmin = winmin\n",
    "        self.stridesec = stridesec\n",
    "        self.load_splitted_dataset = load_splitted_dataset\n",
    "        self.remove_trend = remove_trend\n",
    "        self.remove_walk = remove_walk\n",
    "        self.remove_rest = remove_rest\n",
    "        self.smooth_flag = smooth_flag\n",
    "        self.normalize_flag = normalize_flag\n",
    "        self.gtperc = gtperc,\n",
    "        self.ratio_dataset = ratio_dataset\n",
    "        self.enable_time_feat = enable_time_feat\n",
    "        self.device = device\n",
    "        self.debug_flag= debug_flag\n",
    "        if not self.dataset:\n",
    "            self.get_data(person_name)\n",
    "\n",
    "    def get_data(self, person_name):\n",
    "            \n",
    "            \n",
    "            # files_counts, data, samples_indices, labels_array\n",
    "            # Note: the data preprocessing in this function is for global time series dataset\n",
    "            \n",
    "            self.dataset, self.data, self.data_indices, self.labels = load_train_test_data(data_file_list =self.file_name,\n",
    "                                    load_splitted_dataset = False,\n",
    "                                     ratio_dataset=self.ratio_dataset,\n",
    "                                     enabled_time_feat = self.enable_time_feat, \n",
    "                                     winmin = self.winmin, stridesec = self.stridesec,gtperc = self.gtperc,\n",
    "                                     removerest = self.remove_rest,\n",
    "                                     removewalk = self.remove_walk, smooth_flag = self.smooth_flag, normalize_flag=self.normalize_flag, \n",
    "                                     remove_trend = self.remove_trend,\n",
    "                                     debug_flag=self.debug_flag )\n",
    "            \n",
    "            if self.load_splitted_dataset:\n",
    "                self.dataset = self.get_dataset()\n",
    "                \n",
    "            \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        #这里需要注意的是，第一步：read one data，是一个data\n",
    "        data = self.get_item(index)\n",
    "        if self.get_numpy_data:\n",
    "            return data['data'].numpy() ,data['label']\n",
    "        return data['data'],data['label']\n",
    "        \n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return  len(self.dataset) if self.load_splitted_dataset else len(self.data_indices)\n",
    "    def get_item(self, index, tensor_type=True):\n",
    "        \"\"\"\n",
    "        This function is used to obtain one sample data point\n",
    "        \"\"\"\n",
    "        f,start_time, end_time = self.data_indices[index,0], self.data_indices[index,1], self.data_indices[index,2]\n",
    "        sample = self.data[f][start_time : end_time]\n",
    "        data = pd.DataFrame(columns=['data','label'])    \n",
    "        # Add time feature to data\n",
    "#         if self.enable_time_feat:\n",
    "#             time_offset = self.data_indices[index,3]\n",
    "#             freq = 1.0/15.0\n",
    "#             time_feat = np.array([[i for i in range(len(sample))]],dtype=float).transpose()\n",
    "#             time_feat *= freq\n",
    "#             time_feat += float(start_time)* freq\n",
    "#             time_feat += time_offset\n",
    "#             sample = np.concatenate((sample, time_feat),axis=1)\n",
    "        label = self.labels[index]\n",
    "        if tensor_type:\n",
    "            data = {\"data\":torch.tensor(sample, dtype =torch.float, device =  self.device ), 'label': label}\n",
    "        else:\n",
    "            data = {\"data\":sample, 'label': label}\n",
    "        return data\n",
    "    \n",
    "    def get_dataset(self, start_index = None, end_index = None):\n",
    "        \"\"\"\n",
    "        This function is used to obtain the whole dataset in pandas or part of whole dataset\n",
    "        It is good to use this to sample some data to analyze\n",
    "        \"\"\"\n",
    "        start_i = 0 if not start_index else start_index\n",
    "        end_i = self.__len__() if not end_index else end_index\n",
    "        \n",
    "        dataset = pd.DataFrame(columns=['data','label'])\n",
    "        for i in tqdm(range(start_i, end_i)):\n",
    "            data = self.get_item(i)\n",
    "            dataset = dataset.append(data,ignore_index=True)\n",
    "        self.dataset = dataset\n",
    "        return self.dataset\n",
    "    \n",
    "    def sample(self, num = 1000,random_state = None):\n",
    "        \"\"\"\n",
    "        Simply sample part of data for analysis\n",
    "        \"\"\"\n",
    "        if random_state != None:\n",
    "            np.random.seed(random_state)\n",
    "            \n",
    "        sample_data = pd.DataFrame(columns=['data','label'])\n",
    "        indices = np.random.choice(len(self.labels), num)\n",
    "        for i in tqdm(indices):\n",
    "            data = self.get_item(i)\n",
    "            data[\"data\"] = data[\"data\"].numpy()\n",
    "            sample_data = sample_data.append(data,ignore_index=True)\n",
    "        return sample_data\n",
    "    \n",
    "    def get_subset(self, indices_ls):\n",
    "        axdata = []\n",
    "        aydata = []\n",
    "        for i in indices_ls:\n",
    "            data = self.get_item(i, tensor_type=False)\n",
    "            sample = data['data']\n",
    "            label = data['label']\n",
    "            axdata.append(sample)\n",
    "            aydata.append(label)\n",
    "        subsetData = np.array(axdata, copy=True) # Undersampled Balanced Training Set\n",
    "        subsetLabels = np.array(aydata, copy=True)\n",
    "        del axdata\n",
    "        del aydata\n",
    "        return subsetData, subsetLabels\n",
    "    \n",
    "    def get_GT_segment(self,root_path = \"../data/\",print_file=False):\n",
    "        file_ls = []\n",
    "        fp = open(self.file_name,\"r\")\n",
    "        txt = fp.read()\n",
    "        fp.close()\n",
    "        file_ls = txt.split(\"\\n\")\n",
    "        while '' in file_ls:\n",
    "            file_ls.remove('')\n",
    "        \n",
    "        start_ls = []\n",
    "        end_ls = []\n",
    "        total_events =[]\n",
    "        for file_name in file_ls:\n",
    "            file_name = root_path + file_name\n",
    "            TotalEvents, EventStart, EventEnd, EventNames, TimeOffset,EndTime = loadEvents(file_name, debug_flag = False, print_file=print_file)\n",
    "            start_ls.append(EventStart[:TotalEvents])\n",
    "            end_ls.append(EventEnd[:TotalEvents])\n",
    "            \n",
    "        return  start_ls, end_ls\n",
    "        \n",
    "        \n",
    "    def get_mealdataset_info(self,person_name = None,file_ls = [], file_ls_doc=None,root_path = \"../data/\",print_file=False):\n",
    "        \"\"\"\n",
    "        if file_ls is not given, then get file_ls according to person_name\n",
    "        file path = root_path + file name in all_files_list.txt\n",
    "\n",
    "        return:\n",
    "            meal event count, total minutes of all meals, total hours of all meals,total day counts\n",
    "\n",
    "        \"\"\"\n",
    "        if person_name ==None:\n",
    "            person_name = self.person_name\n",
    "        if len(file_ls) ==0:\n",
    "            if file_ls_doc != None:\n",
    "                data_indices_file = \"../data-file-indices/\" +person_name+\"/\"+ file_ls_doc\n",
    "                fp = open(data_indices_file,\"r\")\n",
    "            else:\n",
    "                fp = open(self.file_name,\"r\")\n",
    "            txt = fp.read()\n",
    "            fp.close()\n",
    "            file_ls = txt.split(\"\\n\")\n",
    "            while '' in file_ls:\n",
    "                file_ls.remove('')\n",
    "\n",
    "        meal_counts = 0\n",
    "        sec_counts = 0\n",
    "        min_counts = 0\n",
    "        hour_counts = 0\n",
    "        total_hours = 0\n",
    "        total_mins = 0\n",
    "        total_sec = 0\n",
    "        day_counts = len(file_ls)\n",
    "        for file_name in file_ls:\n",
    "            file_name = root_path + file_name\n",
    "            TotalEvents, EventStart, EventEnd, EventNames, TimeOffset,EndTime = loadEvents(file_name, debug_flag = False, print_file=print_file)\n",
    "            meal_counts += TotalEvents\n",
    "            total_sec +=  abs(EndTime - TimeOffset)\n",
    "#             total_hours += (EndTime//(60*60) - TimeOffset//(60*60))\n",
    "#             total_mins  += (EndTime%(60*60) - TimeOffset//(60*60))\n",
    "            for i in range(len(EventStart)):\n",
    "                sec_counts += ( EventEnd[i]- EventStart[i])//(15)\n",
    "        total_hours = total_sec//(60*60)\n",
    "        min_counts = sec_counts//60\n",
    "        hour_counts = min_counts//60\n",
    "        \n",
    "        return meal_counts, min_counts,hour_counts, day_counts, total_hours\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "                \n",
    "def balance_data_indices(labels, data_indices=None,sample_num = 4000,mode= \"under\", replace = False,shuffle=True, random_state = 1000):\n",
    "    \"\"\"\n",
    "    sample_num: number of samples of each class after balancing\n",
    "    mode: \n",
    "        under - undersampling\n",
    "        over - oversampling\n",
    "        mix - undersampling negative samples + oversampling positive samples, each class has sample_num amount samples in this mode\n",
    "    return:\n",
    "        balanced indices\n",
    "    \"\"\"\n",
    "    if data_indices:\n",
    "        eat_labels_index = [data_indices[i] for i, e in enumerate(labels) if e >= 0.5]\n",
    "        not_eat_labels_index = [data_indices[i] for i, e in enumerate(labels) if e < 0.5]\n",
    "    else:\n",
    "        eat_labels_index = [i for i, e in enumerate(labels) if e >= 0.5]\n",
    "        not_eat_labels_index = [i for i, e in enumerate(labels) if e < 0.5]\n",
    "        \n",
    "    eat_index = eat_labels_index\n",
    "    not_eat_index = not_eat_labels_index\n",
    "    if random_state != None:\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    if mode == \"over\":\n",
    "        eat_index = np.random.choice(eat_labels_index,len(not_eat_labels_index)).tolist()\n",
    "        pass\n",
    "    elif mode == \"under\":\n",
    "        not_eat_index = np.random.choice(not_eat_labels_index,len(eat_labels_index),replace = replace).tolist()\n",
    "        pass\n",
    "    else:\n",
    "        #default as mix\n",
    "        eat_index = np.random.choice(eat_labels_index,sample_num, replace = replace).tolist()\n",
    "        not_eat_index = np.random.choice(not_eat_labels_index,sample_num, replace = replace).tolist()\n",
    "        pass\n",
    "    \n",
    "    indices_balanced = eat_index + not_eat_index\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices_balanced)\n",
    "    \n",
    "    return indices_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report,recall_score, precision_score\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report,recall_score, precision_score\n",
    "\n",
    "\n",
    "def data_parallel(module, input, device_ids, output_device=None):\n",
    "    if not device_ids:\n",
    "        return module(input)\n",
    "\n",
    "    if output_device is None:\n",
    "        output_device = device_ids[0]\n",
    "\n",
    "    replicas = nn.parallel.replicate(module, device_ids)\n",
    "    inputs = nn.parallel.scatter(input, device_ids)\n",
    "    replicas = replicas[:len(inputs)]\n",
    "    outputs = nn.parallel.parallel_apply(replicas, inputs)\n",
    "    return nn.parallel.gather(outputs, output_device)\n",
    "\n",
    "\n",
    "def eval_model(model,dataloader,device=\"cpu\"):\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    # without update\n",
    "    with torch.no_grad():\n",
    "        for samples, labels in dataloader:\n",
    "            samples = samples.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(samples).squeeze()\n",
    "            #print(\"Output: \", outputs)\n",
    "            outputs = torch.round(torch.sigmoid(outputs))\n",
    "            preds = outputs>=0.5\n",
    "            preds = preds.to(dtype = torch.float)\n",
    "            preds.requires_grad = False\n",
    "#             _,preds = torch.max(outputs,1)\n",
    "            for i in range(len(preds)):\n",
    "                if preds[i] == 1 and labels[i] == 1:\n",
    "                    TP += 1\n",
    "                if preds[i] == 0 and labels[i] == 1:\n",
    "                    FN += 1\n",
    "            correct += torch.sum((preds == labels)).item()\n",
    "            total += float(len(labels))\n",
    "        acc =100 * correct/ total\n",
    "        recall = TP/(TP+FN)\n",
    "#         print(\"Evaluation Acc: %.4f %%,  Recall: %.4f \"%(acc , recall))\n",
    "    return acc, recall\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "def train_model(model,dataloader, optimizer, criterion,lrscheduler,device=\"cpu\" , n_epochs=20,\n",
    "                earlystopping=True, patience= 5, l1_enabled=True,checkpoint_name =\"checkpoint.pt\" ):\n",
    "    loss_ls = [0.0]\n",
    "    train_acc_ls = [0.0]\n",
    "    valid_acc_ls = [0.0]\n",
    "    valid_acc = 0.0\n",
    "    loss =0.0\n",
    "    train_acc = 0.0\n",
    "    patience_count = 0\n",
    "    best_val_score = 0.0\n",
    "    prev_val_score = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    train_dataloader, valid_dataloader = dataloader\n",
    "    print(\"Training set batch amounts:\", len(train_dataloader))\n",
    "    print(\"Test set :\", len(valid_dataloader))\n",
    "    print(\"Start Training..\")\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        correct_cnt = 0.0\n",
    "        total_cnt = 0.0\n",
    "        TP = 0.\n",
    "        FN = 0.\n",
    "        model.train()\n",
    "        for i, (samples, labels) in enumerate(train_dataloader):\n",
    "            samples = samples.to(device)\n",
    "            labels = labels.to(device, dtype=torch.float32)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # reshape samples\n",
    "            outputs = model(samples).squeeze()\n",
    "\n",
    "            #print(\"Output: \", outputs, \"label: \", labels)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            if l1_enabled:\n",
    "                L1_loss = model.l1_loss(0.01).to(device)\n",
    "                loss += L1_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # prediction\n",
    "            #_,preds = torch.max(outputs,1)\n",
    "            outputs = torch.round(torch.sigmoid(outputs))\n",
    "            preds = outputs>=0.5\n",
    "            preds = preds.to(dtype = torch.float)\n",
    "            preds.requires_grad = False\n",
    "            \n",
    "            # Compute count of TP, FN\n",
    "            for j in range(len(preds)):\n",
    "                if preds[j] == 1. and labels[j] == 1.:\n",
    "                    TP += 1\n",
    "                if preds[j] == 0. and labels[j] == 1.:\n",
    "                    FN += 1\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            correct_cnt += torch.sum((preds == labels)).item()\n",
    "            total_cnt += float(len(labels))\n",
    "            batch_acc = 100. * (preds == labels).sum().item()/ float(len(labels))\n",
    "            if i %50 ==0:\n",
    "                #print(\"===> Batch: %d,  Batch_Loss: %.4f, Train Acc: %.4f %%,  Recall: %.f\\n\"%(i, loss,batch_acc, recall))\n",
    "                pass\n",
    "\n",
    "            \n",
    "        \n",
    "        # Compute accuracy and loss of one epoch\n",
    "        epoch_loss = running_loss / len(train_dataloader)  \n",
    "        epoch_acc = 100* correct_cnt/ total_cnt  # in percentage\n",
    "        correct_cnt = 0.0\n",
    "        total_cnt = 0.0\n",
    "        train_recall = TP/(TP+FN)\n",
    "        \n",
    "        #Validation mode\n",
    "        model.eval()\n",
    "        valid_acc, valid_recall= eval_model(model,valid_dataloader,device=device)\n",
    "        \n",
    "        # record loss and accuracy\n",
    "        valid_acc_ls.append(valid_acc)  \n",
    "        train_acc_ls.append(epoch_acc)\n",
    "        loss_ls.append(epoch_loss)\n",
    "        \n",
    "        if e %1==0:\n",
    "            print(\"Epoch: %d,  Epoch_Loss: %.4f, Train Acc: %.4f %%, Train Recall: %.4f, Validation Acc:  %.4f %%,  Validation Recall: %.4f  \"%(e, epoch_loss,\n",
    "                                                                                     epoch_acc,train_recall,valid_acc, valid_recall))\n",
    "        \n",
    "        # Reset train mode\n",
    "        model.train()\n",
    "        lrscheduler.step(valid_acc)\n",
    "        \n",
    "        \n",
    "        # If earlystopping is enabled, then save model if performance is improved\n",
    "        if earlystopping:\n",
    "            if prev_val_score !=0. and valid_acc < prev_val_score :\n",
    "                patience_count += 1\n",
    "            else:\n",
    "                patience_count = 0\n",
    "                \n",
    "            if patience_count >= patience:\n",
    "                break \n",
    "                \n",
    "            prev_val_score = valid_acc\n",
    "            if valid_acc > best_val_score or best_val_score == 0.0:\n",
    "                best_val_score = valid_acc\n",
    "                torch.save(model,checkpoint_name)\n",
    "                print(\"Checkpoint Saved\")\n",
    "            \n",
    "                \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "            \n",
    "    # Load best model\n",
    "    best_model = torch.load(checkpoint_name)\n",
    "    print(\"Load Best Model.\")\n",
    "    print(\"Training completed\")\n",
    "        \n",
    "    return model, best_model,best_val_score,loss_ls, train_acc_ls, valid_acc_ls\n",
    "            \n",
    "\n",
    "def plot_data(train_acc_ls,valid_acc_ls,loss_ls ):\n",
    "    \"\"\"\n",
    "    Plot validation accuracy, training accuracy and loss\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "    epochs = [i for i in range(len(train_acc_ls))]\n",
    "    _ = sns.lineplot(x=epochs, y= train_acc_ls,ax=ax[0])\n",
    "    _ = sns.lineplot(x=epochs, y= valid_acc_ls,ax=ax[0])\n",
    "    ax[0].set_xlabel(\"Epoches\")\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "    ax[0].legend([\"Training Accuracy\", \"Validation Accuracy\"])\n",
    "    \n",
    "    _ = sns.lineplot(x=epochs[1:], y= loss_ls[1:],ax=ax[1])\n",
    "    ax[1].set_xlabel(\"Epoches\")\n",
    "    ax[1].set_ylabel(\"Training Loss\")\n",
    "    ax[1].set(yscale=\"log\")\n",
    "    plt.show()\n",
    "    \n",
    "def split_train_test_indices(X, y, test_size, random_seed = None):\n",
    "    \"\"\"\n",
    "    This function is to split the training set indices into validation set indices and training set indices\n",
    "    \n",
    "    X: indices of dataset/ subset of dataset\n",
    "    y: labels of dataset / subset of dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, train_test_split\n",
    "\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    \n",
    "    if test_size ==0:\n",
    "        train_indices = X\n",
    "        y_train = y\n",
    "    elif test_size == 1:\n",
    "        test_indices = X\n",
    "        y_test = y\n",
    "    elif test_size >0 and test_size <1:\n",
    "        train_indices, test_indices, y_train, y_test = train_test_split(X,y,\n",
    "                                                            stratify=y, \n",
    "                                                            test_size=test_size,random_state = random_seed)\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid split ratio: %.3f\"%(test_size))\n",
    "    if len(train_indices)>0:\n",
    "        print(\"Train set size: %d, with %d positive samples and %d negative samples\"%(len(y_train),sum(y_train==1),\n",
    "                                                                          sum(y_train==0)))\n",
    "    if len(test_indices)>0:\n",
    "        print(\"Test set size: %d, with %d positive samples and %d negative samples\"%(len(y_test),\n",
    "                                                                          sum(y_test==1),\n",
    "                                                                           sum(y_test==0)))\n",
    "    \n",
    "    return train_indices, test_indices\n",
    "            \n",
    "\n",
    "    \n",
    "def print_settings(winmin,stridesec, EPOCHS):\n",
    "    \"\"\"\n",
    "    This is just a function to print information of training settings\n",
    "    \"\"\"\n",
    "    outfile = sys.stdout\n",
    "\n",
    "    winlength = int(winmin * 60 * 15)\n",
    "    step = int(stridesec * 15)\n",
    "    start_time = datetime.datetime.now()\n",
    "    arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "          \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "          \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "\n",
    "    [os.system(cmd) for cmd in arr]\n",
    "    print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "    print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "    print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "    \n",
    "    \n",
    "\n",
    "def cross_validation(dataset, data_indices, model,n_epochs=30,k=5, device=\"cpu\", random_state = 1000, checkpoint_path = \"./\"  ):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "    best_val_score = 0\n",
    "    overall_best_model = None\n",
    "    best_fold = None\n",
    "    all_loss_ls = []\n",
    "    all_train_acc_ls = []\n",
    "    all_valid_acc_ls = []\n",
    "    data_indices = np.array(data_indices)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k)\n",
    "    \n",
    "    labels = dataset.labels[data_indices]\n",
    "    np.random.seed(random_state)\n",
    "    seeds = np.random.randint(low=0, high=1000,size=k)\n",
    "    \n",
    "    \n",
    "    for fold_ind, (train_fold, valid_fold) in enumerate(skf.split(data_indices, labels)):\n",
    "        torch.manual_seed(seeds[fold_ind])\n",
    "        \n",
    "        print(\"===========================> Running Fold: %d\"%(fold_ind))\n",
    "        print()\n",
    "        train_indices = data_indices[train_fold]\n",
    "        valid_indices = data_indices[valid_fold]\n",
    "        # Train set    \n",
    "        train_set_fold = torch.utils.data.Subset(dataset, train_indices)\n",
    "        train_loader_fold = torch.utils.data.DataLoader(train_set_fold,batch_size=32, shuffle=True)\n",
    "\n",
    "        # validation set\n",
    "        valid_set_fold = torch.utils.data.Subset(dataset, valid_indices)\n",
    "        valid_loader_fold = torch.utils.data.DataLoader(valid_set_fold,batch_size=32, shuffle=True)\n",
    "          \n",
    "        # Re-initialize models\n",
    "        cv_model = model\n",
    "        # Since I use a dynamic created layer in network, need to input a sample to initialize the model first\n",
    "        cv_model.apply(weights_init)\n",
    "        cv_model.to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        optimizer = optim.Adam(cv_model.parameters(),lr=0.01,  weight_decay=0.1)\n",
    "        lrscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',patience= 2, factor = 0.1,threshold=0.01)\n",
    "\n",
    "\n",
    "        dataloader = (train_loader_fold,valid_loader_fold )\n",
    "        cv_model, best_model,val_score,loss_ls, train_acc_ls, valid_acc_ls = train_model(cv_model,dataloader, optimizer, criterion, \n",
    "                                                                                      lrscheduler, device= device,\n",
    "                                                                            n_epochs=n_epochs, patience = 5, l1_enabled=False,\n",
    "                                                                            checkpoint_name =checkpoint_path+\"cross_valid_checkpoint_\"+str(fold_ind)+\".pt\")\n",
    "        best_model.eval()\n",
    "        valid_acc, recall = eval_model(best_model, valid_loader_fold,device)\n",
    "        \n",
    "        all_valid_acc_ls.append(valid_acc)\n",
    "        \n",
    "        print(\"Fold %d Completed\"%(fold_ind))\n",
    "    print(\"Cross Validation Completed，score is %.4f %%\"%( np.mean(all_valid_acc_ls)))\n",
    "    \n",
    "    return all_valid_acc_ls\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 1.)\n",
    "#         nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "#         nn.init.normal_(m.weight.data, 0.0, 0.01)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 1.)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "        \n",
    "        \n",
    "def test_model(model, winmin=3, stridesec = 15,names= [\"wenkanw\"],random_seed=1000, split_day=False):\n",
    "    \"\"\"\n",
    "    A function to test tensorflow model\n",
    "    \"\"\"\n",
    "    perf = {\"name\":[],\"model\":[],\"win(sec)\":[], \"acc\":[],\"recall\":[], \"auc\":[]}\n",
    "    for name in names:\n",
    "        person = name\n",
    "        if split_day:\n",
    "            meal_data_test = Person_MealsDataset(person_name= person, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "\n",
    "            # balance test set\n",
    "            testset_labels = meal_data_test.labels\n",
    "            test_indices = balance_data_indices(testset_labels,data_indices=[i for i in range(len(meal_data_test))] ,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            # get numpy dataset\n",
    "            test_Data, test_Labels = meal_data_test.get_subset(test_indices)\n",
    "        else:            \n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "            samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "            # split train set and test set\n",
    "            train_indices, test_indices = split_train_test_indices(X= [i for i in range(len(labels))],\n",
    "                                                                            y = labels, test_size = 0.2,\n",
    "                                                                           random_seed = random_seed)\n",
    "            testset_labels = labels[test_indices]\n",
    "            test_indices = balance_data_indices(testset_labels,data_indices= test_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "            \n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "        predictions = model.predict(x=test_Data)\n",
    "        threshold = 0.5\n",
    "        acc =  accuracy_score(predictions>=threshold,test_Labels)\n",
    "        recall = recall_score(predictions>=threshold,test_Labels)\n",
    "        auc = roc_auc_score(predictions>=threshold,test_Labels)\n",
    "        print(\"Test Accuracy:\", acc)\n",
    "        print(\"Recall Accuracy:\", recall)\n",
    "        print(\"AUC Score:\", auc)\n",
    "        perf[\"name\"].append(name)\n",
    "        perf[\"model\"].append(\"ActiModel\")\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        perf[\"acc\"].append(acc)\n",
    "        perf[\"recall\"].append(recall)\n",
    "        perf[\"auc\"].append(auc)\n",
    "\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    return perf_df\n",
    "\n",
    "def train_models_v2(model, win_ls = [],EPOCHS = 10,stridesec = 1,name = \"wenkanw\",model_name=\"v2\" ,random_seed= 1000, split_day=False):\n",
    "    \"\"\"\n",
    "    A function to train tensorflow models\n",
    "    \"\"\"\n",
    "    from numpy.random import seed\n",
    "    seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "#     tf.set_random_seed(random_seed)\n",
    "    from datetime  import datetime\n",
    "    batch_size = 128\n",
    "    outfile = sys.stdout\n",
    "    perf = {\"model\":[],\"win(sec)\":[], \"acc\":[],\"recall\":[], \"auc\":[]}\n",
    "    model_ls = []\n",
    "    hist_ls = []\n",
    "    for winsize in win_ls:\n",
    "        tf.random.set_seed(random_seed)\n",
    "        seed(random_seed)\n",
    "        \n",
    "        winmin = winsize\n",
    "        winlength = int(winmin * 60 * 15)\n",
    "        step = int(stridesec * 15)\n",
    "        start_time = datetime.now()\n",
    "        arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "              \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "              \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "        [os.system(cmd) for cmd in arr]\n",
    "        print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "        print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "        print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "\n",
    "\n",
    "        pathtemp = \"../models/\" + name+\"_models\" +\"/\"+model_name+\"_M_F_\"\n",
    "        modelpath = pathtemp + \"{:f}Min.h5\".format(winmin)\n",
    "        jsonpath = pathtemp + \"{:f}Min.json\".format(winmin)\n",
    "        print(\"Model to Save: \",modelpath)\n",
    "        print()\n",
    "        # Load the dataset\n",
    "        \n",
    "        person = name\n",
    "        if split_day:\n",
    "            create_train_test_file_list(file_name= \"all_files_list.txt\",person_name =name,\n",
    "                         out_path = \"../data-file-indices/\",root_path= \"../\",\n",
    "                         test_ratio = 0.2, print_flag = True, shuffle=True, random_state=random_seed)\n",
    "\n",
    "            meal_data_train = Person_MealsDataset(person_name= person, file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "            meal_data_test = Person_MealsDataset(person_name= person, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "\n",
    "            train_indices, valid_indices = split_train_test_indices(X= [i for i in range(len(meal_data_train.labels))],\n",
    "                                                                    y = meal_data_train.labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            #balanced train set\n",
    "            trainset_labels = meal_data_train.labels[train_indices]\n",
    "            train_indices = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "\n",
    "            # balance test set\n",
    "            testset_labels = meal_data_test.labels\n",
    "            test_indices = balance_data_indices(testset_labels,data_indices=[i for i in range(len(meal_data_test))] ,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            # get numpy dataset\n",
    "            balancedData, balancedLabels = meal_data_train.get_subset(train_indices)\n",
    "            valid_balancedData, valid_balancedLabels = meal_data_train.get_subset(valid_indices)\n",
    "            test_Data, test_Labels = meal_data_test.get_subset(test_indices)\n",
    "\n",
    "        else:\n",
    "        \n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "            samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "            # split train set and test set\n",
    "            train_indices, test_indices = split_train_test_indices(X= [i for i in range(len(labels))],\n",
    "                                                                    y = labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            # balance train set\n",
    "            trainset_labels = labels[train_indices]\n",
    "            train_indices_balanced = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            #balance test set\n",
    "            testset_labels = labels[test_indices]\n",
    "            test_indices = balance_data_indices(testset_labels,data_indices= test_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "\n",
    "            train_set_balanced = torch.utils.data.Subset(meal_data, train_indices_balanced)\n",
    "            test_set = torch.utils.data.Subset(meal_data, test_indices)\n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "            test_loader = torch.utils.data.DataLoader(test_set ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "            print(\"Data Loader Created\")            \n",
    "            \n",
    "            # split validation set\n",
    "            balanced_trainset_labels = labels[train_indices_balanced]\n",
    "            train_indices, valid_indices = split_train_test_indices(X= train_indices_balanced,\n",
    "                                                                    y = balanced_trainset_labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            valid_set_balanced = torch.utils.data.Subset(meal_data, valid_indices)\n",
    "            valid_loader = torch.utils.data.DataLoader(valid_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "            balancedData, balancedLabels = meal_data.get_subset(train_indices)\n",
    "            valid_balancedData, valid_balancedLabels = meal_data.get_subset(valid_indices)\n",
    "            test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "        \n",
    "\n",
    "        #training settings\n",
    "        mcp_save = tf.keras.callbacks.ModelCheckpoint(modelpath, save_best_only=True, monitor='accuracy')\n",
    "        \n",
    "\n",
    "        scheduler = tf.keras.callbacks.ReduceLROnPlateau( monitor='val_loss', factor=0.1, patience=3, verbose=0,\n",
    "                                             mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.)\n",
    "        #train model\n",
    "        H = model.fit(x=balancedData, y = balancedLabels,\n",
    "                       validation_data=(valid_balancedData, valid_balancedLabels),\n",
    "                    epochs = EPOCHS, batch_size=batch_size, verbose=1,\n",
    "                    callbacks=[mcp_save,scheduler]) # removed addons.LossHistory(jsonpath) for compatibility with TensorFlow 2.2.0, needs to be re-added at some point\n",
    "\n",
    "        print(\"Max value: \", max(H.history['accuracy']), \" at epoch\", np.argmax(H.history['accuracy']) + 1)\n",
    "\n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "        predictions = model.predict(x=test_Data)\n",
    "        threshold = 0.5\n",
    "        acc =  accuracy_score(predictions>=threshold,test_Labels)\n",
    "        recall = recall_score(predictions>=threshold,test_Labels)\n",
    "        auc = roc_auc_score(predictions>=threshold,test_Labels)\n",
    "        print(\"Test Accuracy:\", acc)\n",
    "        print(\"Recall Accuracy:\", recall)\n",
    "        print(\"AUC Score:\", auc)\n",
    "\n",
    "        perf[\"model\"].append(\"ActiModel\")\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        perf[\"acc\"].append(acc)\n",
    "        perf[\"recall\"].append(recall)\n",
    "        perf[\"auc\"].append(auc)\n",
    "        model_ls.append(model)\n",
    "        hist_ls.append(H)\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    print(perf_df)\n",
    "    return perf_df, model_ls, hist_ls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Discriminator_BN_Bias(nn.Module):\n",
    "    def __init__(self, ngpu, input_shape , out_fea = 1, bias=True):\n",
    "        super(Discriminator_BN_Bias, self).__init__()\n",
    "        \n",
    "        # (input_shape[0], input_shape[1] )=  (number of data point, 6 axis channel )\n",
    "        win_size,in_channels = input_shape[0], input_shape[1]\n",
    "        self.in_channels = in_channels\n",
    "        self.ngpu = ngpu  \n",
    "        self.out_fea = out_fea\n",
    "        filter_num = 10\n",
    "        \n",
    "        self.conv0 = nn.Conv1d(in_channels = self.in_channels, \n",
    "                               out_channels = filter_num, \n",
    "                               kernel_size= 44,  stride= 2, padding=0, bias=bias)\n",
    "        \n",
    "        self.relu0= nn.ReLU()\n",
    "        num_fea = (win_size-44)//2 +1\n",
    "        self.conv1 = nn.Conv1d(filter_num,filter_num, kernel_size= 20,stride= 2, padding=0, bias=bias)\n",
    "        self.relu1= nn.ReLU()\n",
    "        num_fea = (num_fea-20)//2 +1\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(filter_num)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(filter_num,filter_num, kernel_size= 4, stride= 2, padding=0, bias=bias)\n",
    "        self.relu2= nn.ReLU()\n",
    "        \n",
    "        num_fea = (num_fea-4)//2 +1\n",
    "        self.bn2 = nn.BatchNorm1d(filter_num)\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=10)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = None \n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=200, out_features=out_fea, bias=bias)\n",
    "        self.softmax = nn.Softmax(dim=out_fea)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        nn.init.normal_(self.conv0.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.conv1.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.conv2.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.linear2.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv0.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv1.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv2.bias.data, 0.0, 1.)\n",
    "#         nn.init.normal_(self.avgpool.weight.data, 0.0, 1.)\n",
    "        \n",
    "    def l1_loss(self,factor=0.01):\n",
    "        l1_crit = nn.L1Loss(size_average=False)\n",
    "        reg_loss = 0.\n",
    "        loss = 0.\n",
    "        layers = [self.conv0, self.conv1, self.conv2]\n",
    "        for layer in layers:\n",
    "            for p in layer.parameters():\n",
    "                #print(p)\n",
    "                reg_loss += l1_crit(p, torch.zeros(p.shape))\n",
    "\n",
    "        loss = factor * reg_loss\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = input.permute(0,2,1)\n",
    "        x = self.conv0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = self.conv1(x)\n",
    "        x=  self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x=  self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.avgpool(x)\n",
    "        #print(\"Pooling shape:\",x.shape)\n",
    "        x = self.flatten(x)\n",
    "        if self.linear1 == None:\n",
    "            self.linear1 = nn.Linear(in_features=x.shape[1], out_features=200, bias=True)\n",
    "            nn.init.normal_(self.linear1.weight.data, 0.0, 1.)\n",
    "            nn.init.normal_(self.linear1.bias.data, 0.0, 1.)\n",
    "            \n",
    "        x = self.relu4(self.linear1(x))\n",
    "        out = self.linear2(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels ,bias= True, filter_num = 10):\n",
    "        super(BasicBlock, self).__init__()       \n",
    "        self.conv0 = nn.Conv1d(in_channels = in_channels, \n",
    "                               out_channels = filter_num, \n",
    "                               kernel_size= 3,  stride= 1, padding=1, bias=bias)\n",
    "        self.bn0 = nn.BatchNorm1d(filter_num)\n",
    "        self.relu0 = nn.ReLU()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels = in_channels, \n",
    "                               out_channels = filter_num, \n",
    "                               kernel_size= 3,  stride= 1, padding=1, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm1d(filter_num)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv0(x)\n",
    "        out = self.bn0(out)\n",
    "        out = self.relu0(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        \n",
    "        out += residual\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Discriminator_ResNet(nn.Module):\n",
    "    def __init__(self, ngpu, input_shape , out_fea = 1):\n",
    "        super(Discriminator_ResNet, self).__init__()\n",
    "        \n",
    "        # (input_shape[0], input_shape[1] )=  (number of data point, 6 axis channel )\n",
    "        win_size, in_channels= input_shape[0], input_shape[1]\n",
    "        self.in_channels = in_channels\n",
    "        self.ngpu = ngpu  \n",
    "        self.out_fea = out_fea\n",
    "        filter_num = 10\n",
    "        \n",
    "        self.conv0 = nn.Conv1d(in_channels = self.in_channels, \n",
    "                               out_channels = filter_num, \n",
    "                               kernel_size= 44,  stride= 2, padding=0, bias=True)\n",
    "        \n",
    "        self.relu0= nn.ReLU()\n",
    "        num_fea = (win_size-44)//2 +1\n",
    "        self.conv1 = nn.Conv1d(filter_num,filter_num, kernel_size= 20,stride= 2, padding=0, bias=True)\n",
    "        self.relu1= nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(filter_num)\n",
    "        \n",
    "        self.block1 =  BasicBlock( in_channels=filter_num  ,bias= True, filter_num = filter_num)\n",
    "        self.block2 =  BasicBlock( in_channels=filter_num  ,bias= True, filter_num = filter_num)\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=10)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = None \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=200, out_features=out_fea, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=out_fea)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        nn.init.normal_(self.conv0.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.conv1.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.linear2.bias.data, 0.0, 1.)\n",
    "        \n",
    "    def l1_loss(self,factor=0.01):\n",
    "        l1_crit = nn.L1Loss(size_average=False)\n",
    "        reg_loss = 0.\n",
    "        loss = 0.\n",
    "        layers = [self.conv0, self.conv1]\n",
    "        for layer in layers:\n",
    "            for p in layer.parameters():\n",
    "                #print(p)\n",
    "                reg_loss += l1_crit(p, torch.zeros(p.shape))\n",
    "\n",
    "        loss = factor * reg_loss\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "\n",
    "        x = input.permute(0,2,1)\n",
    "        x = self.conv0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = self.conv1(x)\n",
    "        x=  self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        #print(\"Pooling shape:\",x.shape)\n",
    "        x = self.flatten(x)\n",
    "        if self.linear1 == None:\n",
    "            self.linear1 = nn.Linear(in_features=x.shape[1], out_features=200, bias=True)\n",
    "            nn.init.normal_(self.linear1.weight.data, 0.0, 1.)\n",
    "            nn.init.normal_(self.linear1.bias.data, 0.0, 1.)\n",
    "            \n",
    "        x = self.relu2(self.linear1(x))\n",
    "        out = self.linear2(x)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "from torch import nn\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu, input_shape , out_fea = 1, device=\"cpu\"):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.device =device\n",
    "        # (input_shape[0], input_shape[1] )=  (number of data point, 6 axis channel )\n",
    "        win_size, in_channels = input_shape[0], input_shape[1]\n",
    "        self.in_channels = in_channels\n",
    "        self.ngpu = ngpu  \n",
    "        self.out_fea = out_fea\n",
    "        filter_size = 10\n",
    "        self.conv0 = nn.Conv1d(in_channels = self.in_channels, \n",
    "                               out_channels = filter_size, \n",
    "                               kernel_size= 44,  stride= 2, padding=0, bias=False)\n",
    "        \n",
    "        self.relu0= nn.ReLU()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(filter_size,filter_size, kernel_size= 20,stride= 2, padding=0, bias=False)\n",
    "        self.relu1= nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(filter_size,filter_size, kernel_size= 4, stride= 2, padding=0, bias=False)\n",
    "        self.relu2= nn.ReLU()\n",
    "        \n",
    "#         self.conv3 = nn.Conv1d(filter_size,filter_size, kernel_size= 4, stride= 2, padding=0, bias=False)\n",
    "#         self.relu3= nn.LeakyReLU(0, inplace=True)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=10)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = None \n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=200, out_features=out_fea, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=out_fea)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        nn.init.normal_(self.conv0.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.conv1.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.conv2.weight.data, 0.0, 1.)\n",
    "        nn.init.normal_(self.linear2.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv0.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv1.bias.data, 0.0, 1.)\n",
    "        #nn.init.normal_(self.conv2.bias.data, 0.0, 1.)\n",
    "#         nn.init.normal_(self.avgpool.weight.data, 0.0, 1.)\n",
    "        \n",
    "    def l1_loss(self,factor=0.01):\n",
    "        l1_crit = nn.L1Loss(size_average=False)\n",
    "        reg_loss = 0.\n",
    "        loss = 0.\n",
    "        layers = [self.conv1, self.conv2]\n",
    "        for layer in layers:\n",
    "            for p in layer.parameters():\n",
    "                #print(p)\n",
    "                reg_loss += l1_crit(p, torch.zeros(p.shape, device= self.device))\n",
    "\n",
    "        loss = factor * reg_loss\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input):\n",
    "#         print(\"input shape:\",input.shape)\n",
    "        x = input.permute(0,2,1)\n",
    "        x = self.conv0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.avgpool(x)\n",
    "        #print(\"Pooling shape:\",x.shape)\n",
    "        x = self.flatten(x)\n",
    "        if self.linear1 == None:\n",
    "            self.linear1 = nn.Linear(in_features=x.shape[1], out_features=200, bias=True)\n",
    "            nn.init.normal_(self.linear1.weight.data, 0.0, 1.)\n",
    "            nn.init.normal_(self.linear1.bias.data, 0.0, 1.)\n",
    "            \n",
    "        x = self.relu4(self.linear1(x))\n",
    "        out = self.linear2(x)\n",
    "#         out = self.sigmoid(x)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  ../data/ShimmerData/P2001/P2001.shm\n",
      "Path:  ../data/ShimmerData/P2011/P2011.shm\n",
      "Path:  ../data/ShimmerData/P2012/P2012.shm\n",
      "Path:  ../data/ShimmerData/P2013/P2013.shm\n",
      "Path:  ../data/ShimmerData/P2014/P2014.shm\n",
      "Path:  ../data/ShimmerData/P2015/P2015.shm\n",
      "Path:  ../data/ShimmerData/P2016/P2016.shm\n",
      "Path:  ../data/ShimmerData/P2017/P2017.shm\n",
      "Path:  ../data/ShimmerData/P2018/P2018.shm\n",
      "Path:  ../data/ShimmerData/P2020/P2020.shm\n",
      "Path:  ../data/ShimmerData/P2030/P2030.shm\n",
      "Path:  ../data/ShimmerData/P2031/P2031.shm\n",
      "Path:  ../data/ShimmerData/P2033/P2033.shm\n",
      "Path:  ../data/ShimmerData/P2035/P2035.shm\n",
      "Path:  ../data/ShimmerData/P2036/P2036.shm\n",
      "Path:  ../data/ShimmerData/P2037/P2037.shm\n",
      "Path:  ../data/ShimmerData/P2038/P2038.shm\n",
      "Path:  ../data/ShimmerData/P2044/P2044.shm\n",
      "Path:  ../data/ShimmerData/P2046/P2046.shm\n",
      "Path:  ../data/ShimmerData/P2051/P2051.shm\n",
      "Path:  ../data/ShimmerData/P2055/P2055.shm\n",
      "Path:  ../data/ShimmerData/P2057/P2057.shm\n",
      "Path:  ../data/ShimmerData/P2058/P2058.shm\n",
      "Path:  ../data/ShimmerData/P2063/P2063.shm\n",
      "Path:  ../data/ShimmerData/P2100/P2100.shm\n",
      "Path:  ../data/ShimmerData/P2101/P2101.shm\n",
      "Path:  ../data/ShimmerData/P2102/P2102.shm\n",
      "Path:  ../data/ShimmerData/P2103/P2103.shm\n",
      "Path:  ../data/ShimmerData/P2105/P2105.shm\n",
      "Path:  ../data/ShimmerData/P2106/P2106.shm\n",
      "Path:  ../data/ShimmerData/P2107/P2107.shm\n",
      "Path:  ../data/ShimmerData/P2108/P2108.shm\n",
      "Path:  ../data/ShimmerData/P2109/P2109.shm\n",
      "Path:  ../data/ShimmerData/P2110/P2110.shm\n",
      "Path:  ../data/ShimmerData/P2111/P2111.shm\n",
      "Path:  ../data/ShimmerData/P2113/P2113.shm\n",
      "Path:  ../data/ShimmerData/P2114/P2114.shm\n",
      "Path:  ../data/ShimmerData/P2115/P2115.shm\n",
      "Path:  ../data/ShimmerData/P2116/P2116.shm\n",
      "Path:  ../data/ShimmerData/P2117/P2117.shm\n",
      "Path:  ../data/ShimmerData/P2118/P2118.shm\n",
      "Path:  ../data/ShimmerData/P2120/P2120.shm\n",
      "Path:  ../data/ShimmerData/P2121/P2121.shm\n",
      "Path:  ../data/ShimmerData/P2122/P2122.shm\n",
      "Path:  ../data/ShimmerData/P2123/P2123.shm\n",
      "Path:  ../data/ShimmerData/P2125/P2125.shm\n",
      "Path:  ../data/ShimmerData/P2126/P2126.shm\n",
      "Path:  ../data/ShimmerData/P2127/P2127.shm\n",
      "Path:  ../data/ShimmerData/P2128/P2128.shm\n",
      "Path:  ../data/ShimmerData/P2129/P2129.shm\n",
      "Path:  ../data/ShimmerData/P2130/P2130.shm\n",
      "Path:  ../data/ShimmerData/P2131/P2131.shm\n",
      "Path:  ../data/ShimmerData/P2132/P2132.shm\n",
      "Path:  ../data/ShimmerData/P2133/P2133.shm\n",
      "Path:  ../data/ShimmerData/P2134/P2134.shm\n",
      "Path:  ../data/ShimmerData/P2135/P2135.shm\n",
      "Path:  ../data/ShimmerData/P2136/P2136.shm\n",
      "Path:  ../data/ShimmerData/P2137/P2137.shm\n",
      "Path:  ../data/ShimmerData/P2138/P2138.shm\n",
      "Path:  ../data/ShimmerData/P2139/P2139.shm\n",
      "Path:  ../data/ShimmerData/P2140/P2140.shm\n",
      "Path:  ../data/ShimmerData/P2141/P2141.shm\n",
      "Path:  ../data/ShimmerData/P2142/P2142.shm\n",
      "Path:  ../data/ShimmerData/P2143/P2143.shm\n",
      "Path:  ../data/ShimmerData/P2144/P2144.shm\n",
      "Path:  ../data/ShimmerData/P2145/P2145.shm\n",
      "Path:  ../data/ShimmerData/P2146/P2146.shm\n",
      "Path:  ../data/ShimmerData/P2147/P2147.shm\n",
      "Path:  ../data/ShimmerData/P2148/P2148.shm\n",
      "Path:  ../data/ShimmerData/P2149/P2149.shm\n",
      "Path:  ../data/ShimmerData/P2150/P2150.shm\n",
      "Path:  ../data/ShimmerData/P2151/P2151.shm\n",
      "Path:  ../data/ShimmerData/P2152/P2152.shm\n",
      "Path:  ../data/ShimmerData/P2153/P2153.shm\n",
      "Path:  ../data/ShimmerData/P2154/P2154.shm\n",
      "Path:  ../data/ShimmerData/P2155/P2155.shm\n",
      "Path:  ../data/ShimmerData/P2157/P2157.shm\n",
      "Path:  ../data/ShimmerData/P2158/P2158.shm\n",
      "Path:  ../data/ShimmerData/P2159/P2159.shm\n",
      "Path:  ../data/ShimmerData/P2160/P2160.shm\n",
      "Path:  ../data/ShimmerData/P2161/P2161.shm\n",
      "Path:  ../data/ShimmerData/P2164/P2164.shm\n",
      "Path:  ../data/ShimmerData/P2165/P2165.shm\n",
      "Path:  ../data/ShimmerData/P2166/P2166.shm\n",
      "Path:  ../data/ShimmerData/P2168/P2168.shm\n",
      "Path:  ../data/ShimmerData/P2169/P2169.shm\n",
      "Path:  ../data/ShimmerData/P2170/P2170.shm\n",
      "Path:  ../data/ShimmerData/P2171/P2171.shm\n",
      "Path:  ../data/ShimmerData/P2172/P2172.shm\n",
      "Path:  ../data/ShimmerData/P2173/P2173.shm\n",
      "Path:  ../data/ShimmerData/P2174/P2174.shm\n",
      "Path:  ../data/ShimmerData/P2175/P2175.shm\n",
      "Path:  ../data/ShimmerData/P2176/P2176.shm\n",
      "Path:  ../data/ShimmerData/P2177/P2177.shm\n",
      "Path:  ../data/ShimmerData/P2178/P2178.shm\n",
      "Path:  ../data/ShimmerData/P2179/P2179.shm\n",
      "Path:  ../data/ShimmerData/P2180/P2180.shm\n",
      "Path:  ../data/ShimmerData/P2181/P2181.shm\n",
      "Path:  ../data/ShimmerData/P2182/P2182.shm\n",
      "Path:  ../data/ShimmerData/P2183/P2183.shm\n",
      "Path:  ../data/ShimmerData/P2184/P2184.shm\n",
      "Path:  ../data/ShimmerData/P2185/P2185.shm\n",
      "Path:  ../data/ShimmerData/P2186/P2186.shm\n",
      "Path:  ../data/ShimmerData/P2187/P2187.shm\n",
      "Path:  ../data/ShimmerData/P2188/P2188.shm\n",
      "Path:  ../data/ShimmerData/P2189/P2189.shm\n",
      "Path:  ../data/ShimmerData/P2190/P2190.shm\n",
      "Path:  ../data/ShimmerData/P2191/P2191.shm\n",
      "Path:  ../data/ShimmerData/P2192/P2192.shm\n",
      "Path:  ../data/ShimmerData/P2193/P2193.shm\n",
      "Path:  ../data/ShimmerData/P2194/P2194.shm\n",
      "Path:  ../data/ShimmerData/P2195/P2195.shm\n",
      "Path:  ../data/ShimmerData/P2196/P2196.shm\n",
      "Path:  ../data/ShimmerData/P2197/P2197.shm\n",
      "Path:  ../data/ShimmerData/P2198/P2198.shm\n",
      "Path:  ../data/ShimmerData/P2200/P2200.shm\n",
      "Path:  ../data/ShimmerData/P2201/P2201.shm\n",
      "Path:  ../data/ShimmerData/P2203/P2203.shm\n",
      "Path:  ../data/ShimmerData/P2204/P2204.shm\n",
      "Path:  ../data/ShimmerData/P2205/P2205.shm\n",
      "Path:  ../data/ShimmerData/P2206/P2206.shm\n",
      "Path:  ../data/ShimmerData/P2207/P2207.shm\n",
      "Path:  ../data/ShimmerData/P2208/P2208.shm\n",
      "Path:  ../data/ShimmerData/P2209/P2209.shm\n",
      "Path:  ../data/ShimmerData/P2210/P2210.shm\n",
      "Path:  ../data/ShimmerData/P2211/P2211.shm\n",
      "Path:  ../data/ShimmerData/P2212/P2212.shm\n",
      "Path:  ../data/ShimmerData/P2213/P2213.shm\n",
      "Path:  ../data/ShimmerData/P2214/P2214.shm\n",
      "Path:  ../data/ShimmerData/P2215/P2215.shm\n",
      "Path:  ../data/ShimmerData/P2216/P2216.shm\n",
      "Path:  ../data/ShimmerData/P2217/P2217.shm\n",
      "Path:  ../data/ShimmerData/P2218/P2218.shm\n",
      "Path:  ../data/ShimmerData/P2219/P2219.shm\n",
      "Path:  ../data/ShimmerData/P2221/P2221.shm\n",
      "Path:  ../data/ShimmerData/P2222/P2222.shm\n",
      "Path:  ../data/ShimmerData/P2223/P2223.shm\n",
      "Path:  ../data/ShimmerData/P2224/P2224.shm\n",
      "Path:  ../data/ShimmerData/P2226/P2226.shm\n",
      "Path:  ../data/ShimmerData/P2227/P2227.shm\n",
      "Path:  ../data/ShimmerData/P2228/P2228.shm\n",
      "Path:  ../data/ShimmerData/P2229/P2229.shm\n",
      "Path:  ../data/ShimmerData/P2230/P2230.shm\n",
      "Path:  ../data/ShimmerData/P2231/P2231.shm\n",
      "Path:  ../data/ShimmerData/P2232/P2232.shm\n",
      "Path:  ../data/ShimmerData/P2233/P2233.shm\n",
      "Path:  ../data/ShimmerData/P2234/P2234.shm\n",
      "Path:  ../data/ShimmerData/P2235/P2235.shm\n",
      "Path:  ../data/ShimmerData/P2236/P2236.shm\n",
      "Path:  ../data/ShimmerData/P2237/P2237.shm\n",
      "Path:  ../data/ShimmerData/P2238/P2238.shm\n",
      "Path:  ../data/ShimmerData/P2239/P2239.shm\n",
      "Path:  ../data/ShimmerData/P2240/P2240.shm\n",
      "Path:  ../data/ShimmerData/P2242/P2242.shm\n",
      "Path:  ../data/ShimmerData/P2243/P2243.shm\n",
      "Path:  ../data/ShimmerData/P2244/P2244.shm\n",
      "Path:  ../data/ShimmerData/P2245/P2245.shm\n",
      "Path:  ../data/ShimmerData/P2246/P2246.shm\n",
      "Path:  ../data/ShimmerData/P2247/P2247.shm\n",
      "Path:  ../data/ShimmerData/P2248/P2248.shm\n",
      "Path:  ../data/ShimmerData/P2249/P2249.shm\n",
      "Path:  ../data/ShimmerData/P2250/P2250.shm\n",
      "Path:  ../data/ShimmerData/P2251/P2251.shm\n",
      "Path:  ../data/ShimmerData/P2253/P2253.shm\n",
      "Path:  ../data/ShimmerData/P2254/P2254.shm\n",
      "Path:  ../data/ShimmerData/P2255/P2255.shm\n",
      "Path:  ../data/ShimmerData/P2256/P2256.shm\n",
      "Path:  ../data/ShimmerData/P2257/P2257.shm\n",
      "Path:  ../data/ShimmerData/P2258/P2258.shm\n",
      "Path:  ../data/ShimmerData/P2259/P2259.shm\n",
      "Path:  ../data/ShimmerData/P2262/P2262.shm\n",
      "Path:  ../data/ShimmerData/P2263/P2263.shm\n",
      "Path:  ../data/ShimmerData/P2264/P2264.shm\n",
      "Path:  ../data/ShimmerData/P2265/P2265.shm\n",
      "Path:  ../data/ShimmerData/P2267/P2267.shm\n",
      "Path:  ../data/ShimmerData/P2268/P2268.shm\n",
      "Path:  ../data/ShimmerData/P2269/P2269.shm\n",
      "Path:  ../data/ShimmerData/P2270/P2270.shm\n",
      "Path:  ../data/ShimmerData/P2271/P2271.shm\n",
      "Path:  ../data/ShimmerData/P2272/P2272.shm\n",
      "Path:  ../data/ShimmerData/P2273/P2273.shm\n",
      "Path:  ../data/ShimmerData/P2274/P2274.shm\n",
      "Path:  ../data/ShimmerData/P2275/P2275.shm\n",
      "Path:  ../data/ShimmerData/P2276/P2276.shm\n",
      "Path:  ../data/ShimmerData/P2277/P2277.shm\n",
      "Path:  ../data/ShimmerData/P2278/P2278.shm\n",
      "Path:  ../data/ShimmerData/P2280/P2280.shm\n",
      "Path:  ../data/ShimmerData/P2282/P2282.shm\n",
      "Path:  ../data/ShimmerData/P2283/P2283.shm\n",
      "Path:  ../data/ShimmerData/P2284/P2284.shm\n",
      "Path:  ../data/ShimmerData/P2285/P2285.shm\n",
      "Path:  ../data/ShimmerData/P2286/P2286.shm\n",
      "Path:  ../data/ShimmerData/P2287/P2287.shm\n",
      "Path:  ../data/ShimmerData/P2288/P2288.shm\n",
      "Path:  ../data/ShimmerData/P2289/P2289.shm\n",
      "Path:  ../data/ShimmerData/P2290/P2290.shm\n",
      "Path:  ../data/ShimmerData/P2291/P2291.shm\n",
      "Path:  ../data/ShimmerData/P2292/P2292.shm\n",
      "Path:  ../data/ShimmerData/P2293/P2293.shm\n",
      "Path:  ../data/ShimmerData/P2294/P2294.shm\n",
      "Path:  ../data/ShimmerData/P2295/P2295.shm\n",
      "Path:  ../data/ShimmerData/P2296/P2296.shm\n",
      "Path:  ../data/ShimmerData/P2298/P2298.shm\n",
      "Path:  ../data/ShimmerData/P2299/P2299.shm\n",
      "Path:  ../data/ShimmerData/P2300/P2300.shm\n",
      "Path:  ../data/ShimmerData/P2301/P2301.shm\n",
      "Path:  ../data/ShimmerData/P2302/P2302.shm\n",
      "Path:  ../data/ShimmerData/P2303/P2303.shm\n",
      "Path:  ../data/ShimmerData/P2304/P2304.shm\n",
      "Path:  ../data/ShimmerData/P2305/P2305.shm\n",
      "Path:  ../data/ShimmerData/P2306/P2306.shm\n",
      "Path:  ../data/ShimmerData/P2307/P2307.shm\n",
      "Path:  ../data/ShimmerData/P2308/P2308.shm\n",
      "Path:  ../data/ShimmerData/P2309/P2309.shm\n",
      "Path:  ../data/ShimmerData/P2311/P2311.shm\n",
      "Path:  ../data/ShimmerData/P2312/P2312.shm\n",
      "Path:  ../data/ShimmerData/P2313/P2313.shm\n",
      "Path:  ../data/ShimmerData/P2314/P2314.shm\n",
      "Path:  ../data/ShimmerData/P2315/P2315.shm\n",
      "Path:  ../data/ShimmerData/P2316/P2316.shm\n",
      "Path:  ../data/ShimmerData/P2317/P2317.shm\n",
      "Path:  ../data/ShimmerData/P2318/P2318.shm\n",
      "Path:  ../data/ShimmerData/P2319/P2319.shm\n",
      "Path:  ../data/ShimmerData/P2320/P2320.shm\n",
      "Path:  ../data/ShimmerData/P2321/P2321.shm\n",
      "Path:  ../data/ShimmerData/P2323/P2323.shm\n",
      "Path:  ../data/ShimmerData/P2325/P2325.shm\n",
      "Path:  ../data/ShimmerData/P2326/P2326.shm\n",
      "Path:  ../data/ShimmerData/P2329/P2329.shm\n",
      "Path:  ../data/ShimmerData/P2331/P2331.shm\n",
      "Path:  ../data/ShimmerData/P2332/P2332.shm\n",
      "Path:  ../data/ShimmerData/P2334/P2334.shm\n",
      "Path:  ../data/ShimmerData/P2335/P2335.shm\n",
      "Path:  ../data/ShimmerData/P2336/P2336.shm\n",
      "Path:  ../data/ShimmerData/P2337/P2337.shm\n",
      "Path:  ../data/ShimmerData/P2338/P2338.shm\n",
      "Path:  ../data/ShimmerData/P2340/P2340.shm\n",
      "Path:  ../data/ShimmerData/P2341/P2341.shm\n",
      "Path:  ../data/ShimmerData/P2342/P2342.shm\n",
      "Path:  ../data/ShimmerData/P2343/P2343.shm\n",
      "Path:  ../data/ShimmerData/P2344/P2344.shm\n",
      "Path:  ../data/ShimmerData/P2345/P2345.shm\n",
      "Path:  ../data/ShimmerData/P2346/P2346.shm\n",
      "Path:  ../data/ShimmerData/P2347/P2347.shm\n",
      "Path:  ../data/ShimmerData/P2348/P2348.shm\n",
      "Path:  ../data/ShimmerData/P2350/P2350.shm\n",
      "Path:  ../data/ShimmerData/P2351/P2351.shm\n",
      "Path:  ../data/ShimmerData/P2352/P2352.shm\n",
      "Path:  ../data/ShimmerData/P2353/P2353.shm\n",
      "Path:  ../data/ShimmerData/P2354/P2354.shm\n",
      "Path:  ../data/ShimmerData/P2356/P2356.shm\n",
      "Path:  ../data/ShimmerData/P2357/P2357.shm\n",
      "Path:  ../data/ShimmerData/P2358/P2358.shm\n",
      "Path:  ../data/ShimmerData/P2359/P2359.shm\n",
      "Path:  ../data/ShimmerData/P2360/P2360.shm\n",
      "Path:  ../data/ShimmerData/P2361/P2361.shm\n",
      "Path:  ../data/ShimmerData/P2362/P2362.shm\n",
      "Path:  ../data/ShimmerData/P2363/P2363.shm\n",
      "Path:  ../data/ShimmerData/P2364/P2364.shm\n",
      "Path:  ../data/ShimmerData/P2365/P2365.shm\n",
      "Path:  ../data/ShimmerData/P2366/P2366.shm\n",
      "Path:  ../data/ShimmerData/P2367/P2367.shm\n",
      "Path:  ../data/ShimmerData/P2368/P2368.shm\n",
      "Path:  ../data/ShimmerData/P2369/P2369.shm\n",
      "Path:  ../data/ShimmerData/P2370/P2370.shm\n",
      "Path:  ../data/ShimmerData/P2373/P2373.shm\n",
      "Path:  ../data/ShimmerData/P2374/P2374.shm\n",
      "Path:  ../data/ShimmerData/P2375/P2375.shm\n",
      "Path:  ../data/ShimmerData/P2377/P2377.shm\n",
      "Path:  ../data/ShimmerData/P2378/P2378.shm\n",
      "Path:  ../data/ShimmerData/P2379/P2379.shm\n",
      "Path:  ../data/ShimmerData/P2380/P2380.shm\n",
      "Path:  ../data/ShimmerData/P2381/P2381.shm\n",
      "Path:  ../data/ShimmerData/P2382/P2382.shm\n",
      "Path:  ../data/ShimmerData/P2383/P2383.shm\n",
      "Path:  ../data/ShimmerData/P2384/P2384.shm\n",
      "Path:  ../data/ShimmerData/P2386/P2386.shm\n",
      "Path:  ../data/ShimmerData/P2387/P2387.shm\n",
      "Path:  ../data/ShimmerData/P2388/P2388.shm\n",
      "Path:  ../data/ShimmerData/P2390/P2390.shm\n",
      "Path:  ../data/ShimmerData/P2391/P2391.shm\n",
      "Path:  ../data/ShimmerData/P2392/P2392.shm\n",
      "Path:  ../data/ShimmerData/P2394/P2394.shm\n",
      "Path:  ../data/ShimmerData/P2395/P2395.shm\n",
      "Path:  ../data/ShimmerData/P2396/P2396.shm\n",
      "Path:  ../data/ShimmerData/P2398/P2398.shm\n",
      "Path:  ../data/ShimmerData/P2399/P2399.shm\n",
      "Path:  ../data/ShimmerData/P2401/P2401.shm\n",
      "Path:  ../data/ShimmerData/P2402/P2402.shm\n",
      "Path:  ../data/ShimmerData/P2403/P2403.shm\n",
      "Path:  ../data/ShimmerData/P2406/P2406.shm\n",
      "Path:  ../data/ShimmerData/P2407/P2407.shm\n",
      "Path:  ../data/ShimmerData/P2408/P2408.shm\n",
      "Path:  ../data/ShimmerData/P2409/P2409.shm\n",
      "Path:  ../data/ShimmerData/P2410/P2410.shm\n",
      "Path:  ../data/ShimmerData/P2413/P2413.shm\n",
      "Path:  ../data/ShimmerData/P2415/P2415.shm\n",
      "Path:  ../data/ShimmerData/P2416/P2416.shm\n",
      "Path:  ../data/ShimmerData/P2417/P2417.shm\n",
      "Path:  ../data/ShimmerData/P2418/P2418.shm\n",
      "Path:  ../data/ShimmerData/P2419/P2419.shm\n",
      "Path:  ../data/ShimmerData/P2421/P2421.shm\n",
      "Path:  ../data/ShimmerData/P2422/P2422.shm\n",
      "Path:  ../data/ShimmerData/P2423/P2423.shm\n",
      "Path:  ../data/ShimmerData/P2424/P2424.shm\n",
      "Path:  ../data/ShimmerData/P2425/P2425.shm\n",
      "Path:  ../data/ShimmerData/P2426/P2426.shm\n",
      "Path:  ../data/ShimmerData/P2427/P2427.shm\n",
      "Path:  ../data/ShimmerData/P2428/P2428.shm\n",
      "Path:  ../data/ShimmerData/P2429/P2429.shm\n",
      "Path:  ../data/ShimmerData/P2430/P2430.shm\n",
      "Path:  ../data/ShimmerData/P2432/P2432.shm\n",
      "Path:  ../data/ShimmerData/P2434/P2434.shm\n",
      "Path:  ../data/ShimmerData/P2436/P2436.shm\n",
      "Path:  ../data/ShimmerData/P2437/P2437.shm\n",
      "Path:  ../data/ShimmerData/P2438/P2438.shm\n",
      "Path:  ../data/ShimmerData/P2440/P2440.shm\n",
      "Path:  ../data/ShimmerData/P2442/P2442.shm\n",
      "Path:  ../data/ShimmerData/P2443/P2443.shm\n",
      "Path:  ../data/ShimmerData/P2444/P2444.shm\n",
      "Path:  ../data/ShimmerData/P2446/P2446.shm\n",
      "Path:  ../data/ShimmerData/P2447/P2447.shm\n",
      "Path:  ../data/ShimmerData/P2449/P2449.shm\n",
      "Path:  ../data/ShimmerData/P2451/P2451.shm\n",
      "Path:  ../data/ShimmerData/P2452/P2452.shm\n",
      "Path:  ../data/ShimmerData/P2453/P2453.shm\n",
      "Path:  ../data/ShimmerData/P2454/P2454.shm\n",
      "Path:  ../data/ShimmerData/P2455/P2455.shm\n",
      "Path:  ../data/ShimmerData/P2456/P2456.shm\n",
      "Path:  ../data/ShimmerData/P2457/P2457.shm\n",
      "Path:  ../data/ShimmerData/P2458/P2458.shm\n",
      "Path:  ../data/ShimmerData/P2459/P2459.shm\n",
      "Path:  ../data/ShimmerData/P2460/P2460.shm\n",
      "Path:  ../data/ShimmerData/P2461/P2461.shm\n",
      "Path:  ../data/ShimmerData/P2462/P2462.shm\n",
      "Path:  ../data/ShimmerData/P2463/P2463.shm\n",
      "Path:  ../data/ShimmerData/P2464/P2464.shm\n",
      "Path:  ../data/ShimmerData/P2465/P2465.shm\n",
      "Path:  ../data/ShimmerData/P2466/P2466.shm\n",
      "Path:  ../data/ShimmerData/P2467/P2467.shm\n",
      "Path:  ../data/ShimmerData/P2468/P2468.shm\n",
      "Path:  ../data/ShimmerData/P2469/P2469.shm\n",
      "Path:  ../data/ShimmerData/P2470/P2470.shm\n",
      "Path:  ../data/ShimmerData/P2471/P2471.shm\n",
      "Path:  ../data/ShimmerData/P2474/P2474.shm\n",
      "Path:  ../data/ShimmerData/P2475/P2475.shm\n",
      "Path:  ../data/ShimmerData/P2476/P2476.shm\n",
      "Path:  ../data/ShimmerData/P2477/P2477.shm\n",
      "Path:  ../data/ShimmerData/P2478/P2478.shm\n",
      "Path:  ../data/ShimmerData/P2479/P2479.shm\n",
      "Path:  ../data/ShimmerData/P2481/P2481.shm\n",
      "Path:  ../data/ShimmerData/P2482/P2482.shm\n",
      "Path:  ../data/ShimmerData/P2483/P2483.shm\n",
      "Path:  ../data/ShimmerData/P2484/P2484.shm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1063, 15352.6, 255.87666666666667, 354)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_loader import *\n",
    "def get_mealdataset_info(person_name = \"wenkanw\",file_ls = [], root_path = \"../data/\", print_file=False):\n",
    "    \"\"\"\n",
    "    if file_ls is not given, then get file_ls according to person_name\n",
    "    file path = root_path + file name in all_files_list.txt\n",
    "    \n",
    "    return:\n",
    "        meal event count, total minutes of all meals, total hours of all meals,total day counts\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(file_ls) ==0:\n",
    "        data_indices_file = \"../data-file-indices/\" +person_name+\"/all_files_list.txt\"\n",
    "        fp = open(data_indices_file,\"r\")\n",
    "        txt = fp.read()\n",
    "        fp.close()\n",
    "        file_ls = txt.split(\"\\n\")\n",
    "        while '' in file_ls:\n",
    "            file_ls.remove('')\n",
    "\n",
    "    meal_counts = 0\n",
    "    sec_counts = 0\n",
    "    min_counts = 0\n",
    "    hour_counts = 0\n",
    "    day_counts = len(file_ls)\n",
    "    for file_name in file_ls:\n",
    "        file_name = root_path + file_name\n",
    "        print(\"Path: \", file_name)\n",
    "        TotalEvents, EventStart, EventEnd, EventNames, TimeOffset = loadEvents(file_name, debug_flag = False, print_file=print_file)\n",
    "        meal_counts += TotalEvents\n",
    "        for i in range(len(EventStart)):\n",
    "            sec_counts += ( EventEnd[i]- EventStart[i])//(15)\n",
    "    \n",
    "    min_counts = sec_counts/60\n",
    "    hour_counts = min_counts/60\n",
    "    return meal_counts, min_counts,hour_counts, day_counts\n",
    "\n",
    "\n",
    "# print(file_ls)\n",
    "meal_counts, min_counts,hour_counts, day_counts = get_mealdataset_info(person_name=\"CAD\")\n",
    "meal_counts, min_counts,hour_counts, day_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ShimmerData/P2001/P2001.shm',\n",
       " 'ShimmerData/P2011/P2011.shm',\n",
       " 'ShimmerData/P2012/P2012.shm',\n",
       " 'ShimmerData/P2013/P2013.shm',\n",
       " 'ShimmerData/P2014/P2014.shm',\n",
       " 'ShimmerData/P2015/P2015.shm',\n",
       " 'ShimmerData/P2016/P2016.shm',\n",
       " 'ShimmerData/P2017/P2017.shm',\n",
       " 'ShimmerData/P2018/P2018.shm',\n",
       " 'ShimmerData/P2020/P2020.shm',\n",
       " 'ShimmerData/P2030/P2030.shm',\n",
       " 'ShimmerData/P2031/P2031.shm',\n",
       " 'ShimmerData/P2033/P2033.shm',\n",
       " 'ShimmerData/P2035/P2035.shm',\n",
       " 'ShimmerData/P2036/P2036.shm',\n",
       " 'ShimmerData/P2037/P2037.shm',\n",
       " 'ShimmerData/P2038/P2038.shm',\n",
       " 'ShimmerData/P2044/P2044.shm',\n",
       " 'ShimmerData/P2046/P2046.shm',\n",
       " 'ShimmerData/P2051/P2051.shm',\n",
       " 'ShimmerData/P2055/P2055.shm',\n",
       " 'ShimmerData/P2057/P2057.shm',\n",
       " 'ShimmerData/P2058/P2058.shm',\n",
       " 'ShimmerData/P2063/P2063.shm',\n",
       " 'ShimmerData/P2100/P2100.shm',\n",
       " 'ShimmerData/P2101/P2101.shm',\n",
       " 'ShimmerData/P2102/P2102.shm',\n",
       " 'ShimmerData/P2103/P2103.shm',\n",
       " 'ShimmerData/P2105/P2105.shm',\n",
       " 'ShimmerData/P2106/P2106.shm',\n",
       " 'ShimmerData/P2107/P2107.shm',\n",
       " 'ShimmerData/P2108/P2108.shm',\n",
       " 'ShimmerData/P2109/P2109.shm',\n",
       " 'ShimmerData/P2110/P2110.shm',\n",
       " 'ShimmerData/P2111/P2111.shm',\n",
       " 'ShimmerData/P2113/P2113.shm',\n",
       " 'ShimmerData/P2114/P2114.shm',\n",
       " 'ShimmerData/P2115/P2115.shm',\n",
       " 'ShimmerData/P2116/P2116.shm',\n",
       " 'ShimmerData/P2117/P2117.shm',\n",
       " 'ShimmerData/P2118/P2118.shm',\n",
       " 'ShimmerData/P2120/P2120.shm',\n",
       " 'ShimmerData/P2121/P2121.shm',\n",
       " 'ShimmerData/P2122/P2122.shm',\n",
       " 'ShimmerData/P2123/P2123.shm',\n",
       " 'ShimmerData/P2125/P2125.shm',\n",
       " 'ShimmerData/P2126/P2126.shm',\n",
       " 'ShimmerData/P2127/P2127.shm',\n",
       " 'ShimmerData/P2128/P2128.shm',\n",
       " 'ShimmerData/P2129/P2129.shm',\n",
       " 'ShimmerData/P2130/P2130.shm',\n",
       " 'ShimmerData/P2131/P2131.shm',\n",
       " 'ShimmerData/P2132/P2132.shm',\n",
       " 'ShimmerData/P2133/P2133.shm',\n",
       " 'ShimmerData/P2134/P2134.shm',\n",
       " 'ShimmerData/P2135/P2135.shm',\n",
       " 'ShimmerData/P2136/P2136.shm',\n",
       " 'ShimmerData/P2137/P2137.shm',\n",
       " 'ShimmerData/P2138/P2138.shm',\n",
       " 'ShimmerData/P2139/P2139.shm',\n",
       " 'ShimmerData/P2140/P2140.shm',\n",
       " 'ShimmerData/P2141/P2141.shm',\n",
       " 'ShimmerData/P2142/P2142.shm',\n",
       " 'ShimmerData/P2143/P2143.shm',\n",
       " 'ShimmerData/P2144/P2144.shm',\n",
       " 'ShimmerData/P2145/P2145.shm',\n",
       " 'ShimmerData/P2146/P2146.shm',\n",
       " 'ShimmerData/P2147/P2147.shm',\n",
       " 'ShimmerData/P2148/P2148.shm',\n",
       " 'ShimmerData/P2149/P2149.shm',\n",
       " 'ShimmerData/P2150/P2150.shm',\n",
       " 'ShimmerData/P2151/P2151.shm',\n",
       " 'ShimmerData/P2152/P2152.shm',\n",
       " 'ShimmerData/P2153/P2153.shm',\n",
       " 'ShimmerData/P2154/P2154.shm',\n",
       " 'ShimmerData/P2155/P2155.shm',\n",
       " 'ShimmerData/P2157/P2157.shm',\n",
       " 'ShimmerData/P2158/P2158.shm',\n",
       " 'ShimmerData/P2159/P2159.shm',\n",
       " 'ShimmerData/P2160/P2160.shm',\n",
       " 'ShimmerData/P2161/P2161.shm',\n",
       " 'ShimmerData/P2164/P2164.shm',\n",
       " 'ShimmerData/P2165/P2165.shm',\n",
       " 'ShimmerData/P2166/P2166.shm',\n",
       " 'ShimmerData/P2168/P2168.shm',\n",
       " 'ShimmerData/P2169/P2169.shm',\n",
       " 'ShimmerData/P2170/P2170.shm',\n",
       " 'ShimmerData/P2171/P2171.shm',\n",
       " 'ShimmerData/P2172/P2172.shm',\n",
       " 'ShimmerData/P2173/P2173.shm',\n",
       " 'ShimmerData/P2174/P2174.shm',\n",
       " 'ShimmerData/P2175/P2175.shm',\n",
       " 'ShimmerData/P2176/P2176.shm',\n",
       " 'ShimmerData/P2177/P2177.shm',\n",
       " 'ShimmerData/P2178/P2178.shm',\n",
       " 'ShimmerData/P2179/P2179.shm',\n",
       " 'ShimmerData/P2180/P2180.shm',\n",
       " 'ShimmerData/P2181/P2181.shm',\n",
       " 'ShimmerData/P2182/P2182.shm',\n",
       " 'ShimmerData/P2183/P2183.shm',\n",
       " 'ShimmerData/P2184/P2184.shm',\n",
       " 'ShimmerData/P2185/P2185.shm',\n",
       " 'ShimmerData/P2186/P2186.shm',\n",
       " 'ShimmerData/P2187/P2187.shm',\n",
       " 'ShimmerData/P2188/P2188.shm',\n",
       " 'ShimmerData/P2189/P2189.shm',\n",
       " 'ShimmerData/P2190/P2190.shm',\n",
       " 'ShimmerData/P2191/P2191.shm',\n",
       " 'ShimmerData/P2192/P2192.shm',\n",
       " 'ShimmerData/P2193/P2193.shm',\n",
       " 'ShimmerData/P2194/P2194.shm',\n",
       " 'ShimmerData/P2195/P2195.shm',\n",
       " 'ShimmerData/P2196/P2196.shm',\n",
       " 'ShimmerData/P2197/P2197.shm',\n",
       " 'ShimmerData/P2198/P2198.shm',\n",
       " 'ShimmerData/P2200/P2200.shm',\n",
       " 'ShimmerData/P2201/P2201.shm',\n",
       " 'ShimmerData/P2203/P2203.shm',\n",
       " 'ShimmerData/P2204/P2204.shm',\n",
       " 'ShimmerData/P2205/P2205.shm',\n",
       " 'ShimmerData/P2206/P2206.shm',\n",
       " 'ShimmerData/P2207/P2207.shm',\n",
       " 'ShimmerData/P2208/P2208.shm',\n",
       " 'ShimmerData/P2209/P2209.shm',\n",
       " 'ShimmerData/P2210/P2210.shm',\n",
       " 'ShimmerData/P2211/P2211.shm',\n",
       " 'ShimmerData/P2212/P2212.shm',\n",
       " 'ShimmerData/P2213/P2213.shm',\n",
       " 'ShimmerData/P2214/P2214.shm',\n",
       " 'ShimmerData/P2215/P2215.shm',\n",
       " 'ShimmerData/P2216/P2216.shm',\n",
       " 'ShimmerData/P2217/P2217.shm',\n",
       " 'ShimmerData/P2218/P2218.shm',\n",
       " 'ShimmerData/P2219/P2219.shm',\n",
       " 'ShimmerData/P2221/P2221.shm',\n",
       " 'ShimmerData/P2222/P2222.shm',\n",
       " 'ShimmerData/P2223/P2223.shm',\n",
       " 'ShimmerData/P2224/P2224.shm',\n",
       " 'ShimmerData/P2226/P2226.shm',\n",
       " 'ShimmerData/P2227/P2227.shm',\n",
       " 'ShimmerData/P2228/P2228.shm',\n",
       " 'ShimmerData/P2229/P2229.shm',\n",
       " 'ShimmerData/P2230/P2230.shm',\n",
       " 'ShimmerData/P2231/P2231.shm',\n",
       " 'ShimmerData/P2232/P2232.shm',\n",
       " 'ShimmerData/P2233/P2233.shm',\n",
       " 'ShimmerData/P2234/P2234.shm',\n",
       " 'ShimmerData/P2235/P2235.shm',\n",
       " 'ShimmerData/P2236/P2236.shm',\n",
       " 'ShimmerData/P2237/P2237.shm',\n",
       " 'ShimmerData/P2238/P2238.shm',\n",
       " 'ShimmerData/P2239/P2239.shm',\n",
       " 'ShimmerData/P2240/P2240.shm',\n",
       " 'ShimmerData/P2242/P2242.shm',\n",
       " 'ShimmerData/P2243/P2243.shm',\n",
       " 'ShimmerData/P2244/P2244.shm',\n",
       " 'ShimmerData/P2245/P2245.shm',\n",
       " 'ShimmerData/P2246/P2246.shm',\n",
       " 'ShimmerData/P2247/P2247.shm',\n",
       " 'ShimmerData/P2248/P2248.shm',\n",
       " 'ShimmerData/P2249/P2249.shm',\n",
       " 'ShimmerData/P2250/P2250.shm',\n",
       " 'ShimmerData/P2251/P2251.shm',\n",
       " 'ShimmerData/P2253/P2253.shm',\n",
       " 'ShimmerData/P2254/P2254.shm',\n",
       " 'ShimmerData/P2255/P2255.shm',\n",
       " 'ShimmerData/P2256/P2256.shm',\n",
       " 'ShimmerData/P2257/P2257.shm',\n",
       " 'ShimmerData/P2258/P2258.shm',\n",
       " 'ShimmerData/P2259/P2259.shm',\n",
       " 'ShimmerData/P2262/P2262.shm',\n",
       " 'ShimmerData/P2263/P2263.shm',\n",
       " 'ShimmerData/P2264/P2264.shm',\n",
       " 'ShimmerData/P2265/P2265.shm',\n",
       " 'ShimmerData/P2267/P2267.shm',\n",
       " 'ShimmerData/P2268/P2268.shm',\n",
       " 'ShimmerData/P2269/P2269.shm',\n",
       " 'ShimmerData/P2270/P2270.shm',\n",
       " 'ShimmerData/P2271/P2271.shm',\n",
       " 'ShimmerData/P2272/P2272.shm',\n",
       " 'ShimmerData/P2273/P2273.shm',\n",
       " 'ShimmerData/P2274/P2274.shm',\n",
       " 'ShimmerData/P2275/P2275.shm',\n",
       " 'ShimmerData/P2276/P2276.shm',\n",
       " 'ShimmerData/P2277/P2277.shm',\n",
       " 'ShimmerData/P2278/P2278.shm',\n",
       " 'ShimmerData/P2280/P2280.shm',\n",
       " 'ShimmerData/P2282/P2282.shm',\n",
       " 'ShimmerData/P2283/P2283.shm',\n",
       " 'ShimmerData/P2284/P2284.shm',\n",
       " 'ShimmerData/P2285/P2285.shm',\n",
       " 'ShimmerData/P2286/P2286.shm',\n",
       " 'ShimmerData/P2287/P2287.shm',\n",
       " 'ShimmerData/P2288/P2288.shm',\n",
       " 'ShimmerData/P2289/P2289.shm',\n",
       " 'ShimmerData/P2290/P2290.shm',\n",
       " 'ShimmerData/P2291/P2291.shm',\n",
       " 'ShimmerData/P2292/P2292.shm',\n",
       " 'ShimmerData/P2293/P2293.shm',\n",
       " 'ShimmerData/P2294/P2294.shm',\n",
       " 'ShimmerData/P2295/P2295.shm',\n",
       " 'ShimmerData/P2296/P2296.shm',\n",
       " 'ShimmerData/P2298/P2298.shm',\n",
       " 'ShimmerData/P2299/P2299.shm',\n",
       " 'ShimmerData/P2300/P2300.shm',\n",
       " 'ShimmerData/P2301/P2301.shm',\n",
       " 'ShimmerData/P2302/P2302.shm',\n",
       " 'ShimmerData/P2303/P2303.shm',\n",
       " 'ShimmerData/P2304/P2304.shm',\n",
       " 'ShimmerData/P2305/P2305.shm',\n",
       " 'ShimmerData/P2306/P2306.shm',\n",
       " 'ShimmerData/P2307/P2307.shm',\n",
       " 'ShimmerData/P2308/P2308.shm',\n",
       " 'ShimmerData/P2309/P2309.shm',\n",
       " 'ShimmerData/P2311/P2311.shm',\n",
       " 'ShimmerData/P2312/P2312.shm',\n",
       " 'ShimmerData/P2313/P2313.shm',\n",
       " 'ShimmerData/P2314/P2314.shm',\n",
       " 'ShimmerData/P2315/P2315.shm',\n",
       " 'ShimmerData/P2316/P2316.shm',\n",
       " 'ShimmerData/P2317/P2317.shm',\n",
       " 'ShimmerData/P2318/P2318.shm',\n",
       " 'ShimmerData/P2319/P2319.shm',\n",
       " 'ShimmerData/P2320/P2320.shm',\n",
       " 'ShimmerData/P2321/P2321.shm',\n",
       " 'ShimmerData/P2323/P2323.shm',\n",
       " 'ShimmerData/P2325/P2325.shm',\n",
       " 'ShimmerData/P2326/P2326.shm',\n",
       " 'ShimmerData/P2329/P2329.shm',\n",
       " 'ShimmerData/P2331/P2331.shm',\n",
       " 'ShimmerData/P2332/P2332.shm',\n",
       " 'ShimmerData/P2334/P2334.shm',\n",
       " 'ShimmerData/P2335/P2335.shm',\n",
       " 'ShimmerData/P2336/P2336.shm',\n",
       " 'ShimmerData/P2337/P2337.shm',\n",
       " 'ShimmerData/P2338/P2338.shm',\n",
       " 'ShimmerData/P2340/P2340.shm',\n",
       " 'ShimmerData/P2341/P2341.shm',\n",
       " 'ShimmerData/P2342/P2342.shm',\n",
       " 'ShimmerData/P2343/P2343.shm',\n",
       " 'ShimmerData/P2344/P2344.shm',\n",
       " 'ShimmerData/P2345/P2345.shm',\n",
       " 'ShimmerData/P2346/P2346.shm',\n",
       " 'ShimmerData/P2347/P2347.shm',\n",
       " 'ShimmerData/P2348/P2348.shm',\n",
       " 'ShimmerData/P2350/P2350.shm',\n",
       " 'ShimmerData/P2351/P2351.shm',\n",
       " 'ShimmerData/P2352/P2352.shm',\n",
       " 'ShimmerData/P2353/P2353.shm',\n",
       " 'ShimmerData/P2354/P2354.shm',\n",
       " 'ShimmerData/P2356/P2356.shm',\n",
       " 'ShimmerData/P2357/P2357.shm',\n",
       " 'ShimmerData/P2358/P2358.shm',\n",
       " 'ShimmerData/P2359/P2359.shm',\n",
       " 'ShimmerData/P2360/P2360.shm',\n",
       " 'ShimmerData/P2361/P2361.shm',\n",
       " 'ShimmerData/P2362/P2362.shm',\n",
       " 'ShimmerData/P2363/P2363.shm',\n",
       " 'ShimmerData/P2364/P2364.shm',\n",
       " 'ShimmerData/P2365/P2365.shm',\n",
       " 'ShimmerData/P2366/P2366.shm',\n",
       " 'ShimmerData/P2367/P2367.shm',\n",
       " 'ShimmerData/P2368/P2368.shm',\n",
       " 'ShimmerData/P2369/P2369.shm',\n",
       " 'ShimmerData/P2370/P2370.shm',\n",
       " 'ShimmerData/P2373/P2373.shm',\n",
       " 'ShimmerData/P2374/P2374.shm',\n",
       " 'ShimmerData/P2375/P2375.shm',\n",
       " 'ShimmerData/P2377/P2377.shm',\n",
       " 'ShimmerData/P2378/P2378.shm',\n",
       " 'ShimmerData/P2379/P2379.shm',\n",
       " 'ShimmerData/P2380/P2380.shm',\n",
       " 'ShimmerData/P2381/P2381.shm',\n",
       " 'ShimmerData/P2382/P2382.shm',\n",
       " 'ShimmerData/P2383/P2383.shm',\n",
       " 'ShimmerData/P2384/P2384.shm',\n",
       " 'ShimmerData/P2386/P2386.shm',\n",
       " 'ShimmerData/P2387/P2387.shm',\n",
       " 'ShimmerData/P2388/P2388.shm',\n",
       " 'ShimmerData/P2390/P2390.shm',\n",
       " 'ShimmerData/P2391/P2391.shm',\n",
       " 'ShimmerData/P2392/P2392.shm',\n",
       " 'ShimmerData/P2394/P2394.shm',\n",
       " 'ShimmerData/P2395/P2395.shm',\n",
       " 'ShimmerData/P2396/P2396.shm',\n",
       " 'ShimmerData/P2398/P2398.shm',\n",
       " 'ShimmerData/P2399/P2399.shm',\n",
       " 'ShimmerData/P2401/P2401.shm',\n",
       " 'ShimmerData/P2402/P2402.shm',\n",
       " 'ShimmerData/P2403/P2403.shm',\n",
       " 'ShimmerData/P2406/P2406.shm',\n",
       " 'ShimmerData/P2407/P2407.shm',\n",
       " 'ShimmerData/P2408/P2408.shm',\n",
       " 'ShimmerData/P2409/P2409.shm',\n",
       " 'ShimmerData/P2410/P2410.shm',\n",
       " 'ShimmerData/P2413/P2413.shm',\n",
       " 'ShimmerData/P2415/P2415.shm',\n",
       " 'ShimmerData/P2416/P2416.shm',\n",
       " 'ShimmerData/P2417/P2417.shm',\n",
       " 'ShimmerData/P2418/P2418.shm',\n",
       " 'ShimmerData/P2419/P2419.shm',\n",
       " 'ShimmerData/P2421/P2421.shm',\n",
       " 'ShimmerData/P2422/P2422.shm',\n",
       " 'ShimmerData/P2423/P2423.shm',\n",
       " 'ShimmerData/P2424/P2424.shm',\n",
       " 'ShimmerData/P2425/P2425.shm',\n",
       " 'ShimmerData/P2426/P2426.shm',\n",
       " 'ShimmerData/P2427/P2427.shm',\n",
       " 'ShimmerData/P2428/P2428.shm',\n",
       " 'ShimmerData/P2429/P2429.shm',\n",
       " 'ShimmerData/P2430/P2430.shm',\n",
       " 'ShimmerData/P2432/P2432.shm',\n",
       " 'ShimmerData/P2434/P2434.shm',\n",
       " 'ShimmerData/P2436/P2436.shm',\n",
       " 'ShimmerData/P2437/P2437.shm',\n",
       " 'ShimmerData/P2438/P2438.shm',\n",
       " 'ShimmerData/P2440/P2440.shm',\n",
       " 'ShimmerData/P2442/P2442.shm',\n",
       " 'ShimmerData/P2443/P2443.shm',\n",
       " 'ShimmerData/P2444/P2444.shm',\n",
       " 'ShimmerData/P2446/P2446.shm',\n",
       " 'ShimmerData/P2447/P2447.shm',\n",
       " 'ShimmerData/P2449/P2449.shm',\n",
       " 'ShimmerData/P2451/P2451.shm',\n",
       " 'ShimmerData/P2452/P2452.shm',\n",
       " 'ShimmerData/P2453/P2453.shm',\n",
       " 'ShimmerData/P2454/P2454.shm',\n",
       " 'ShimmerData/P2455/P2455.shm',\n",
       " 'ShimmerData/P2456/P2456.shm',\n",
       " 'ShimmerData/P2457/P2457.shm',\n",
       " 'ShimmerData/P2458/P2458.shm',\n",
       " 'ShimmerData/P2459/P2459.shm',\n",
       " 'ShimmerData/P2460/P2460.shm',\n",
       " 'ShimmerData/P2461/P2461.shm',\n",
       " 'ShimmerData/P2462/P2462.shm',\n",
       " 'ShimmerData/P2463/P2463.shm',\n",
       " 'ShimmerData/P2464/P2464.shm',\n",
       " 'ShimmerData/P2465/P2465.shm',\n",
       " 'ShimmerData/P2466/P2466.shm',\n",
       " 'ShimmerData/P2467/P2467.shm',\n",
       " 'ShimmerData/P2468/P2468.shm',\n",
       " 'ShimmerData/P2469/P2469.shm',\n",
       " 'ShimmerData/P2470/P2470.shm',\n",
       " 'ShimmerData/P2471/P2471.shm',\n",
       " 'ShimmerData/P2474/P2474.shm',\n",
       " 'ShimmerData/P2475/P2475.shm',\n",
       " 'ShimmerData/P2476/P2476.shm',\n",
       " 'ShimmerData/P2477/P2477.shm',\n",
       " 'ShimmerData/P2478/P2478.shm',\n",
       " 'ShimmerData/P2479/P2479.shm',\n",
       " 'ShimmerData/P2481/P2481.shm',\n",
       " 'ShimmerData/P2482/P2482.shm',\n",
       " 'ShimmerData/P2483/P2483.shm',\n",
       " 'ShimmerData/P2484/P2484.shm']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_indices_file = \"../data-file-indices/\" +\"CAD\"+\"/all_files_list.txt\"\n",
    "fp = open(data_indices_file,\"r\")\n",
    "txt = fp.read()\n",
    "fp.close()\n",
    "file_ls = txt.split(\"\\n\")\n",
    "while '' in file_ls:\n",
    "    file_ls.remove('')\n",
    "\n",
    "file_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ShimmerData/P2484/P2484.shm'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_ls.remove('')\n",
    "file_ls[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tf_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tf_utils.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from data_loader import loadEvents\n",
    "def get_meal_info(person_name = None,file_ls = [], file_ls_doc=None,root_path = \"../data/\",print_file=False,round_decimal=1):\n",
    "        \"\"\"\n",
    "        if file_ls is not given, then get file_ls according to person_name\n",
    "        file path = root_path + file name in all_files_list.txt\n",
    "\n",
    "        return:\n",
    "            meal event count, total minutes of all meals, total hours of all meals,total day counts\n",
    "\n",
    "        \"\"\"\n",
    "        meal_counts = 0\n",
    "        sec_counts = 0\n",
    "        min_counts = 0\n",
    "        hour_counts = 0\n",
    "        total_hours = 0\n",
    "        total_mins = 0\n",
    "        total_sec = 0\n",
    "        day_counts = 0 \n",
    "        if person_name ==None:\n",
    "            return meal_counts, min_counts,hour_counts, day_counts, total_hours\n",
    "\n",
    "        data_indices_file = \"../data-file-indices/\" +person_name+\"/all_files_list.txt\"\n",
    "        fp = open(data_indices_file,\"r\")\n",
    "        txt = fp.read()\n",
    "        fp.close()\n",
    "        file_ls = txt.split(\"\\n\")\n",
    "        while '' in file_ls:\n",
    "            file_ls.remove('')\n",
    "       \n",
    "        day_counts = len(file_ls)\n",
    "        \n",
    "        for file_name in file_ls:\n",
    "            file_name = root_path + file_name\n",
    "            TotalEvents, EventStart, EventEnd, EventNames, TimeOffset,EndTime = loadEvents(file_name, debug_flag = False, print_file=print_file)\n",
    "            meal_counts += TotalEvents\n",
    "            total_sec +=  abs(EndTime - TimeOffset)\n",
    "            for i in range(len(EventStart)):\n",
    "                sec_counts += ( EventEnd[i]- EventStart[i])//15\n",
    "                \n",
    "        total_hours = total_sec/(60*60)\n",
    "        min_counts = sec_counts/60\n",
    "        hour_counts = min_counts/60\n",
    "        average_meal_per_day = meal_counts/len(file_ls)\n",
    "        average_hour_per_meal = hour_counts/meal_counts\n",
    "        # round numbers\n",
    "        total_hours = round(total_hours, round_decimal)\n",
    "        min_counts = round(min_counts, round_decimal)\n",
    "        hour_counts = round(hour_counts, round_decimal)\n",
    "        average_meal_per_day = round(average_meal_per_day,round_decimal)\n",
    "        average_hour_per_meal = round(average_hour_per_meal, round_decimal)\n",
    "        \n",
    "        no_eating_hours = total_hours - hour_counts\n",
    "        weight_ratio = round(no_eating_hours/hour_counts, round_decimal)\n",
    "        result = pd.DataFrame({\"dataset\": person_name,\"Days\":day_counts, \n",
    "                      \"Total Hours\":total_hours,\"Meal Counts\":meal_counts,\n",
    "                      \"Average Meal Counts Per Day\":average_meal_per_day,\"Average Hours Per Meal\": average_hour_per_meal,\n",
    "                      \"Eating Hours\":hour_counts, \"No Eating Hours\":no_eating_hours,\n",
    "                     \"Balance Ratio(no_eat/eat)\":weight_ratio},index=[0])\n",
    "    \n",
    "        return result\n",
    "\n",
    "          \n",
    "def get_dataset_info(names= [\"wenkanw\"],winmin=6,stridesec=5):\n",
    "    \"\"\"\n",
    "    Function to get information of meal dataset\n",
    "    \"\"\"\n",
    "    meal_info = defaultdict(list)\n",
    "    dataset_results = pd.DataFrame()\n",
    "    for name in names:\n",
    "        result = get_meal_info(person_name=name)\n",
    "        if dataset_results.empty:\n",
    "            dataset_results = result\n",
    "        else:\n",
    "            dataset_results = dataset_results.append(result,ignore_index=True)\n",
    "    \n",
    "    # append total summary\n",
    "#     print( dataset_results)\n",
    "    total_result=pd.DataFrame({\"dataset\":\"total\"},columns = dataset_results.columns,index=[0])\n",
    "    # append average summary\n",
    "    average_result=pd.DataFrame({\"dataset\":\"average\"},columns = dataset_results.columns,index=[0])\n",
    "    key_ls = [\"Days\",\"Total Hours\",\"Meal Counts\",\"Eating Hours\",\"No Eating Hours\"]\n",
    "    for key in dataset_info.columns:\n",
    "        if key in key_ls:\n",
    "            total_result[key].at[0] = round(dataset_results[key].sum() ,1)\n",
    "            average_result[key].at[0] = round(dataset_results[key].mean(),1)\n",
    "\n",
    "    ls = [total_result, average_result]\n",
    "    for df in ls:\n",
    "        df[\"Average Meal Counts Per Day\"].at[0] = round(df[\"Meal Counts\"].values[0]/df[\"Days\"].values[0], 1)\n",
    "        df[\"Average Hours Per Meal\"].at[0] =round( df[\"Eating Hours\"].values[0]/df[\"Meal Counts\"].values[0], 1)\n",
    "        df[\"Balance Ratio(no_eat/eat)\"].at[0] =round(df[\"No Eating Hours\"].values[0]/df[\"Eating Hours\"].values[0],1)\n",
    "        dataset_results =dataset_results.append(df,ignore_index=True)\n",
    "\n",
    "    return dataset_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from dataset import create_train_test_file_list,  balance_data_indices  #Person_MealsDataset,\n",
    "from utils import *\n",
    "from model import *\n",
    "def train_models(model, win_ls = [],EPOCHS = 10,stridesec = 5,name = \"wenkanw\",model_name=\"acti_6min\" ,\n",
    "                 random_seed= 1000, split_day=False,test_balanced=False,\n",
    "                create_file_ls = False):\n",
    "    \"\"\"\n",
    "    Train model using train/test spit\n",
    "    \"\"\"\n",
    "    from numpy.random import seed\n",
    "    seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "#     tf.set_random_seed(random_seed)\n",
    "    from datetime  import datetime\n",
    "    batch_size = 128\n",
    "    outfile = sys.stdout\n",
    "    perf = {\"model\":[],\"data\":[],\"win(sec)\":[], \"wacc\":[],\"f1\":[],\"recall\":[],\"acc\":[]}\n",
    "    model_ls = []\n",
    "    hist_ls = []\n",
    "    for winsize in win_ls:\n",
    "        tf.random.set_seed(random_seed)\n",
    "        seed(random_seed)\n",
    "        \n",
    "        winmin = winsize\n",
    "        winlength = int(winmin * 60 * 15)\n",
    "        step = int(stridesec * 15)\n",
    "        start_time = datetime.now()\n",
    "        arr = [\"echo -n 'PBS: node is '; cat $PBS_NODEFILE\",\\\n",
    "              \"echo PBS: job identifier is $PBS_JOBID\",\\\n",
    "              \"echo PBS: job name is $PBS_JOBNAME\"]\n",
    "        [os.system(cmd) for cmd in arr]\n",
    "        print(\"*****************************************************************\\n\", file=outfile, flush=True)\n",
    "        print(\"Execution Started at \" + start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"), file=outfile, flush=True)\n",
    "        print(\"WindowLength: {:.2f} min ({:d} datum)\\tSlide: {:d} ({:d} datum)\\tEpochs:{:d}\\n\".format(winmin, winlength, stridesec, step, EPOCHS), file=outfile, flush=True)\n",
    "\n",
    "\n",
    "        if split_day:\n",
    "            pathtemp = \"../models/\" + name+\"_models\" +\"/\"+model_name+\"_split_day_M_F_\"\n",
    "        else:\n",
    "            pathtemp = \"../models/\" + name+\"_models\" +\"/\"+model_name+\"_M_F_\"\n",
    "            \n",
    "        #pathtemp = \"../models/\" + name +\"/\"+model_name+\"_M_F_\"\n",
    "        modelpath = pathtemp + \"{:f}Min.h5\".format(winmin)\n",
    "        jsonpath = pathtemp + \"{:f}Min.json\".format(winmin)\n",
    "        \n",
    "        print(\"Model to Save: \",modelpath)\n",
    "        print()\n",
    "        ########### Load the dataset################\n",
    "        person = name\n",
    "        if create_file_ls:\n",
    "            create_train_test_file_list(file_name= \"all_files_list.txt\",person_name =name,\n",
    "                         out_path = \"../data-file-indices/\",root_path= \"../\",\n",
    "                         test_ratio = 0.2, print_flag = True, shuffle=True, random_state=random_seed)\n",
    "        \n",
    "        if split_day:\n",
    "            \n",
    "\n",
    "            meal_data_train = Person_MealsDataset(person_name= person, file_name = \"train_files\", winmin = winmin,stridesec = stridesec)\n",
    "            meal_data_test = Person_MealsDataset(person_name= person, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "\n",
    "            train_indices, valid_indices = split_train_test_indices(X= [i for i in range(len(meal_data_train.labels))],\n",
    "                                                                    y = meal_data_train.labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            #balanced train set\n",
    "            trainset_labels = meal_data_train.labels[train_indices]\n",
    "            train_indices = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "\n",
    "            # balance test set\n",
    "            testset_labels = meal_data_test.labels\n",
    "            if test_balanced:\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices=[i for i in range(len(meal_data_test))] ,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                # without balancing data\n",
    "                test_indices = [i for i in range(len(meal_data_test))] \n",
    "                \n",
    "            # get numpy dataset\n",
    "            balancedData, balancedLabels = meal_data_train.get_subset(train_indices)\n",
    "            valid_balancedData, valid_balancedLabels = meal_data_train.get_subset(valid_indices)\n",
    "            test_Data, test_Labels = meal_data_test.get_subset(test_indices)\n",
    "\n",
    "        else:\n",
    "        \n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "            samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "            # split train set and test set\n",
    "            train_indices, test_indices = split_train_test_indices(X= [i for i in range(len(labels))],\n",
    "                                                                    y = labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            # balance train set\n",
    "            trainset_labels = labels[train_indices]\n",
    "            train_indices_balanced = balance_data_indices(trainset_labels,data_indices= train_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            \n",
    "            \n",
    "            testset_labels = labels[test_indices]\n",
    "            if test_balanced:\n",
    "                #balance test set\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices= test_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                test_indices = test_indices \n",
    "            \n",
    "            \n",
    "            train_set_balanced = torch.utils.data.Subset(meal_data, train_indices_balanced)\n",
    "            test_set = torch.utils.data.Subset(meal_data, test_indices)\n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(train_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "            test_loader = torch.utils.data.DataLoader(test_set ,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "            print(\"Data Loader Created\")            \n",
    "            \n",
    "            # split validation set\n",
    "            balanced_trainset_labels = labels[train_indices_balanced]\n",
    "            train_indices, valid_indices = split_train_test_indices(X= train_indices_balanced,\n",
    "                                                                    y = balanced_trainset_labels, test_size = 0.2,\n",
    "                                                                   random_seed = random_seed)\n",
    "            valid_set_balanced = torch.utils.data.Subset(meal_data, valid_indices)\n",
    "            valid_loader = torch.utils.data.DataLoader(valid_set_balanced,batch_size=batch_size, shuffle=True,num_workers=2)\n",
    "\n",
    "            # Get numpy dataset: balanced trainset, validation set, test set\n",
    "            balancedData, balancedLabels = meal_data.get_subset(train_indices)\n",
    "            valid_balancedData, valid_balancedLabels = meal_data.get_subset(valid_indices)\n",
    "            test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "        \n",
    "\n",
    "        #training settings\n",
    "        mcp_save = tf.keras.callbacks.ModelCheckpoint(modelpath, save_best_only=True, monitor='accuracy')\n",
    "        \n",
    "\n",
    "        scheduler = tf.keras.callbacks.ReduceLROnPlateau( monitor='val_loss', factor=0.1, patience=3, verbose=0,\n",
    "                                             mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.)\n",
    "        \n",
    "        ##########train model ###############\n",
    "        H = model.fit(x=balancedData, y = balancedLabels,\n",
    "                       validation_data=(valid_balancedData, valid_balancedLabels),\n",
    "                    epochs = EPOCHS, batch_size=batch_size, verbose=1,\n",
    "                    callbacks=[mcp_save,scheduler]) # removed addons.LossHistory(jsonpath) for compatibility with TensorFlow 2.2.0, needs to be re-added at some point\n",
    "\n",
    "        print(\"Max value: \", max(H.history['accuracy']), \" at epoch\", np.argmax(H.history['accuracy']) + 1)\n",
    "\n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, balanced_accuracy_score, f1_score\n",
    "        predictions = model.predict(x=test_Data)\n",
    "        threshold = 0.5\n",
    "        wacc =  balanced_accuracy_score(test_Labels,predictions>=threshold)\n",
    "        f1 =  f1_score(test_Labels,predictions>=threshold)\n",
    "        acc =  accuracy_score(test_Labels,predictions>=threshold)\n",
    "        recall = recall_score(test_Labels,predictions>=threshold)\n",
    "        \n",
    "        #auc = roc_auc_score(test_Labels,predictions>=threshold)\n",
    "        print(\"Weighted Accuracy:\", wacc)\n",
    "        print(\"Test Accuracy:\", acc)\n",
    "        print(\"F1-score:\", f1)\n",
    "        print(\"Recall Accuracy:\", recall)\n",
    "        #print(\"AUC Score:\", auc)\n",
    "\n",
    "        perf[\"model\"].append(\"ActiModel\")\n",
    "        perf[\"data\"].append(name)\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        perf[\"wacc\"].append(wacc)\n",
    "        perf[\"f1\"].append(f1)\n",
    "        perf[\"acc\"].append(acc)\n",
    "        perf[\"recall\"].append(recall)\n",
    "        #perf[\"auc\"].append(auc)\n",
    "        model_ls.append(model)\n",
    "        hist_ls.append(H)\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    print(perf_df)\n",
    "    return perf_df, model_ls, hist_ls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "name_ls = [\"wenkanw\",'adam',\"lawler\",\"shaurya\"]\n",
    "dataset_info = get_dataset_info(names= name_ls)\n",
    "balance_ratio = dataset_info[dataset_info[\"dataset\"]==\"total\"]['Balance Ratio(no_eat/eat)'].values[0]\n",
    "\n",
    "def weight_accuracy(y_true, y_pred,weight= balance_ratio):\n",
    "    TP = sum( (y_true==1) &(y_pred==1) )\n",
    "    FN = sum( (y_true==1) &(y_pred==0) )\n",
    "    TN = sum( (y_true==0) &(y_pred==0) )\n",
    "    FP = sum( (y_true==0) &(y_pred==1) )\n",
    "    print(\"TP: \",TP, \"FP: \",FP, \"TN: \",TN, \"FN: \",FN)\n",
    "    return (weight*TP + TN)/(weight*(TP+FN) + (TN+FP))\n",
    "\n",
    "def test_models_time_metric(winmin=1, stridesec = 5,names= [\"wenkanw\"],random_seed=1000, split_day=False, test_balance=False, test_CAD=False):\n",
    "    \"\"\"\n",
    "    Test time metrics\n",
    "    \"\"\"\n",
    "    perf = defaultdict(list)\n",
    "    meal_info = defaultdict(list)\n",
    "    # Don't test on CAD test set if it is not enabled\n",
    "    if not test_CAD and \"CAD\" in names:\n",
    "        names.remove(\"CAD\")\n",
    "        \n",
    "    for name in names:\n",
    "        person = name\n",
    "        if split_day:\n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "\n",
    "            # balance test set\n",
    "            testset_labels = meal_data.labels\n",
    "            if test_balance:\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices=[i for i in range(len(meal_data))] ,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                test_indices = [i for i in range(len(meal_data))]\n",
    "            # get numpy dataset\n",
    "            test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "        else:            \n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "            samples,labels =  meal_data.data_indices, meal_data.labels\n",
    "            # split train set and test set\n",
    "            train_indices, test_indices = split_train_test_indices(X= [i for i in range(len(labels))],\n",
    "                                                                            y = labels, test_size = 0.2,\n",
    "                                                                           random_seed = random_seed)\n",
    "            \n",
    "            if test_balance:\n",
    "                testset_labels = labels[test_indices]\n",
    "                test_indices = balance_data_indices(testset_labels,data_indices= test_indices,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "            else:\n",
    "                test_indices = test_indices\n",
    "            testset_labels = labels[test_indices]\n",
    "            print(\"Testing on : \", sum(testset_labels==1),\"positive samples, \",sum(testset_labels==0),\" negative samples\" )\n",
    "            test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "        if name !=\"CAD\":\n",
    "            meal_counts, min_counts,hour_counts, day_counts,total_hours = meal_data.get_mealdataset_info(person_name=name, file_ls_doc=\"all_files_list.txt\")\n",
    "        else:\n",
    "            # data from paper\n",
    "            meal_counts, min_counts,hour_counts, day_counts,total_hours = 1063, 250*60,250, 354, 4680\n",
    "        meal_info[\"dataset\"].append(name)\n",
    "        meal_info[\"Days\"].append(day_counts)\n",
    "        meal_info[\"Meal_Hours\"].append(round(hour_counts,1)) \n",
    "        meal_info[\"Meal_Counts\"].append(meal_counts) \n",
    "        meal_info[\"Total_Hours\"].append(total_hours) \n",
    "        \n",
    "        perf[\"dataset\"].append(name)\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, f1_score, balanced_accuracy_score, precision_score\n",
    "        group_model_W  = tf.keras.models.load_model('../models/CAD_models/acti_6min_M_F_6.000000Min.h5')\n",
    "        \n",
    "        if name != \"CAD\":\n",
    "            if split_day:\n",
    "                individual_model = tf.keras.models.load_model('../models/'+ name+ '_models/acti_6min_split_day_M_F_6.000000Min.h5')\n",
    "            else:    \n",
    "                individual_model = tf.keras.models.load_model('../models/'+ name+ '_models/acti_6min_M_F_6.000000Min.h5')\n",
    "        models = {\"suffix\":['Individual-Model','GroupModel'],  \"model\":[individual_model,group_model_W]}\n",
    "        \n",
    "        for i in range(len(models[\"suffix\"])):\n",
    "            suffix = models[\"suffix\"][i]\n",
    "            model = models[\"model\"][i]\n",
    "            # if the dataset is CAD group dataset and model is individual model\n",
    "            # we don't need to make prediction on that data\n",
    "            if name == \"CAD\" and suffix =='Individual-Model':\n",
    "                acc = None\n",
    "                auc = None\n",
    "                recall = None\n",
    "            else:\n",
    "\n",
    "                predictions = model.predict(x=test_Data).squeeze(1)\n",
    "                threshold = 0.5\n",
    "                prediction = (predictions>=threshold).astype(int)\n",
    "                wacc =  balanced_accuracy_score(test_Labels,prediction)\n",
    "                acc =  accuracy_score(test_Labels,prediction)\n",
    "                recall = recall_score(test_Labels,prediction)\n",
    "                f1 = f1_score(test_Labels,prediction)\n",
    "                precision = precision_score(test_Labels,prediction)\n",
    "                print(\"Test label: \",test_Labels)\n",
    "                print(\"Predictions:\",prediction)\n",
    "                \n",
    "                # weighted accuracy 2 is computed by (weight*TP +TN)/(weight*(TP+FN) + (TN+FP))\n",
    "                wacc2 = weight_accuracy(test_Labels,prediction)\n",
    "            \n",
    "            \n",
    "            print(\"Weighted Accuracy:\", wacc)\n",
    "            print(\"Weighted Accuracy2:\", wacc2)\n",
    "            print(\"Recall:\", recall)\n",
    "            print(\"Precision:\", precision)\n",
    "            print(\"F1:\", f1)\n",
    "            print(\"Test Accuracy:\", acc)\n",
    "            \n",
    "            perf[\"WAcc: \"+suffix].append(wacc)\n",
    "            perf[\"WAcc2: \"+suffix].append(wacc2)\n",
    "            perf[\"Recall: \"+suffix].append(recall)\n",
    "            perf[\"Precision: \"+suffix].append(precision)\n",
    "            perf[\"F1: \"+suffix].append(f1)\n",
    "            perf[\"Acc: \"+suffix].append(acc)\n",
    "\n",
    "    meal_info = pd.DataFrame(meal_info)\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    return meal_info, perf_df\n",
    "\n",
    "\n",
    "def print_time_metrics(result, round_decimal = 3):\n",
    "    perf_df = pd.DataFrame()\n",
    "    mykeys = [\"dataset\",\"win(sec)\",\"WAcc\", \"F1\",\"Precision\",\"Recall\"]\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']    \n",
    "    for k in mykeys:\n",
    "        for key in result.keys():\n",
    "            if k in key:\n",
    "                perf_df[key] = result[key]\n",
    "                if result[key].dtype in numerics:\n",
    "                    perf_df[key] = np.round(perf_df[key],round_decimal)\n",
    "    for key in result.keys():\n",
    "        if \"WAcc\" not in key and \"Acc\" in key:\n",
    "            perf_df[key] = result[key].values.round(round_decimal)\n",
    "    \n",
    "    mean_perf = pd.DataFrame(columns = perf_df.keys())\n",
    "    mean_perf = mean_perf.append({\"dataset\":\"average performance\",\"win(sec)\":\"-\"},ignore_index=True)\n",
    "    for key in perf_df.keys():\n",
    "        if key.lower() != \"dataset\" and  key.lower() != \"win(sec)\":\n",
    "            mean_perf[key].at[0] = perf_df[key].mean().round(round_decimal)\n",
    "    \n",
    "    perf_df = perf_df.append(mean_perf,ignore_index=True)\n",
    "            \n",
    "    return perf_df\n",
    "\n",
    "\n",
    "\n",
    "def hysteresis_threshold(model, data,start_threshold=0.8, end_threshold=0.3, winmin = 6,\n",
    "                        stepsec=5, episode_min = 1.,t_pause = 900):\n",
    "    \"\"\"\n",
    "    model: tensorflow model\n",
    "    data:  This dataset must be the self-defined class of Person_MealsDataset  datasetset in my dataset.py/pytorch dataset without using shuffle. \n",
    "    Keep the order of dataset after extracting window samples!  You can also define your own dataset using class object to create the interface\n",
    "    \n",
    "    start_threshold: the high threshold of the beginning of segmentation\n",
    "    \n",
    "    end_threshold: the end threshold of the end of segmentation\n",
    "    \n",
    "    winmin: size of a window sample in unit of  minute\n",
    "    \n",
    "    stepsec: stride to move the window in unit of second / the number of second between two adjacent window samples\n",
    "    \n",
    "    episode_min: the minimum length of eating episode in unit of minute. If end of segmentation -start of segmentation < episode_min,\n",
    "        then the episode will not be counted\n",
    "    \n",
    "    \"\"\"\n",
    "    result_ls = []\n",
    "    \n",
    "    \n",
    "    days = set(data.data_indices[:,0])\n",
    "    for day in days:\n",
    "        # Select and Extract the data and labels of the corresponding day from the whole dataset\n",
    "        sample_indices= np.where(data.data_indices[:,0]==day)[0]\n",
    "        result = {'day':day,\"stepsec\": stepsec,'segment_start':[], 'segment_end':[],'proba':[],'predictions':np.zeros([len(sample_indices)]),'labels':[],\"segment_count\":0}\n",
    "        \n",
    "        # get the numpy array of samples and labels\n",
    "        samples, labels = data.get_subset(sample_indices)\n",
    "        probas = model(samples)\n",
    "        state = 0\n",
    "        start = 0\n",
    "        end = 0 \n",
    "        pause_counter = 0\n",
    "        # one day data\n",
    "        print(\"Day: \",day)\n",
    "        for i in range(len(sample_indices)):\n",
    "            #print(\"i:\",i)\n",
    "            #sample, label = data[i][0].numpy(),data[i][1]\n",
    "            #sample = np.expand_dims(sample,axis=0)\n",
    "            #proba = model(sample).numpy()[0][0]\n",
    "            sample = samples[i]\n",
    "            label = labels[i]\n",
    "            proba = probas[i].numpy()[0]\n",
    "            \n",
    "            result['proba'].append(proba)\n",
    "            result['labels'].append(label)\n",
    "            \n",
    "            if state ==0 and proba > start_threshold:\n",
    "                state = 1\n",
    "                start = i\n",
    "            elif state == 1 and proba <end_threshold:\n",
    "                state = 2\n",
    "                end = i+1\n",
    "                pause_counter = 0\n",
    "            elif state ==2:\n",
    "                if proba > start_threshold:\n",
    "                    state = 1\n",
    "                else:\n",
    "                    pause_counter += stepsec\n",
    "                    if pause_counter >= t_pause:\n",
    "                        # convert time to second and check threshold\n",
    "                        if (end-start)*stepsec >= episode_min*60:\n",
    "                            # save data\n",
    "                            result['segment_start'].append(start)\n",
    "                            result['segment_end'].append(end)\n",
    "                            result['segment_count'] += 1\n",
    "                            result['predictions'][start:end] = 1\n",
    "                            pass\n",
    "                        end = 0\n",
    "                        state = 0\n",
    "        if state != 0:\n",
    "            # if segment ended at the end of data\n",
    "            if end != 0:\n",
    "                result['segment_start'].append(start)\n",
    "                result['segment_end'].append(end)\n",
    "                result['predictions'][start:end] = 1\n",
    "            else:\n",
    "                result['segment_count'] -= 1  \n",
    "            result['segment_count'] += 1\n",
    "            \n",
    "        result_ls.append(result)\n",
    "        print(\"Segmentation Completed. \")\n",
    "                            \n",
    "    return pd.DataFrame(result_ls)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_episode_metrics(result, meal_data):\n",
    "    \"\"\"\n",
    "    result: result from  hysteresis threshold function\n",
    "    meal_data: meal dataset of Person_MealData\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    perf = {\"TPR\":[],\"FP/TP\":[],\"TP\":[], \"FP\":[],\"FN\":[]}\n",
    "    tpr = 0. \n",
    "    FP_TP = 0.\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    # get segmentation of ground truth labeled by user\n",
    "    start_ls, end_ls = meal_data.get_GT_segment()\n",
    "    \n",
    "    meal_counts, min_counts,hour_counts, day_counts,total_hours = meal_data.get_mealdataset_info()\n",
    "    \n",
    "    # iterate every day\n",
    "    for i in range(len(result)):\n",
    "        #preds = result.iloc[i]['predictions']\n",
    "        #labels =  result.iloc[i]['labels']\n",
    "        event_start, event_end= start_ls[i], end_ls[i]\n",
    "        detect_start, detect_end = result.iloc[i]['segment_start'],result.iloc[i]['segment_end']\n",
    "        GT = np.array([-1]*len(event_start) )  # default all meals are missing -1, FN\n",
    "        detect = np.array([-1]*len(detect_start)) # default all detected meals are wrong -1, FP\n",
    "        for index in range(len(event_start)):\n",
    "            # e_s: event start,  e_e: event end\n",
    "            # d_s: detection start,  d_e: detection end\n",
    "            e_s, e_e = event_start[index], event_end[index]\n",
    "            for index2 in range(len(detect_start)):\n",
    "                # convert segment from sec to index of data point\n",
    "                d_s = detect_start[index2] * result.iloc[i]['stepsec']*15\n",
    "                d_e = detect_end[index2]* result.iloc[i]['stepsec']*15\n",
    "                #print(\"ds: {} d_e: {}, e_s:{}, e_e: {}\".format(d_s,d_e, e_s, e_e))\n",
    "                if (e_s>=d_s and e_s <= d_e) or (d_s>= e_s and d_s<= e_e):\n",
    "                    GT[index] = index2\n",
    "                    detect[index2] = index\n",
    "        #print(\"GT:\",GT, \"Detect:\", detect)\n",
    "        TP += sum(GT!=-1)\n",
    "        FN += sum(GT==-1)\n",
    "        FP += sum(detect==-1)\n",
    "                \n",
    "    \n",
    "    print(\"total_meal:\",meal_counts, \"TP: \", TP, \"FP: \", FP, \"FN: \", FN)\n",
    "    perf['TPR'].append(TP/(TP+FN))\n",
    "    if TP ==0:\n",
    "        perf['FP/TP'].append(None)\n",
    "    else:\n",
    "        perf['FP/TP'].append(FP/TP)\n",
    "    perf[\"TP\"].append(TP)\n",
    "    perf[\"FP\"].append(FP)\n",
    "    perf[\"FN\"].append(FN)\n",
    "    result_df = pd.DataFrame(perf)\n",
    "        \n",
    "    return pd.DataFrame(result_df)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def test_models_episode_metric(winmin=6, stridesec = 5,names= [\"wenkanw\"],random_seed=1000, test_balance=False, test_CAD=False,test_alldata=False):\n",
    "    perf = defaultdict(list)\n",
    "    meal_info = defaultdict(list)\n",
    "    threshold = {'wenkanw':[0.8, 0.3], 'adam':[0.8,0.3],'lawler':[0.8,0.3], 'shaurya':[0.8,0.3]}\n",
    "    # Don't test on CAD test set if it is not enabled\n",
    "    if not test_CAD and \"CAD\" in names:\n",
    "        names.remove(\"CAD\")\n",
    "        \n",
    "    for name in names:\n",
    "        \n",
    "        high_th, low_th = threshold[name][0], threshold[name][1]\n",
    "        \n",
    "        person = name\n",
    "        # test episode metrics that split dataset by days\n",
    "        if not test_alldata:\n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"test_files\", winmin = winmin,stridesec = stridesec)\n",
    "        else:\n",
    "            meal_data = Person_MealsDataset(person_name= person, file_name = \"all_files_list\", winmin = winmin,stridesec = stridesec)\n",
    "        # balance test set    \n",
    "        if test_balance:\n",
    "            testset_labels = meal_data.labels\n",
    "            test_indices = balance_data_indices(testset_labels,data_indices=[i for i in range(len(meal_data))] ,mode=\"under\", shuffle=True,random_state = random_seed,replace= False)\n",
    "        else:\n",
    "            test_indices = [i for i in range(len(meal_data))]\n",
    "        # get numpy dataset\n",
    "        #test_Data, test_Labels = meal_data.get_subset(test_indices)\n",
    "        \n",
    "        \n",
    "        meal_counts, min_counts,hour_counts, day_counts,total_hours = meal_data.get_mealdataset_info(person_name=name)\n",
    "                \n",
    "        \n",
    "        perf[\"Days\"].append(day_counts)\n",
    "        perf[\"Meal_Hours\"].append(round(hour_counts,1)) \n",
    "        perf[\"Meal_Counts\"].append(meal_counts) \n",
    "        perf[\"dataset\"].append(name)\n",
    "        perf[\"win(sec)\"].append(winmin*60)\n",
    "        \n",
    "            \n",
    "        from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, f1_score, balanced_accuracy_score, precision_score\n",
    "        group_model_W  = tf.keras.models.load_model('../models/CAD_models/acti_6min_M_F_6.000000Min.h5')\n",
    "        \n",
    "        individual_model = tf.keras.models.load_model('../models/'+ name+ '_models/acti_6min_split_day_M_F_6.000000Min.h5')\n",
    "            \n",
    "        models = {\"suffix\":['Individual-Model','GroupModel'],  \"model\":[individual_model,group_model_W]}\n",
    "        \n",
    "        for i in range(len(models[\"suffix\"])):\n",
    "            suffix = models[\"suffix\"][i]\n",
    "            model = models[\"model\"][i]\n",
    "            # if the dataset is CAD group dataset and model is individual model\n",
    "            # we don't need to make prediction on that data\n",
    "            \n",
    "            result = hysteresis_threshold(model, meal_data,start_threshold=high_th, end_threshold=low_th, winmin = 6, stepsec=5, episode_min = 1.)\n",
    "            episode_perf_df = get_episode_metrics(result,meal_data)\n",
    "            perf[\"TPR: \"+suffix].append(episode_perf_df[\"TPR\"].iloc[0])\n",
    "            perf[\"FP/TP: \"+suffix].append(episode_perf_df[\"FP/TP\"].iloc[0])\n",
    "            perf[\"TP: \"+suffix].append(episode_perf_df[\"TP\"].iloc[0])\n",
    "            perf[\"FP: \"+suffix].append(episode_perf_df[\"FP\"].iloc[0])\n",
    "            perf[\"FN: \"+suffix].append(episode_perf_df[\"FN\"].iloc[0])\n",
    "            \n",
    "            print(episode_perf_df)\n",
    "\n",
    "    perf_df = pd.DataFrame(perf)\n",
    "    return perf_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv_v2",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
